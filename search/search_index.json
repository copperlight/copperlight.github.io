{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Copperlight Writes \u00b6 2017-10-13 Discuss The purpose of this site is to share notes and neat ways of solving problems. If you have questions or comments on this site, file a GitHub issue and discussion will take place there. Top-10 Popular Pages (Jul 2019) \u00b6 Scalafmt Configuration Tips Ansible Vault and SSH Key Distribution Testing AWS Clients with IAM AssumeRole Credentials in Scala Galaxy's Best Margarita AWS Credential Files for Java and Python Reset Jenkins Build Number IntelliJ Configuration Tips Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina Memory Profiling with Pyrasite and Heapy JMH Testing with Scala","title":"Home"},{"location":"#copperlight-writes","text":"2017-10-13 Discuss The purpose of this site is to share notes and neat ways of solving problems. If you have questions or comments on this site, file a GitHub issue and discussion will take place there.","title":"Copperlight Writes"},{"location":"#top-10-popular-pages-jul-2019","text":"Scalafmt Configuration Tips Ansible Vault and SSH Key Distribution Testing AWS Clients with IAM AssumeRole Credentials in Scala Galaxy's Best Margarita AWS Credential Files for Java and Python Reset Jenkins Build Number IntelliJ Configuration Tips Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina Memory Profiling with Pyrasite and Heapy JMH Testing with Scala","title":"Top-10 Popular Pages (Jul 2019)"},{"location":"ansible/ansible-vault-and-ssh-key-distribution/","text":"Ansible Vault and SSH Key Distribution \u00b6 2014-06-30 Discuss There are two types of SSH key distribution discussed in this post: private keys on local hosts and public keys on remote hosts. SSH private key distribution is best used for setting up your own workstation or possibly an Ansible Tower server. In general, you should not be distributing private keys widely; with a good SSH tunneling configuration and SSH public key distribution, there should be no need for the private keys to be installed in more than few places. This configuration will show off a technique for configuring an SSH jump host bastion that allows you to keep your private key on your own workstation; there is no need to have the SSH private key on the bastion host. For the purpose of this post, I have generated a new SSH key pair to demonstrate this technique; this keypair is used nowhere. Part of the trick to making this work is that the private key needs to be base64 encoded so that line breaks are preserved when it is stored as a yaml string in the vars_files . Template files are created for the public and private keys to preserve file change detection. Generate a new key pair: $ ssh-keygen -b 2048 -f junk_key -C junk Generating public/private rsa key pair. Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in junk_key. Your public key has been saved in junk_key.pub. The key fingerprint is: 94 :94:ae:ac:c3:5e:ee:7d:fa:2c:cb:0f:ae:19:c8:99 junk The key ' s randomart image is: +-- [ RSA 2048 ] ----+ | .. | | ... | | .o | | .. | | . .S | | . +o | | .E.o . | | +o *.o. | | ..o = .*B+ | +-----------------+ </pre> Base64 encode the private key: base64 -i junk_key > junk_key.b64 Create an Ansible vars_files yaml data file named ssh_keys/ssh_key_vault.yml . The ssh_private_key variable should contain the base64 encoded private key and the ssh_public_key variable should contain the public key. Encrypt the file with ansible-vault : ssh_private_key : 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 ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk ansible-vault encrypt ssh_keys/ssh_key_vault.yml Vault password: Confirm Vault password: Encryption successful Create an inventory file named inventory , showing off the SSH jump host connection capability: [localhost] localhost ansible_connection = local [group-all:children] group-01 group-02 [group-01] i-00000001 ansible_ssh_host = bastion+192.168.1.1 ansible_ssh_user=remoteuser i-00000002 ansible_ssh_host = bastion+192.168.1.2 ansible_ssh_user=remoteuser [group-02] i-00000003 ansible_ssh_host = bastion+192.168.1.3 ansible_ssh_user=remoteuser i-00000004 ansible_ssh_host = bastion+192.168.1.4 ansible_ssh_user=remoteuser Create a template file for the private key named templates/HOME_.ssh_junk : {{ ssh_private_key_decoded.stdout }} Create a template file for the public key named templates/HOME_.ssh_junk.pub : {{ ssh_public_key }} Create a template file for the SSH jump host configuration named templates/HOME_.ssh_config : Host * ServerAliveInterval 30 ServerAliveCountMax 5 Host bastion User {{ remote_user }} IdentityFile ~/.ssh/junk Hostname bastion Host bastion+* User {{ remote_user }} IdentityFile ~/.ssh/junk ProxyCommand ssh -T -a bastion nc $(echo %h |cut -d+ -f2) %p 2>/dev/null StrictHostKeyChecking no This configuration assumes that you have a consistent remote username defined on the bastion server and your protected hosts. Write a playbook to install the SSH key and configuration on your local workstation named config_local-ssh.yml : - name : configure local ssh hosts : - localhost gather_facts : false sudo : false vars : local_home : \"{{ lookup('env','HOME') }}\" local_user : \"{{ lookup('env','USER') }}\" remote_user : remoteuser vars_files : - ssh_keys/ssh_key_vault.yml tasks : - file : path={{local_home}}/.ssh state=directory mode=0700 owner={{local_user}} - template : src=templates/HOME_.ssh_config dest={{local_home}}/.ssh/config mode=0644 owner={{local_user}} backup=yes - shell : echo {{ssh_private_key}} |base64 --decode register : ssh_private_key_decoded - template : src=templates/HOME_.ssh_junk dest={{local_home}}/.ssh/junk mode=0600 owner={{local_user}} - template : src=templates/HOME_.ssh_junk.pub dest={{local_home}}/.ssh/junk.pub mode=0644 owner={{local_user}} Run the playbook to setup your local workstation with SSH keys and configuration: ansible-playbook -i inventory config_local-ssh.yml --ask-vault-pass Vault password: Test your SSH tunneling access to a remote host behind the bastion server: ssh bastion+192.168.1.1 ssh bastion+192.168.1.1 \"date; date > /tmp/date.out\" scp bastion+192.168.1.1:/tmp/date.out . Notice that the hosts behind the bastion server are referenced in the SSH command the same way that they are referenced in the Ansible inventory file. The \"+\" character used as a separator was selected explicitly for its ability to be used interchangeably at the command line and in the Ansible inventory. IP addresses are being used on the right hand side of the expression since the secondary connection to the protected host relies on the name resolution capabilities of the first host in the tunnel. If you had a reliable dynamic DNS service that was keeping up with changes to the protected hosts and was accessible to the bastion host, then you could use host names instead, such as bastion+webserver01 . This host selection technique can be extended to an Ansible dynamic inventory script, if you were running instances at a cloud provider such as AWS. When you write a dynamic inventory script , the data format should look like this: { \"group-01\" : { \"hosts\" : [ \"i-00000001\" , \"i-00000002\" ] }, \"group-02\" : { \"hosts\" : [ \"i-00000003\" , \"i-00000004\" ] }, \"group-all\" : { \"children\" : [ \"group-01\" , \"group-02\" ] }, \"localhost\" : [ \"localhost\" ], \"_meta\" : { \"hostvars\" : { \"i-00000001\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.1\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"i-00000002\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.2\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"i-00000003\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.3\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"i-00000004\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.4\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"localhost\" : { \"ansible_connection\" : \"local\" }, } } } The nice thing about this style of SSH configuration is that you can have multiple bastion hosts in different locations and target the hosts behind each of them, provided that you give your bastion hosts different names. The method of accessing them is the same between direct SSH connections and Ansible execution. Once you have this infrastructure in place, you can start distributing public SSH keys to your protected hosts. Write a templates/etc_sudoers file that grants NOPASSWD access to the sudo group: Defaults env_reset Defaults mail_badpass Defaults secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" # Host alias specification # User alias specification # Cmnd alias specification # User privilege specification root ALL=(ALL:ALL) ALL # Allow members of group sudo to execute any command %sudo ALL=NOPASSWD: ALL #includedir /etc/sudoers.d Write a playbook update_remote-ssh.yml to configure NOPASSWD sudo access for your remote user and distribute SSH public keys on your remote hosts. This will allow subsequent playbook execution to operate more easily against your remote hosts. In order for this to work, the paramiko connection type must be used initially, so that the password can be requested once and re-used across all hosts. - name : update remote ssh hosts : - group-all gather_facts : false sudo : true connection : paramiko vars_files : - ssh_keys/ssh_key_vault.yml tasks : - copy : src=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root - user : name=remoteuser groups=sudo shell=/bin/bash state=present - authorized_key : user=remoteuser state=present key={{ssh_public_key}} Run an SSH configuration playbook against remote hosts through the SSH tunnel, providing the SSH password, sudo password and vault password: ansible-playbook -i inventory update_remote-ssh.yml --ask-pass --ask-sudo-pass --ask-vault-pass SSH password: sudo password [ defaults to SSH password ] : Vault password: PLAY [ update ssh ] ************************************************************* TASK: [ copy src = templates/etc_sudoers dest = /etc/sudoers mode = 0440 owner = root group = root ] *** ok: [ i-00000001 ] ok: [ i-00000002 ] ok: [ i-00000003 ] ok: [ i-00000004 ] TASK: [ user name = remoteuser groups = sudo shell = /bin/bash state = present ] *** ok: [ i-00000001 ] ok: [ i-00000002 ] ok: [ i-00000003 ] ok: [ i-00000004 ] TASK: [ authorized_key user = remoteuser state = present key = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\" ] *** ok: [ i-00000001 ] ok: [ i-00000002 ] ok: [ i-00000003 ] ok: [ i-00000004 ] PLAY RECAP ******************************************************************** i-00000001 : ok = 3 changed = 0 unreachable = 0 failed = 0 i-00000002 : ok = 3 changed = 0 unreachable = 0 failed = 0 i-00000003 : ok = 3 changed = 0 unreachable = 0 failed = 0 i-00000004 : ok = 3 changed = 0 unreachable = 0 failed = 0 The best way to keep Ansible output concise is to run without verbosity -- only crank this up if you need it to diagnose a problem.","title":"Ansible Vault and SSH Key Distribution"},{"location":"ansible/ansible-vault-and-ssh-key-distribution/#ansible-vault-and-ssh-key-distribution","text":"2014-06-30 Discuss There are two types of SSH key distribution discussed in this post: private keys on local hosts and public keys on remote hosts. SSH private key distribution is best used for setting up your own workstation or possibly an Ansible Tower server. In general, you should not be distributing private keys widely; with a good SSH tunneling configuration and SSH public key distribution, there should be no need for the private keys to be installed in more than few places. This configuration will show off a technique for configuring an SSH jump host bastion that allows you to keep your private key on your own workstation; there is no need to have the SSH private key on the bastion host. For the purpose of this post, I have generated a new SSH key pair to demonstrate this technique; this keypair is used nowhere. Part of the trick to making this work is that the private key needs to be base64 encoded so that line breaks are preserved when it is stored as a yaml string in the vars_files . Template files are created for the public and private keys to preserve file change detection. Generate a new key pair: $ ssh-keygen -b 2048 -f junk_key -C junk Generating public/private rsa key pair. Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in junk_key. Your public key has been saved in junk_key.pub. The key fingerprint is: 94 :94:ae:ac:c3:5e:ee:7d:fa:2c:cb:0f:ae:19:c8:99 junk The key ' s randomart image is: +-- [ RSA 2048 ] ----+ | .. | | ... | | .o | | .. | | . .S | | . +o | | .E.o . | | +o *.o. | | ..o = .*B+ | +-----------------+ </pre> Base64 encode the private key: base64 -i junk_key > junk_key.b64 Create an Ansible vars_files yaml data file named ssh_keys/ssh_key_vault.yml . The ssh_private_key variable should contain the base64 encoded private key and the ssh_public_key variable should contain the public key. Encrypt the file with ansible-vault : ssh_private_key : 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 ssh_public_key : ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk ansible-vault encrypt ssh_keys/ssh_key_vault.yml Vault password: Confirm Vault password: Encryption successful Create an inventory file named inventory , showing off the SSH jump host connection capability: [localhost] localhost ansible_connection = local [group-all:children] group-01 group-02 [group-01] i-00000001 ansible_ssh_host = bastion+192.168.1.1 ansible_ssh_user=remoteuser i-00000002 ansible_ssh_host = bastion+192.168.1.2 ansible_ssh_user=remoteuser [group-02] i-00000003 ansible_ssh_host = bastion+192.168.1.3 ansible_ssh_user=remoteuser i-00000004 ansible_ssh_host = bastion+192.168.1.4 ansible_ssh_user=remoteuser Create a template file for the private key named templates/HOME_.ssh_junk : {{ ssh_private_key_decoded.stdout }} Create a template file for the public key named templates/HOME_.ssh_junk.pub : {{ ssh_public_key }} Create a template file for the SSH jump host configuration named templates/HOME_.ssh_config : Host * ServerAliveInterval 30 ServerAliveCountMax 5 Host bastion User {{ remote_user }} IdentityFile ~/.ssh/junk Hostname bastion Host bastion+* User {{ remote_user }} IdentityFile ~/.ssh/junk ProxyCommand ssh -T -a bastion nc $(echo %h |cut -d+ -f2) %p 2>/dev/null StrictHostKeyChecking no This configuration assumes that you have a consistent remote username defined on the bastion server and your protected hosts. Write a playbook to install the SSH key and configuration on your local workstation named config_local-ssh.yml : - name : configure local ssh hosts : - localhost gather_facts : false sudo : false vars : local_home : \"{{ lookup('env','HOME') }}\" local_user : \"{{ lookup('env','USER') }}\" remote_user : remoteuser vars_files : - ssh_keys/ssh_key_vault.yml tasks : - file : path={{local_home}}/.ssh state=directory mode=0700 owner={{local_user}} - template : src=templates/HOME_.ssh_config dest={{local_home}}/.ssh/config mode=0644 owner={{local_user}} backup=yes - shell : echo {{ssh_private_key}} |base64 --decode register : ssh_private_key_decoded - template : src=templates/HOME_.ssh_junk dest={{local_home}}/.ssh/junk mode=0600 owner={{local_user}} - template : src=templates/HOME_.ssh_junk.pub dest={{local_home}}/.ssh/junk.pub mode=0644 owner={{local_user}} Run the playbook to setup your local workstation with SSH keys and configuration: ansible-playbook -i inventory config_local-ssh.yml --ask-vault-pass Vault password: Test your SSH tunneling access to a remote host behind the bastion server: ssh bastion+192.168.1.1 ssh bastion+192.168.1.1 \"date; date > /tmp/date.out\" scp bastion+192.168.1.1:/tmp/date.out . Notice that the hosts behind the bastion server are referenced in the SSH command the same way that they are referenced in the Ansible inventory file. The \"+\" character used as a separator was selected explicitly for its ability to be used interchangeably at the command line and in the Ansible inventory. IP addresses are being used on the right hand side of the expression since the secondary connection to the protected host relies on the name resolution capabilities of the first host in the tunnel. If you had a reliable dynamic DNS service that was keeping up with changes to the protected hosts and was accessible to the bastion host, then you could use host names instead, such as bastion+webserver01 . This host selection technique can be extended to an Ansible dynamic inventory script, if you were running instances at a cloud provider such as AWS. When you write a dynamic inventory script , the data format should look like this: { \"group-01\" : { \"hosts\" : [ \"i-00000001\" , \"i-00000002\" ] }, \"group-02\" : { \"hosts\" : [ \"i-00000003\" , \"i-00000004\" ] }, \"group-all\" : { \"children\" : [ \"group-01\" , \"group-02\" ] }, \"localhost\" : [ \"localhost\" ], \"_meta\" : { \"hostvars\" : { \"i-00000001\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.1\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"i-00000002\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.2\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"i-00000003\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.3\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"i-00000004\" : { \"ansible_ssh_host\" : \"bastion+192.168.1.4\" , \"ansible_ssh_user\" : \"remoteuser\" }, \"localhost\" : { \"ansible_connection\" : \"local\" }, } } } The nice thing about this style of SSH configuration is that you can have multiple bastion hosts in different locations and target the hosts behind each of them, provided that you give your bastion hosts different names. The method of accessing them is the same between direct SSH connections and Ansible execution. Once you have this infrastructure in place, you can start distributing public SSH keys to your protected hosts. Write a templates/etc_sudoers file that grants NOPASSWD access to the sudo group: Defaults env_reset Defaults mail_badpass Defaults secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" # Host alias specification # User alias specification # Cmnd alias specification # User privilege specification root ALL=(ALL:ALL) ALL # Allow members of group sudo to execute any command %sudo ALL=NOPASSWD: ALL #includedir /etc/sudoers.d Write a playbook update_remote-ssh.yml to configure NOPASSWD sudo access for your remote user and distribute SSH public keys on your remote hosts. This will allow subsequent playbook execution to operate more easily against your remote hosts. In order for this to work, the paramiko connection type must be used initially, so that the password can be requested once and re-used across all hosts. - name : update remote ssh hosts : - group-all gather_facts : false sudo : true connection : paramiko vars_files : - ssh_keys/ssh_key_vault.yml tasks : - copy : src=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root - user : name=remoteuser groups=sudo shell=/bin/bash state=present - authorized_key : user=remoteuser state=present key={{ssh_public_key}} Run an SSH configuration playbook against remote hosts through the SSH tunnel, providing the SSH password, sudo password and vault password: ansible-playbook -i inventory update_remote-ssh.yml --ask-pass --ask-sudo-pass --ask-vault-pass SSH password: sudo password [ defaults to SSH password ] : Vault password: PLAY [ update ssh ] ************************************************************* TASK: [ copy src = templates/etc_sudoers dest = /etc/sudoers mode = 0440 owner = root group = root ] *** ok: [ i-00000001 ] ok: [ i-00000002 ] ok: [ i-00000003 ] ok: [ i-00000004 ] TASK: [ user name = remoteuser groups = sudo shell = /bin/bash state = present ] *** ok: [ i-00000001 ] ok: [ i-00000002 ] ok: [ i-00000003 ] ok: [ i-00000004 ] TASK: [ authorized_key user = remoteuser state = present key = \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\" ] *** ok: [ i-00000001 ] ok: [ i-00000002 ] ok: [ i-00000003 ] ok: [ i-00000004 ] PLAY RECAP ******************************************************************** i-00000001 : ok = 3 changed = 0 unreachable = 0 failed = 0 i-00000002 : ok = 3 changed = 0 unreachable = 0 failed = 0 i-00000003 : ok = 3 changed = 0 unreachable = 0 failed = 0 i-00000004 : ok = 3 changed = 0 unreachable = 0 failed = 0 The best way to keep Ansible output concise is to run without verbosity -- only crank this up if you need it to diagnose a problem.","title":"Ansible Vault and SSH Key Distribution"},{"location":"ansible/running-ansible-playbooks-on-windows/","text":"Running Ansible Playbooks on Windows \u00b6 2014-06-29 Discuss But First, Some History \u00b6 In early 2006, running almost a thousand servers for Blackboard Product Development that were evenly distributed across Windows, Linux and Solaris, we needed an automation tool that would allow us to quickly deploy and configure the Blackboard Learning Management System (LMS). The real sticking point for us was managing the Windows ecosystem. The first commit of PuppetLabs Puppet occurred in April 2005 and the first tagged release was on Jan 3, 2006. The first commit of OpsCode Chef occurred in March 2008 and the first tagged release was on Jan 31, 2009. Needless to say, the modern configuration management ecosystem was much sparser in 2006 then as compared to today. Puppet was still very new and in the process of gaining mind share and adding functionality; it did not support Windows at first. CFEngine was available, but it also did not support Windows in the open source version. I worked with Dave Carter at the beginning of 2006 to develop our own in-house configuration management system that we called Fusion. It was based on Ant and Ant-Contrib . Since Blackboard was a Java based application, we always had one or more versions of JDKs installed on our systems as a part of the imaging or virtual machine cloning process, so picking a tool that ran on the JDK made sense and offered us the platform independence we needed. Dave developed a state machine with a socket listener that would accept XML-formatted messages and then kick off various tasks. I developed a library and property inheritance hierarchy for the the system, along with a parallel job execution client and added the set of scripts that deployed and configured the Blackboard LMS. I figured out that by cherry-picking a few key utilities out of the UnxUtils distribution , I could write Windows batch scripts in a manner similar to Linux bash scripts and this lent to a somewhat manageable level of consistency between the disparate operating systems without completely abandoning the hooks we needed for Windows. Motivation \u00b6 Anyone who has spoken with me in the past year about my work knows that Ansible is hands-down my favorite piece of software tooling. Using Ansible, I was able to effectively manage 500-odd Ubuntu Linux systems at Blackboard, half of which were deployed in a VPC at AWS and half of which were deployed in the Blackboard Managed Hosting data centers. Part of the challenge that we had at Blackboard in the Product Development department is that 30-40% of the several thousand servers used for development and testing were Windows, which made it difficult to choose a single configuration management system to rule them all. For the better part of a year, I kept saying that it was a terrible shame that Ansible did not support Windows and that Opscode Chef would probably be the best choice for configuration management of all systems, since it arguably had the best support for that platform in 2013. After meeting with Michael DeHaan , creator and CTO of Ansible, at Blackboard headquarters, we talked through their rough plans for Windows support. In short, it was something they wanted to be thoughtful about. Some time later, we had a hack day organized at Blackboard and I decided that I would attempt to develop an Ansible playbook that could install and uninstall a JDK on Windows, using my previous experience with building the Fusion configuration management system. It turns out that this works. Quite well. Even though it wasn't intended to. Ansible Roadmap Update \u00b6 On June 19, 2014, Michael DeHaan announced Windows Is Coming . PowerShell remoting is a far cleaner solution and I am looking forward to seeing it hit the release branch, although I don't have to worry about Windows machines so much these days. I learned this nifty fact from Ansible Weekly Issue 38 ; this is not a bad way to keep up on the latest Ansible news. Pre-Requisites \u00b6 Windows + Cygwin + SSHd + Python Playbooks \u00b6 Ansible inventory file hosts : [w7x64-jf] w7x64-jf.pd.local JDK7 installation playbook jdk7_install.yml : - name : install oracle jdk7 hosts : - w7x64-jf user : administrator gather_facts : false vars : version : 7u45 build : b18 version_padded : 1.7.0_45 dodrootca2 : c:\\\\jdk\\{{version_padded}}\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem cacerts : c:\\\\jdk{{version_padded}}\\\\jre\\\\lib\\\\security\\\\cacerts tasks : - command : wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/{{version}}-{{build}}/jdk-{{version}}-windows-x64.exe creates=/usr/local/src/jdk-{{version}}-windows-x64.exe - file : path=/usr/local/src/jdk-{{version}}-windows-x64.exe mode=0755 - shell : /usr/local/src/jdk-{{version}}-windows-x64.exe /s INSTALLDIR=c:\\\\jdk{{version_padded}} /INSTALLDIRPUBJRE=c:\\\\jre{{version_padded}} REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk{{version_padded}} - copy : src=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/dod.root-ca-2.pem - command : /cygdrive/c/jdk{{version_padded}}/bin/keytool -import -trustcacerts -alias dodrootca2 -file {{dodrootca2}} -keystore $cacerts -storepass changeit -noprompt ignore_errors : true - copy : src=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/US_export_policy.jar - copy : src=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/local_policy.jar - shell : /cygdrive/c/jdk{{version_padded}}/bin/java -version 2>&1 |head -1 |awk '{print $3}' |sed -e 's/\"//g' register : java_version - fail : msg=\"The Java version does not match the expected value {{ version_padded }}.\" when : \"'{{ java_version.stdout }}' != '{{ version_padded }}'\" JDK7 uninstallation playbook jdk7_uninstall.yml : - name : uninstall oracle jdk7 hosts : - w7x64-jf user : administrator gather_facts : false vars : version : 7u45 version_padded : 1.7.0_45 version_text : \"7 Update 45\" tasks : - template : src=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs - shell : cscript remove-programs.vbs |grep \"Java {{version_text}} (64-bit)\" chdir=/usr/local/src register : result ignore_errors : true - command : cscript remove-programs.vbs /uninstall \"Java {{version_text}} (64-bit)\" chdir=/usr/local/src when : result|success - shell : cscript remove-programs.vbs |grep \"Java SE Development Kit {{version_text}} (64-bit)\" chdir=/usr/local/src register : result ignore_errors : true - command : cscript remove-programs.vbs /uninstall \"Java SE Development Kit {{version_text}} (64-bit)\" chdir=/usr/local/src when : result|success - file : path={{item}} state=absent with_items : - /usr/local/src/jdk-{{version}}-windows-x64.exe - /usr/local/src/jdk-install-log.txt - /usr/local/src/remove-programs.vbs The remove-programs.vbs helper script: If Wscript . Arguments . Count = 0 Then inventory_software () ElseIf Wscript . Arguments . Count = 2 Then If Wscript . Arguments ( 0 ) = \"/uninstall\" Then 'Expecting: cscript remove-programs.vbs /uninstall \"Java(TM) 6 Update 26\" 'Expecting: cscript remove-programs.vbs /uninstall \"Java(TM) SE Development Kit 6 Update 26\" uninstall_software ( Wscript . Arguments ( 1 )) Else Wscript . Echo \"Usage: remove-programs.vbs [/uninstall \"\"software\"\"]\" End If Else Wscript . Echo \"Usage: remove-programs.vbs [/uninstall \"\"software\"\"]\" End If Sub inventory_software () strComputer = \".\" Set objWMIService = GetObject ( \"winmgmts:\" _ & \"{impersonationLevel=impersonate}!\\\\\" _ & strComputer & \"\\root\\cimv2\" ) Set colSoftware = objWMIService . ExecQuery _ ( \"Select * from Win32_Product\" ) For Each objSoftware in colSoftware Wscript . Echo \"Name: \" & objSoftware . Name 'Wscript.Echo \"Version: \" & objSoftware.Version Next End Sub Sub inventory_java_software () strComputer = \".\" Set objWMIService = GetObject ( \"winmgmts:\" _ & \"{impersonationLevel=impersonate}!\\\\\" _ & strComputer & \"\\root\\cimv2\" ) Set colSoftware = objWMIService . ExecQuery _ ( \"Select * from Win32_Product \" _ & \"Where Name Like 'Java%'\" ) For Each objSoftware in colSoftware Wscript . Echo \"Name: \" & objSoftware . Name Wscript . Echo \"Version: \" & objSoftware . Version Next End Sub Sub uninstall_software ( strApplicationName ) 'Make sure to run this with Administrator privileges strComputer = \".\" Set objWMIService = GetObject ( \"winmgmts:\" _ & \"{impersonationLevel=impersonate}!\\\\\" _ & strComputer & \"\\root\\cimv2\" ) Set colSoftware = objWMIService . ExecQuery _ ( \"Select * From Win32_Product Where Name = '\" _ & strApplicationName & \"'\" ) For Each objSoftware in colSoftware objSoftware . Uninstall () Next End Sub Demonstration \u00b6 File system state before install: administrator@W7X64-JF~ $ ls -l /cygdrive/c | egrep \"jdk|jre\" drwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7 Install JDK7 on Windows: copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_install.yml PLAY [ install oracle jdk7 ] **************************************************** TASK: [ command wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24 = http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-windows-x64.exe creates = /usr/local/src/jdk-7u45-windows-x64.exe ] *** changed: [ w7x64-jf.pd.local ] TASK: [ file path = /usr/local/src/jdk-7u45-windows-x64.exe mode = 0755 ] *********** changed: [ w7x64-jf.pd.local ] TASK: [ shell /usr/local/src/jdk-7u45-windows-x64.exe /s INSTALLDIR = c: \\\\ jdk1.7.0_45 /INSTALLDIRPUBJRE = c: \\\\ jre1.7.0_45 REBOOT = Suppress ADDLOCAL = ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE = 0 SYSTRAY = 0 SYSTRAY = 0 /L c: \\\\ cygwin \\\\ usr \\\\ local \\\\ src \\\\ jdk-install-log.txt creates = /cygdrive/c/jdk1.7.0_45 ] *** changed: [ w7x64-jf.pd.local ] TASK: [ copy src = ../certs/dod.root-ca-2.pem dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/dod.root-ca-2.pem ] *** changed: [ w7x64-jf.pd.local ] TASK: [ command /cygdrive/c/jdk1.7.0_45/bin/keytool -import -trustcacerts -alias dodrootca2 -file c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ dod.root-ca-2.pem -keystore c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ cacerts -storepass changeit -noprompt ] *** changed: [ w7x64-jf.pd.local ] TASK: [ copy src = files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/US_export_policy.jar ] *** changed: [ w7x64-jf.pd.local ] TASK: [ copy src = files/cygdrive_c_java_jre_lib_security_local_policy.jar dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/local_policy.jar ] *** changed: [ w7x64-jf.pd.local ] TASK: [ shell /cygdrive/c/jdk1.7.0_45/bin/java -version 2 > & 1 | head -1 | awk '{print $3}' | sed -e 's/\"//g' ] *** changed: [ w7x64-jf.pd.local ] TASK: [ fail msg = \"The Java version does not match the expected value 1.7.0_45.\" ] *** skipping: [ w7x64-jf.pd.local ] PLAY RECAP ******************************************************************** w7x64-jf.pd.local : ok = 8 changed = 8 unreachable = 0 failed = 0 File system state after install and before uninstall: administrator@W7X64-JF ~ $ ls -l /cygdrive/c | egrep \"jdk|jre\" drwx------+ 1 SYSTEM SYSTEM 0 Dec 12 23 :17 jdk1.7.0_45 drwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7 drwx------+ 1 SYSTEM SYSTEM 0 Dec 12 23 :17 jre1.7.0_45 Uninstall JDK7 on Windows: copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_uninstall.yml PLAY [ uninstall oracle jdk7 ] ************************************************** TASK: [ template src = templates/usr_local_src_remove-programs.vbs dest = /usr/local/src/remove-programs.vbs ] *** changed: [ w7x64-jf.pd.local ] TASK: [ shell cscript remove-programs.vbs | grep \"Java 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ command cscript remove-programs.vbs /uninstall \"Java 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ shell cscript remove-programs.vbs | grep \"Java SE Development Kit 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ command cscript remove-programs.vbs /uninstall \"Java SE Development Kit 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ file path = $item state = absent ] ****************************************** changed: [ w7x64-jf.pd.local ] = > ( item = /usr/local/src/jdk-7u45-windows-x64.exe ) changed: [ w7x64-jf.pd.local ] = > ( item = /usr/local/src/jdk-install-log.txt ) changed: [ w7x64-jf.pd.local ] = > ( item = /usr/local/src/remove-programs.vbs ) PLAY RECAP ******************************************************************** w7x64-jf.pd.local : ok = 6 changed = 6 unreachable = 0 failed = 0 File system state after uninstall: administrator@W7X64-JF ~ $ ls -l /cygdrive/c | egrep \"jdk|jre\" drwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7","title":"Running Ansible Playbooks on Windows"},{"location":"ansible/running-ansible-playbooks-on-windows/#running-ansible-playbooks-on-windows","text":"2014-06-29 Discuss","title":"Running Ansible Playbooks on Windows"},{"location":"ansible/running-ansible-playbooks-on-windows/#but-first-some-history","text":"In early 2006, running almost a thousand servers for Blackboard Product Development that were evenly distributed across Windows, Linux and Solaris, we needed an automation tool that would allow us to quickly deploy and configure the Blackboard Learning Management System (LMS). The real sticking point for us was managing the Windows ecosystem. The first commit of PuppetLabs Puppet occurred in April 2005 and the first tagged release was on Jan 3, 2006. The first commit of OpsCode Chef occurred in March 2008 and the first tagged release was on Jan 31, 2009. Needless to say, the modern configuration management ecosystem was much sparser in 2006 then as compared to today. Puppet was still very new and in the process of gaining mind share and adding functionality; it did not support Windows at first. CFEngine was available, but it also did not support Windows in the open source version. I worked with Dave Carter at the beginning of 2006 to develop our own in-house configuration management system that we called Fusion. It was based on Ant and Ant-Contrib . Since Blackboard was a Java based application, we always had one or more versions of JDKs installed on our systems as a part of the imaging or virtual machine cloning process, so picking a tool that ran on the JDK made sense and offered us the platform independence we needed. Dave developed a state machine with a socket listener that would accept XML-formatted messages and then kick off various tasks. I developed a library and property inheritance hierarchy for the the system, along with a parallel job execution client and added the set of scripts that deployed and configured the Blackboard LMS. I figured out that by cherry-picking a few key utilities out of the UnxUtils distribution , I could write Windows batch scripts in a manner similar to Linux bash scripts and this lent to a somewhat manageable level of consistency between the disparate operating systems without completely abandoning the hooks we needed for Windows.","title":"But First, Some History"},{"location":"ansible/running-ansible-playbooks-on-windows/#motivation","text":"Anyone who has spoken with me in the past year about my work knows that Ansible is hands-down my favorite piece of software tooling. Using Ansible, I was able to effectively manage 500-odd Ubuntu Linux systems at Blackboard, half of which were deployed in a VPC at AWS and half of which were deployed in the Blackboard Managed Hosting data centers. Part of the challenge that we had at Blackboard in the Product Development department is that 30-40% of the several thousand servers used for development and testing were Windows, which made it difficult to choose a single configuration management system to rule them all. For the better part of a year, I kept saying that it was a terrible shame that Ansible did not support Windows and that Opscode Chef would probably be the best choice for configuration management of all systems, since it arguably had the best support for that platform in 2013. After meeting with Michael DeHaan , creator and CTO of Ansible, at Blackboard headquarters, we talked through their rough plans for Windows support. In short, it was something they wanted to be thoughtful about. Some time later, we had a hack day organized at Blackboard and I decided that I would attempt to develop an Ansible playbook that could install and uninstall a JDK on Windows, using my previous experience with building the Fusion configuration management system. It turns out that this works. Quite well. Even though it wasn't intended to.","title":"Motivation"},{"location":"ansible/running-ansible-playbooks-on-windows/#ansible-roadmap-update","text":"On June 19, 2014, Michael DeHaan announced Windows Is Coming . PowerShell remoting is a far cleaner solution and I am looking forward to seeing it hit the release branch, although I don't have to worry about Windows machines so much these days. I learned this nifty fact from Ansible Weekly Issue 38 ; this is not a bad way to keep up on the latest Ansible news.","title":"Ansible Roadmap Update"},{"location":"ansible/running-ansible-playbooks-on-windows/#pre-requisites","text":"Windows + Cygwin + SSHd + Python","title":"Pre-Requisites"},{"location":"ansible/running-ansible-playbooks-on-windows/#playbooks","text":"Ansible inventory file hosts : [w7x64-jf] w7x64-jf.pd.local JDK7 installation playbook jdk7_install.yml : - name : install oracle jdk7 hosts : - w7x64-jf user : administrator gather_facts : false vars : version : 7u45 build : b18 version_padded : 1.7.0_45 dodrootca2 : c:\\\\jdk\\{{version_padded}}\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem cacerts : c:\\\\jdk{{version_padded}}\\\\jre\\\\lib\\\\security\\\\cacerts tasks : - command : wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/{{version}}-{{build}}/jdk-{{version}}-windows-x64.exe creates=/usr/local/src/jdk-{{version}}-windows-x64.exe - file : path=/usr/local/src/jdk-{{version}}-windows-x64.exe mode=0755 - shell : /usr/local/src/jdk-{{version}}-windows-x64.exe /s INSTALLDIR=c:\\\\jdk{{version_padded}} /INSTALLDIRPUBJRE=c:\\\\jre{{version_padded}} REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk{{version_padded}} - copy : src=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/dod.root-ca-2.pem - command : /cygdrive/c/jdk{{version_padded}}/bin/keytool -import -trustcacerts -alias dodrootca2 -file {{dodrootca2}} -keystore $cacerts -storepass changeit -noprompt ignore_errors : true - copy : src=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/US_export_policy.jar - copy : src=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/local_policy.jar - shell : /cygdrive/c/jdk{{version_padded}}/bin/java -version 2>&1 |head -1 |awk '{print $3}' |sed -e 's/\"//g' register : java_version - fail : msg=\"The Java version does not match the expected value {{ version_padded }}.\" when : \"'{{ java_version.stdout }}' != '{{ version_padded }}'\" JDK7 uninstallation playbook jdk7_uninstall.yml : - name : uninstall oracle jdk7 hosts : - w7x64-jf user : administrator gather_facts : false vars : version : 7u45 version_padded : 1.7.0_45 version_text : \"7 Update 45\" tasks : - template : src=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs - shell : cscript remove-programs.vbs |grep \"Java {{version_text}} (64-bit)\" chdir=/usr/local/src register : result ignore_errors : true - command : cscript remove-programs.vbs /uninstall \"Java {{version_text}} (64-bit)\" chdir=/usr/local/src when : result|success - shell : cscript remove-programs.vbs |grep \"Java SE Development Kit {{version_text}} (64-bit)\" chdir=/usr/local/src register : result ignore_errors : true - command : cscript remove-programs.vbs /uninstall \"Java SE Development Kit {{version_text}} (64-bit)\" chdir=/usr/local/src when : result|success - file : path={{item}} state=absent with_items : - /usr/local/src/jdk-{{version}}-windows-x64.exe - /usr/local/src/jdk-install-log.txt - /usr/local/src/remove-programs.vbs The remove-programs.vbs helper script: If Wscript . Arguments . Count = 0 Then inventory_software () ElseIf Wscript . Arguments . Count = 2 Then If Wscript . Arguments ( 0 ) = \"/uninstall\" Then 'Expecting: cscript remove-programs.vbs /uninstall \"Java(TM) 6 Update 26\" 'Expecting: cscript remove-programs.vbs /uninstall \"Java(TM) SE Development Kit 6 Update 26\" uninstall_software ( Wscript . Arguments ( 1 )) Else Wscript . Echo \"Usage: remove-programs.vbs [/uninstall \"\"software\"\"]\" End If Else Wscript . Echo \"Usage: remove-programs.vbs [/uninstall \"\"software\"\"]\" End If Sub inventory_software () strComputer = \".\" Set objWMIService = GetObject ( \"winmgmts:\" _ & \"{impersonationLevel=impersonate}!\\\\\" _ & strComputer & \"\\root\\cimv2\" ) Set colSoftware = objWMIService . ExecQuery _ ( \"Select * from Win32_Product\" ) For Each objSoftware in colSoftware Wscript . Echo \"Name: \" & objSoftware . Name 'Wscript.Echo \"Version: \" & objSoftware.Version Next End Sub Sub inventory_java_software () strComputer = \".\" Set objWMIService = GetObject ( \"winmgmts:\" _ & \"{impersonationLevel=impersonate}!\\\\\" _ & strComputer & \"\\root\\cimv2\" ) Set colSoftware = objWMIService . ExecQuery _ ( \"Select * from Win32_Product \" _ & \"Where Name Like 'Java%'\" ) For Each objSoftware in colSoftware Wscript . Echo \"Name: \" & objSoftware . Name Wscript . Echo \"Version: \" & objSoftware . Version Next End Sub Sub uninstall_software ( strApplicationName ) 'Make sure to run this with Administrator privileges strComputer = \".\" Set objWMIService = GetObject ( \"winmgmts:\" _ & \"{impersonationLevel=impersonate}!\\\\\" _ & strComputer & \"\\root\\cimv2\" ) Set colSoftware = objWMIService . ExecQuery _ ( \"Select * From Win32_Product Where Name = '\" _ & strApplicationName & \"'\" ) For Each objSoftware in colSoftware objSoftware . Uninstall () Next End Sub","title":"Playbooks"},{"location":"ansible/running-ansible-playbooks-on-windows/#demonstration","text":"File system state before install: administrator@W7X64-JF~ $ ls -l /cygdrive/c | egrep \"jdk|jre\" drwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7 Install JDK7 on Windows: copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_install.yml PLAY [ install oracle jdk7 ] **************************************************** TASK: [ command wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24 = http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-windows-x64.exe creates = /usr/local/src/jdk-7u45-windows-x64.exe ] *** changed: [ w7x64-jf.pd.local ] TASK: [ file path = /usr/local/src/jdk-7u45-windows-x64.exe mode = 0755 ] *********** changed: [ w7x64-jf.pd.local ] TASK: [ shell /usr/local/src/jdk-7u45-windows-x64.exe /s INSTALLDIR = c: \\\\ jdk1.7.0_45 /INSTALLDIRPUBJRE = c: \\\\ jre1.7.0_45 REBOOT = Suppress ADDLOCAL = ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE = 0 SYSTRAY = 0 SYSTRAY = 0 /L c: \\\\ cygwin \\\\ usr \\\\ local \\\\ src \\\\ jdk-install-log.txt creates = /cygdrive/c/jdk1.7.0_45 ] *** changed: [ w7x64-jf.pd.local ] TASK: [ copy src = ../certs/dod.root-ca-2.pem dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/dod.root-ca-2.pem ] *** changed: [ w7x64-jf.pd.local ] TASK: [ command /cygdrive/c/jdk1.7.0_45/bin/keytool -import -trustcacerts -alias dodrootca2 -file c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ dod.root-ca-2.pem -keystore c: \\\\ jdk1.7.0_45 \\\\ jre \\\\ lib \\\\ security \\\\ cacerts -storepass changeit -noprompt ] *** changed: [ w7x64-jf.pd.local ] TASK: [ copy src = files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/US_export_policy.jar ] *** changed: [ w7x64-jf.pd.local ] TASK: [ copy src = files/cygdrive_c_java_jre_lib_security_local_policy.jar dest = /cygdrive/c/jdk1.7.0_45/jre/lib/security/local_policy.jar ] *** changed: [ w7x64-jf.pd.local ] TASK: [ shell /cygdrive/c/jdk1.7.0_45/bin/java -version 2 > & 1 | head -1 | awk '{print $3}' | sed -e 's/\"//g' ] *** changed: [ w7x64-jf.pd.local ] TASK: [ fail msg = \"The Java version does not match the expected value 1.7.0_45.\" ] *** skipping: [ w7x64-jf.pd.local ] PLAY RECAP ******************************************************************** w7x64-jf.pd.local : ok = 8 changed = 8 unreachable = 0 failed = 0 File system state after install and before uninstall: administrator@W7X64-JF ~ $ ls -l /cygdrive/c | egrep \"jdk|jre\" drwx------+ 1 SYSTEM SYSTEM 0 Dec 12 23 :17 jdk1.7.0_45 drwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7 drwx------+ 1 SYSTEM SYSTEM 0 Dec 12 23 :17 jre1.7.0_45 Uninstall JDK7 on Windows: copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_uninstall.yml PLAY [ uninstall oracle jdk7 ] ************************************************** TASK: [ template src = templates/usr_local_src_remove-programs.vbs dest = /usr/local/src/remove-programs.vbs ] *** changed: [ w7x64-jf.pd.local ] TASK: [ shell cscript remove-programs.vbs | grep \"Java 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ command cscript remove-programs.vbs /uninstall \"Java 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ shell cscript remove-programs.vbs | grep \"Java SE Development Kit 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ command cscript remove-programs.vbs /uninstall \"Java SE Development Kit 7 Update 45 (64-bit)\" chdir = /usr/local/src ] *** changed: [ w7x64-jf.pd.local ] TASK: [ file path = $item state = absent ] ****************************************** changed: [ w7x64-jf.pd.local ] = > ( item = /usr/local/src/jdk-7u45-windows-x64.exe ) changed: [ w7x64-jf.pd.local ] = > ( item = /usr/local/src/jdk-install-log.txt ) changed: [ w7x64-jf.pd.local ] = > ( item = /usr/local/src/remove-programs.vbs ) PLAY RECAP ******************************************************************** w7x64-jf.pd.local : ok = 6 changed = 6 unreachable = 0 failed = 0 File system state after uninstall: administrator@W7X64-JF ~ $ ls -l /cygdrive/c | egrep \"jdk|jre\" drwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6 drwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7","title":"Demonstration"},{"location":"ansible/testing-ansible-galaxy-roles-with-docker/","text":"Testing Ansible Galaxy Roles with Docker \u00b6 2014-10-14 Discuss I learned a neat trick for testing Ansible Galaxy roles at AnsibleFest 2014 . To get started with Docker on your Mac, install VirtualBox and then install boot2docker , using Homebrew . The boot2docker package will install docker as a dependency and it runs a small (24MB) Linux VirtualBox virtual machine that provides a platform for running Docker images: brew install boot2docker boot2docker init boot2docker up $(boot2docker shellinit) The Ansible Team (thanks, Toshio! ) has released Docker images that are included in the Docker Hub Registry , which can be used for testing: docker search ansible |grep ^ansible ansible/ubuntu14.04-ansible Ubuntu 14.04 LTS with ansible 12 ansible/centos7-ansible Ansible on Centos7 11 There are two tags associated with each of these images: latest and devel. The latest contains a layered Ansible stable and devel contains a layered Ansible HEAD. Pull one of these images from the Docker Hub: docker pull ansible/ubuntu14.04-ansible This technique will allow you to rinse and repeat installs with different roles, which will help you figure out which ones deliver the most suitable functionality for your use cases. Launch the docker image and leave a shell process running: docker run -i -t ansible/ubuntu14.04-ansible:stable /bin/bash Download a role from Ansible Galaxy: ansible-galaxy install geerlingguy.memcached Create a local site.yml playbook to run the role: - hosts: localhost roles: - role: geerlingguy.memcached Execute the playbook: root@ccca9e7f365f:/# ansible-playbook site.yml -c local PLAY [localhost] ************************************************************** GATHERING FACTS *************************************************************** ok: [localhost] TASK: [geerlingguy.memcached | Install Memcached.] **************************** skipping: [localhost] TASK: [geerlingguy.memcached | Update apt cache.] ***************************** ok: [localhost] TASK: [geerlingguy.memcached | Install Memcached.] **************************** changed: [localhost] TASK: [geerlingguy.memcached | Copy Memcached configuration.] ***************** changed: [localhost] TASK: [geerlingguy.memcached | Ensure Memcached is started and set to run on startup.] *** changed: [localhost] NOTIFIED: [geerlingguy.memcached | restart memcached] ************************* changed: [localhost] PLAY RECAP ******************************************************************** localhost : ok=6 changed=4 unreachable=0 failed=0 root@ccca9e7f365f:/#","title":"Testing Ansible Galaxy Roles with Docker"},{"location":"ansible/testing-ansible-galaxy-roles-with-docker/#testing-ansible-galaxy-roles-with-docker","text":"2014-10-14 Discuss I learned a neat trick for testing Ansible Galaxy roles at AnsibleFest 2014 . To get started with Docker on your Mac, install VirtualBox and then install boot2docker , using Homebrew . The boot2docker package will install docker as a dependency and it runs a small (24MB) Linux VirtualBox virtual machine that provides a platform for running Docker images: brew install boot2docker boot2docker init boot2docker up $(boot2docker shellinit) The Ansible Team (thanks, Toshio! ) has released Docker images that are included in the Docker Hub Registry , which can be used for testing: docker search ansible |grep ^ansible ansible/ubuntu14.04-ansible Ubuntu 14.04 LTS with ansible 12 ansible/centos7-ansible Ansible on Centos7 11 There are two tags associated with each of these images: latest and devel. The latest contains a layered Ansible stable and devel contains a layered Ansible HEAD. Pull one of these images from the Docker Hub: docker pull ansible/ubuntu14.04-ansible This technique will allow you to rinse and repeat installs with different roles, which will help you figure out which ones deliver the most suitable functionality for your use cases. Launch the docker image and leave a shell process running: docker run -i -t ansible/ubuntu14.04-ansible:stable /bin/bash Download a role from Ansible Galaxy: ansible-galaxy install geerlingguy.memcached Create a local site.yml playbook to run the role: - hosts: localhost roles: - role: geerlingguy.memcached Execute the playbook: root@ccca9e7f365f:/# ansible-playbook site.yml -c local PLAY [localhost] ************************************************************** GATHERING FACTS *************************************************************** ok: [localhost] TASK: [geerlingguy.memcached | Install Memcached.] **************************** skipping: [localhost] TASK: [geerlingguy.memcached | Update apt cache.] ***************************** ok: [localhost] TASK: [geerlingguy.memcached | Install Memcached.] **************************** changed: [localhost] TASK: [geerlingguy.memcached | Copy Memcached configuration.] ***************** changed: [localhost] TASK: [geerlingguy.memcached | Ensure Memcached is started and set to run on startup.] *** changed: [localhost] NOTIFIED: [geerlingguy.memcached | restart memcached] ************************* changed: [localhost] PLAY RECAP ******************************************************************** localhost : ok=6 changed=4 unreachable=0 failed=0 root@ccca9e7f365f:/#","title":"Testing Ansible Galaxy Roles with Docker"},{"location":"aws/assume-role-with-awscli/","text":"Assume Role with AWSCLI \u00b6 2016-05-19 Discuss If you have either static or instance profile credentials that grant you STS permissions, then you can gather a set of time-limited role credentials as follows: #!/bin/bash TEST_CREDENTIALS=$( \\ aws sts assume-role \\ --role-arn arn:aws:iam::$AWS_ACCOUNT_ID_1:role/$ROLE_NAME \\ --role-session-name $USER \\ |jq '.Credentials' ) PROD_CREDENTIALS=$( \\ aws sts assume-role \\ --role-arn arn:aws:iam::$AWS_ACCOUNT_ID_2:role/$ROLE_NAME \\ --role-session-name $USER \\ |jq '.Credentials' ) cat >>$HOME/.aws/credentials <<EOF [test-$ROLE_NAME] aws_access_key_id=$(echo $TEST_CREDENTIALS |jq -r '.AccessKeyId') aws_secret_access_key=$(echo $TEST_CREDENTIALS |jq -r '.SecretAccessKey') aws_session_token=$(echo $TEST_CREDENTIALS |jq -r '.SessionToken') expiration=$(echo $TEST_CREDENTIALS |jq -r '.Expiration') [prod-$ROLE_NAME] aws_access_key_id=$(echo $PROD_CREDENTIALS |jq -r '.AccessKeyId') aws_secret_access_key=$(echo $PROD_CREDENTIALS |jq -r '.SecretAccessKey') aws_session_token=$(echo $PROD_CREDENTIALS |jq -r '.SessionToken') expiration=$(echo $PROD_CREDENTIALS |jq -r '.Expiration') EOF","title":"Assume Role with AWSCLI"},{"location":"aws/assume-role-with-awscli/#assume-role-with-awscli","text":"2016-05-19 Discuss If you have either static or instance profile credentials that grant you STS permissions, then you can gather a set of time-limited role credentials as follows: #!/bin/bash TEST_CREDENTIALS=$( \\ aws sts assume-role \\ --role-arn arn:aws:iam::$AWS_ACCOUNT_ID_1:role/$ROLE_NAME \\ --role-session-name $USER \\ |jq '.Credentials' ) PROD_CREDENTIALS=$( \\ aws sts assume-role \\ --role-arn arn:aws:iam::$AWS_ACCOUNT_ID_2:role/$ROLE_NAME \\ --role-session-name $USER \\ |jq '.Credentials' ) cat >>$HOME/.aws/credentials <<EOF [test-$ROLE_NAME] aws_access_key_id=$(echo $TEST_CREDENTIALS |jq -r '.AccessKeyId') aws_secret_access_key=$(echo $TEST_CREDENTIALS |jq -r '.SecretAccessKey') aws_session_token=$(echo $TEST_CREDENTIALS |jq -r '.SessionToken') expiration=$(echo $TEST_CREDENTIALS |jq -r '.Expiration') [prod-$ROLE_NAME] aws_access_key_id=$(echo $PROD_CREDENTIALS |jq -r '.AccessKeyId') aws_secret_access_key=$(echo $PROD_CREDENTIALS |jq -r '.SecretAccessKey') aws_session_token=$(echo $PROD_CREDENTIALS |jq -r '.SessionToken') expiration=$(echo $PROD_CREDENTIALS |jq -r '.Expiration') EOF","title":"Assume Role with AWSCLI"},{"location":"aws/aws-credential-files-for-java-and-python/","text":"AWS Credential Files for Java and Python \u00b6 2014-07-14 Discuss Each of the AWS tools has slightly different expectations about the location and naming of the credentials file and the various properties within it. It seems like the Python tools are moving closer to the Java standard as they iterate through releases, but it is still necessary to use a patchwork solution to be able to have a unified credentials file. Java SDK \u00b6 Version: \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\" Installation: sbt Link: AWS Java SDK Class ProfilesConfigFile The standard location for the credentials file is ~/.aws/credentials , which can be overridden with the AWS_CREDENTIAL_PROFILES_FILE environment variable or by specifying an alternate file location in the constructor. The format of this file is described below: [default] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken [test-user] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken Java Command Line Tools \u00b6 Version: 1.6.13.0 Installation: brew install ec2-api-tools Link: Setting Up the Amazon EC2 Command Line Interface Tools on Linux/Unix and Mac OS X The standard configuration is to use environment variables, since these tools have not been updated to read the standard AWS credentials file. Add the following to your ~/.bash_profile , to link the required data to the standard credentials file and allow for session tokens: export AWS_CREDENTIAL_FILE=\"$HOME/.aws/credentials\" export AWS_ACCESS_KEY=\"$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_SECRET_KEY=\"$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_DELEGATION_TOKEN=\"$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\" Python Boto \u00b6 Version: boto==2.31.1 botocore==0.56.0 Installation: pip install boto Link: Boto Config The latest version of boto needs to have aws_security_token defined, rather than aws_session_token , in the credentials file. The simplest solution for this is to duplicate the token between both names; the Java SDK will throw the following log message when reading the extra property, but will work as expected: INFO: Skip unsupported property name aws_security_token in profile [default]. Boto will not throw log messages about the existence of the aws_session_token property. AWS CLI \u00b6 Version: aws-cli/1.3.22 Installation: pip install awscli Link: Configuring the AWS Command Line Interface The standard location for the credentials file is ~/.aws/config , which can be overridden with the AWS_CREDENTIAL_FILE environment variable. The latest version of this tool accepts the Java SDK credential file format as-is, including the use of aws_session_token , whereas previous versions wanted aws_security_token instead. When you have multiple profiles in the credentials file, you can select a profile with the tool like so: aws --profile test-user s3 ls Unified Solution \u00b6 The best approach for creating a unified credentials file is to follow the Java credentials file format as closely as possible, while redirecting the Python tools to that file and adding properties to cover the corner cases. To do this, create a ~/.aws/credentials file that duplicates the necessary properties: [default] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken aws_security_token=testSessionToken [test-user] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken aws_security_token=testSessionToken [prod-user] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken aws_security_token=testSessionToken And add a section to your ~/.bash_profile : export AWS_CREDENTIAL_FILE=\"$HOME/.aws/credentials\" export AWS_ACCESS_KEY=\"$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_SECRET_KEY=\"$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_DELEGATION_TOKEN=\"$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\" With this configuration, you should be able to move seamlessly between the various Java and Python tools available for AWS.","title":"AWS Credential Files for Java and Python"},{"location":"aws/aws-credential-files-for-java-and-python/#aws-credential-files-for-java-and-python","text":"2014-07-14 Discuss Each of the AWS tools has slightly different expectations about the location and naming of the credentials file and the various properties within it. It seems like the Python tools are moving closer to the Java standard as they iterate through releases, but it is still necessary to use a patchwork solution to be able to have a unified credentials file.","title":"AWS Credential Files for Java and Python"},{"location":"aws/aws-credential-files-for-java-and-python/#java-sdk","text":"Version: \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\" Installation: sbt Link: AWS Java SDK Class ProfilesConfigFile The standard location for the credentials file is ~/.aws/credentials , which can be overridden with the AWS_CREDENTIAL_PROFILES_FILE environment variable or by specifying an alternate file location in the constructor. The format of this file is described below: [default] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken [test-user] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken","title":"Java SDK"},{"location":"aws/aws-credential-files-for-java-and-python/#java-command-line-tools","text":"Version: 1.6.13.0 Installation: brew install ec2-api-tools Link: Setting Up the Amazon EC2 Command Line Interface Tools on Linux/Unix and Mac OS X The standard configuration is to use environment variables, since these tools have not been updated to read the standard AWS credentials file. Add the following to your ~/.bash_profile , to link the required data to the standard credentials file and allow for session tokens: export AWS_CREDENTIAL_FILE=\"$HOME/.aws/credentials\" export AWS_ACCESS_KEY=\"$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_SECRET_KEY=\"$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_DELEGATION_TOKEN=\"$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\"","title":"Java Command Line Tools"},{"location":"aws/aws-credential-files-for-java-and-python/#python-boto","text":"Version: boto==2.31.1 botocore==0.56.0 Installation: pip install boto Link: Boto Config The latest version of boto needs to have aws_security_token defined, rather than aws_session_token , in the credentials file. The simplest solution for this is to duplicate the token between both names; the Java SDK will throw the following log message when reading the extra property, but will work as expected: INFO: Skip unsupported property name aws_security_token in profile [default]. Boto will not throw log messages about the existence of the aws_session_token property.","title":"Python Boto"},{"location":"aws/aws-credential-files-for-java-and-python/#aws-cli","text":"Version: aws-cli/1.3.22 Installation: pip install awscli Link: Configuring the AWS Command Line Interface The standard location for the credentials file is ~/.aws/config , which can be overridden with the AWS_CREDENTIAL_FILE environment variable. The latest version of this tool accepts the Java SDK credential file format as-is, including the use of aws_session_token , whereas previous versions wanted aws_security_token instead. When you have multiple profiles in the credentials file, you can select a profile with the tool like so: aws --profile test-user s3 ls","title":"AWS CLI"},{"location":"aws/aws-credential-files-for-java-and-python/#unified-solution","text":"The best approach for creating a unified credentials file is to follow the Java credentials file format as closely as possible, while redirecting the Python tools to that file and adding properties to cover the corner cases. To do this, create a ~/.aws/credentials file that duplicates the necessary properties: [default] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken aws_security_token=testSessionToken [test-user] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken aws_security_token=testSessionToken [prod-user] aws_access_key_id=testAccessKey aws_secret_access_key=testSecretKey aws_session_token=testSessionToken aws_security_token=testSessionToken And add a section to your ~/.bash_profile : export AWS_CREDENTIAL_FILE=\"$HOME/.aws/credentials\" export AWS_ACCESS_KEY=\"$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_SECRET_KEY=\"$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\" export AWS_DELEGATION_TOKEN=\"$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\" With this configuration, you should be able to move seamlessly between the various Java and Python tools available for AWS.","title":"Unified Solution"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/","text":"Build a Fake Instance Metadata Server for Ubuntu on Vagrant \u00b6 2014-07-22 Discuss Let's say that you have a proper build/bake/deploy pipeline in place for running appications in AWS. This is a reliable way to deploy applications at scale and move traffic between different versions of applications as a part of the deployment pipeline. However, the whole process may take 20-40 minutes or so to complete for any particular build. If you want to iterate more rapidly on your development efforts, you could skip the full process with a quickpatch ssh/rsync deployment to a single server or you could stand up a local Vagrant base image and iterate on that. Now let's say that the application you are working on is intended to work with instance metadata, particularly for the purpose of obtaining a rotating set of access and secret keys. It might be nice to have a fake metadata service running on your local Vagrant image so that you can test your application in a manner similar to how it will be running in the cloud. In this post, I describe how to build and configure a fake metadata service for an Ubuntu image running on Vagrant. Package Layout \u00b6 This layout assumes that you will be installing custom packages to the /apps directory, and there is a daemontools service hierarchy located at /service . The fake-metadata/service/run file is a script suitable for use with daemontools. root/ \u251c\u2500\u2500 apps \u2502 \u2514\u2500\u2500 fake-metadata \u2502 \u251c\u2500\u2500 app.py \u2502 \u2514\u2500\u2500 service \u2502 \u2514\u2500\u2500 run \u2514\u2500\u2500 etc \u251c\u2500\u2500 init.d \u2502 \u2514\u2500\u2500 fake-metadata \u251c\u2500\u2500 logrotate.d \u2502 \u2514\u2500\u2500 fake-metadata \u2514\u2500\u2500 network \u2514\u2500\u2500 iptables.rules Building and Packaging \u00b6 For cross-platform build packaging, it will be easiest to use the nebula ospackage plugin with Gradle. With this plugin available, your build script will look something like this: apply plugin: 'nebula-ospackage' ospackage { version = '1.0' packageName = 'fake-metadata' requires ( 'python-flask' ) link ( '/apps/fake-metadata/logs' , '/mnt/logs/fake-metadata' ) link ( '/service/fake-metadata' , '/apps/fake-metadata/service' ) } buildDeb { postInstall file ( 'scripts/postInstall.sh' ) preUninstall 'svc -d /service/fake-metadata' postUninstall file ( 'scripts/postUninstall.sh' ) } task build ( dependsOn: [ 'buildDeb' ]) Fake Metadata Application \u00b6 The simplest approach to building the service is to create a Flask service and have it run bare on the Vagrant instance. Given how small it will be and limited amount of traffic it will need to serve, there is no need to run this behind a dedicated static webserver like nginx or Apache. The nice thing about using Flask and having a basic structure in place is that it is then easy to extend the application to add other endpoints when needed. #!/usr/bin/env python from flask import Flask, jsonify, abort, make_response, request import os import sys app = Flask(__name__) BaseIAMRole = { 'Code': 'Success', 'LastUpdated' : '', 'Type': 'AWS-HMAC', 'AccessKeyId': '', 'SecretAccessKey': '', 'Token': '', 'Expiration': '' } @app.route('/', methods = ['GET']) def index(): return 'latest' @app.route('/latest/', methods = ['GET']) def latest(): return 'meta-data' @app.route('/latest/meta-data/', methods = ['GET']) def meta_data(): endpoints = [ 'iam', 'public-hostname', 'public-ipv4' ] return ('\\n').join(endpoints) @app.route('/latest/meta-data/iam/', methods = ['GET']) def iam(): return 'security-credentials' @app.route('/latest/meta-data/iam/security-credentials/', methods = ['GET']) def security_credentials(): return 'BaseIAMRole' @app.route('/latest/meta-data/iam/security-credentials/BaseIAMRole', methods = ['GET']) def base_iam_role(): # update the payload to contain a current set of accesss and secrey keys return jsonify(BaseIAMRole) @app.route('/latest/meta-data/public-hostname', methods = ['GET']) def public_hostname(): return os.environ['EC2_LOCAL_HOSTNAME'] @app.route('/latest/meta-data/public-ipv4', methods = ['GET']) def public_ipv4(): return os.environ['EC2_LOCAL_IPV4'] @app.errorhandler(400) def not_found(error): return make_response(jsonify( { 'error': 'bad request' } ), 400) @app.errorhandler(404) def not_found(error): return make_response(jsonify( { 'error': 'not found' } ), 404) @app.errorhandler(500) def not_found(error): return make_response(jsonify( { 'error': 'internal server error' } ), 500) if __name__ == '__main__': if len(sys.argv) > 1: if ':' in sys.argv[1]: host=sys.argv[1].split(':')[0] port=int(sys.argv[1].split(':')[1]) app.run(host=host, port=port) else: app.run(host=sys.argv[1]) else: app.run(debug=True) Daemontools Run Script \u00b6 This script is watched by the supervise process, which then starts (or restarts) the application if it is not running. Switching to a non-root user and redirecting output to the log file occurs here. #!/bin/bash ulimit -n 32768 source /etc/profile.d/environment.sh export PATH=/command:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/scripts if [ ! -d \"/mnt/logs/fake-metadata\" ]; then mkdir -p /mnt/logs/fake-metadata chmod 0777 /mnt/logs/fake-metadata fi USER=someuser PYTHON=/usr/bin/python APP=/apps/fake-metadata/app.py OPTS=127.0.0.1:8000 LOG=/mnt/logs/fake-metadata/server.log echo \"starting fake-metadata\" exec setuidgid $USER $PYTHON $APP $OPTS >> $LOG 2>&1 PostInstall Script \u00b6 This is where most of the trickiness occurs. The post install script is responsible for modifying the /etc/network/interfaces file, adding the metadata server IP address and configuring iptables in an idempotent manner. When the post install script is packaged by the nebula ospackage plugin into a Debian package, it gets a #!/bin/sh -e shebang invocation, which means that the script will halt execution at any point where it evaluates a non-zero return code. This means that the script needs to be written such a way that the environment state testing being done always returns a true value so that the script does not fail, hence the ||true constructs. We are attaching an extra IP address to the loopback interface, so we need to redirect traffic from 169.254.169.254:80 to the location where the fake metadata server is running. We are dealing with the loopback interface, which means that the PREROUTING nat table is never hit and we must use the OUTPUT table instead. You cannot DNAT packets destined for the loopback interface, because the kernel will treat them as martians and drop them, so you must REDIRECT the packets to the desired port. When performing the redirection from port 80 to 8000 on the loopback interface, it sends the packets to 127.0.0.1:8000, not 169.254.169.254:8000, so the fake metadata server must be listening on localhost port 8000. If for some reason, you need to troubleshoot the post install script, it can be found at /var/lib/dpkg/info/fake-metadata.postinst following an attempted package installation. LINE = $( grep 169 .254.169.254/32 /etc/network/interfaces || true ) if [[ ! \" $LINE \" == *169.254.169.254/32* ]] ; then sed -i '/iface lo inet loopback/a up ip addr add 169.254.169.254/32 dev lo scope host' /etc/network/interfaces fi LINE = $( /sbin/ip addr | grep 169 .254.169.254/32 || true ) if [[ ! \" $LINE \" == *169.254.169.254/32* ]] ; then /sbin/ip addr add 169 .254.169.254/32 dev lo scope host fi LINE = $( grep iptables-restore /etc/network/interfaces || true ) if [[ ! \" $LINE \" == *iptables-restore* ]] ; then sed -i '/up ip addr add/a pre-up iptables-restore < /etc/network/iptables.rules' /etc/network/interfaces fi LINE = $( iptables -t nat -L | grep 8000 || true ) if [[ ! \" $LINE \" == *8000* ]] ; then iptables -t nat -A OUTPUT -p tcp -d 169 .254.169.254/32 --dport 80 -j REDIRECT --to-ports 8000 fi /usr/sbin/update-rc.d fake-metadata defaults PostUninstall Script \u00b6 This script is the inverse of the post install script; it returns the system to its previous state. LINE = $( grep 169 .254.169.254/32 /etc/network/interfaces || true ) if [[ \" $LINE \" == *169.254.169.254/32* ]] ; then sed -i '/up ip addr add 169.254.169.254\\/32 dev lo scope host/d' /etc/network/interfaces fi LINE = $( /sbin/ip addr | grep 169 .254.169.254/32 || true ) if [[ \" $LINE \" == *169.254.169.254/32* ]] ; then /sbin/ip addr delete 169 .254.169.254/32 dev lo scope host fi LINE = $( grep iptables-restore /etc/network/interfaces || true ) if [[ \" $LINE \" == *iptables-restore* ]] ; then sed -i '/pre-up iptables-restore < \\/etc\\/network\\/iptables.rules/d' /etc/network/interfaces fi LINE = $( iptables -t nat -L | grep 8000 || true ) if [[ \" $LINE \" == *8000* ]] ; then iptables -t nat -D OUTPUT -p tcp -d 169 .254.169.254/32 --dport 80 -j REDIRECT --to-ports 8000 fi /usr/sbin/update-rc.d -f fake-metadata remove rm -rf /apps/fake-metadata pkill -f 'supervise fake-metadata' Log Rotation \u00b6 To keep the local Vagrant instance clean, it is useful to configure log rotation. Sending a HUP signal to the service allows it to continue writing to the new logfile. /mnt/logs/fake-metadata/server.log { daily rotate 7 compress missingok notifempty create 644 root root postrotate svc -h /service/fake-metadata endscript }","title":"Build a Fake Instance Metadata Server for Ubuntu on Vagrant"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant","text":"2014-07-22 Discuss Let's say that you have a proper build/bake/deploy pipeline in place for running appications in AWS. This is a reliable way to deploy applications at scale and move traffic between different versions of applications as a part of the deployment pipeline. However, the whole process may take 20-40 minutes or so to complete for any particular build. If you want to iterate more rapidly on your development efforts, you could skip the full process with a quickpatch ssh/rsync deployment to a single server or you could stand up a local Vagrant base image and iterate on that. Now let's say that the application you are working on is intended to work with instance metadata, particularly for the purpose of obtaining a rotating set of access and secret keys. It might be nice to have a fake metadata service running on your local Vagrant image so that you can test your application in a manner similar to how it will be running in the cloud. In this post, I describe how to build and configure a fake metadata service for an Ubuntu image running on Vagrant.","title":"Build a Fake Instance Metadata Server for Ubuntu on Vagrant"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#package-layout","text":"This layout assumes that you will be installing custom packages to the /apps directory, and there is a daemontools service hierarchy located at /service . The fake-metadata/service/run file is a script suitable for use with daemontools. root/ \u251c\u2500\u2500 apps \u2502 \u2514\u2500\u2500 fake-metadata \u2502 \u251c\u2500\u2500 app.py \u2502 \u2514\u2500\u2500 service \u2502 \u2514\u2500\u2500 run \u2514\u2500\u2500 etc \u251c\u2500\u2500 init.d \u2502 \u2514\u2500\u2500 fake-metadata \u251c\u2500\u2500 logrotate.d \u2502 \u2514\u2500\u2500 fake-metadata \u2514\u2500\u2500 network \u2514\u2500\u2500 iptables.rules","title":"Package Layout"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#building-and-packaging","text":"For cross-platform build packaging, it will be easiest to use the nebula ospackage plugin with Gradle. With this plugin available, your build script will look something like this: apply plugin: 'nebula-ospackage' ospackage { version = '1.0' packageName = 'fake-metadata' requires ( 'python-flask' ) link ( '/apps/fake-metadata/logs' , '/mnt/logs/fake-metadata' ) link ( '/service/fake-metadata' , '/apps/fake-metadata/service' ) } buildDeb { postInstall file ( 'scripts/postInstall.sh' ) preUninstall 'svc -d /service/fake-metadata' postUninstall file ( 'scripts/postUninstall.sh' ) } task build ( dependsOn: [ 'buildDeb' ])","title":"Building and Packaging"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#fake-metadata-application","text":"The simplest approach to building the service is to create a Flask service and have it run bare on the Vagrant instance. Given how small it will be and limited amount of traffic it will need to serve, there is no need to run this behind a dedicated static webserver like nginx or Apache. The nice thing about using Flask and having a basic structure in place is that it is then easy to extend the application to add other endpoints when needed. #!/usr/bin/env python from flask import Flask, jsonify, abort, make_response, request import os import sys app = Flask(__name__) BaseIAMRole = { 'Code': 'Success', 'LastUpdated' : '', 'Type': 'AWS-HMAC', 'AccessKeyId': '', 'SecretAccessKey': '', 'Token': '', 'Expiration': '' } @app.route('/', methods = ['GET']) def index(): return 'latest' @app.route('/latest/', methods = ['GET']) def latest(): return 'meta-data' @app.route('/latest/meta-data/', methods = ['GET']) def meta_data(): endpoints = [ 'iam', 'public-hostname', 'public-ipv4' ] return ('\\n').join(endpoints) @app.route('/latest/meta-data/iam/', methods = ['GET']) def iam(): return 'security-credentials' @app.route('/latest/meta-data/iam/security-credentials/', methods = ['GET']) def security_credentials(): return 'BaseIAMRole' @app.route('/latest/meta-data/iam/security-credentials/BaseIAMRole', methods = ['GET']) def base_iam_role(): # update the payload to contain a current set of accesss and secrey keys return jsonify(BaseIAMRole) @app.route('/latest/meta-data/public-hostname', methods = ['GET']) def public_hostname(): return os.environ['EC2_LOCAL_HOSTNAME'] @app.route('/latest/meta-data/public-ipv4', methods = ['GET']) def public_ipv4(): return os.environ['EC2_LOCAL_IPV4'] @app.errorhandler(400) def not_found(error): return make_response(jsonify( { 'error': 'bad request' } ), 400) @app.errorhandler(404) def not_found(error): return make_response(jsonify( { 'error': 'not found' } ), 404) @app.errorhandler(500) def not_found(error): return make_response(jsonify( { 'error': 'internal server error' } ), 500) if __name__ == '__main__': if len(sys.argv) > 1: if ':' in sys.argv[1]: host=sys.argv[1].split(':')[0] port=int(sys.argv[1].split(':')[1]) app.run(host=host, port=port) else: app.run(host=sys.argv[1]) else: app.run(debug=True)","title":"Fake Metadata Application"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#daemontools-run-script","text":"This script is watched by the supervise process, which then starts (or restarts) the application if it is not running. Switching to a non-root user and redirecting output to the log file occurs here. #!/bin/bash ulimit -n 32768 source /etc/profile.d/environment.sh export PATH=/command:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/scripts if [ ! -d \"/mnt/logs/fake-metadata\" ]; then mkdir -p /mnt/logs/fake-metadata chmod 0777 /mnt/logs/fake-metadata fi USER=someuser PYTHON=/usr/bin/python APP=/apps/fake-metadata/app.py OPTS=127.0.0.1:8000 LOG=/mnt/logs/fake-metadata/server.log echo \"starting fake-metadata\" exec setuidgid $USER $PYTHON $APP $OPTS >> $LOG 2>&1","title":"Daemontools Run Script"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postinstall-script","text":"This is where most of the trickiness occurs. The post install script is responsible for modifying the /etc/network/interfaces file, adding the metadata server IP address and configuring iptables in an idempotent manner. When the post install script is packaged by the nebula ospackage plugin into a Debian package, it gets a #!/bin/sh -e shebang invocation, which means that the script will halt execution at any point where it evaluates a non-zero return code. This means that the script needs to be written such a way that the environment state testing being done always returns a true value so that the script does not fail, hence the ||true constructs. We are attaching an extra IP address to the loopback interface, so we need to redirect traffic from 169.254.169.254:80 to the location where the fake metadata server is running. We are dealing with the loopback interface, which means that the PREROUTING nat table is never hit and we must use the OUTPUT table instead. You cannot DNAT packets destined for the loopback interface, because the kernel will treat them as martians and drop them, so you must REDIRECT the packets to the desired port. When performing the redirection from port 80 to 8000 on the loopback interface, it sends the packets to 127.0.0.1:8000, not 169.254.169.254:8000, so the fake metadata server must be listening on localhost port 8000. If for some reason, you need to troubleshoot the post install script, it can be found at /var/lib/dpkg/info/fake-metadata.postinst following an attempted package installation. LINE = $( grep 169 .254.169.254/32 /etc/network/interfaces || true ) if [[ ! \" $LINE \" == *169.254.169.254/32* ]] ; then sed -i '/iface lo inet loopback/a up ip addr add 169.254.169.254/32 dev lo scope host' /etc/network/interfaces fi LINE = $( /sbin/ip addr | grep 169 .254.169.254/32 || true ) if [[ ! \" $LINE \" == *169.254.169.254/32* ]] ; then /sbin/ip addr add 169 .254.169.254/32 dev lo scope host fi LINE = $( grep iptables-restore /etc/network/interfaces || true ) if [[ ! \" $LINE \" == *iptables-restore* ]] ; then sed -i '/up ip addr add/a pre-up iptables-restore < /etc/network/iptables.rules' /etc/network/interfaces fi LINE = $( iptables -t nat -L | grep 8000 || true ) if [[ ! \" $LINE \" == *8000* ]] ; then iptables -t nat -A OUTPUT -p tcp -d 169 .254.169.254/32 --dport 80 -j REDIRECT --to-ports 8000 fi /usr/sbin/update-rc.d fake-metadata defaults","title":"PostInstall Script"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postuninstall-script","text":"This script is the inverse of the post install script; it returns the system to its previous state. LINE = $( grep 169 .254.169.254/32 /etc/network/interfaces || true ) if [[ \" $LINE \" == *169.254.169.254/32* ]] ; then sed -i '/up ip addr add 169.254.169.254\\/32 dev lo scope host/d' /etc/network/interfaces fi LINE = $( /sbin/ip addr | grep 169 .254.169.254/32 || true ) if [[ \" $LINE \" == *169.254.169.254/32* ]] ; then /sbin/ip addr delete 169 .254.169.254/32 dev lo scope host fi LINE = $( grep iptables-restore /etc/network/interfaces || true ) if [[ \" $LINE \" == *iptables-restore* ]] ; then sed -i '/pre-up iptables-restore < \\/etc\\/network\\/iptables.rules/d' /etc/network/interfaces fi LINE = $( iptables -t nat -L | grep 8000 || true ) if [[ \" $LINE \" == *8000* ]] ; then iptables -t nat -D OUTPUT -p tcp -d 169 .254.169.254/32 --dport 80 -j REDIRECT --to-ports 8000 fi /usr/sbin/update-rc.d -f fake-metadata remove rm -rf /apps/fake-metadata pkill -f 'supervise fake-metadata'","title":"PostUninstall Script"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#log-rotation","text":"To keep the local Vagrant instance clean, it is useful to configure log rotation. Sending a HUP signal to the service allows it to continue writing to the new logfile. /mnt/logs/fake-metadata/server.log { daily rotate 7 compress missingok notifempty create 644 root root postrotate svc -h /service/fake-metadata endscript }","title":"Log Rotation"},{"location":"aws/checking-and-changing-content-type-of-s3-object-with-awscli/","text":"Checking and Changing Content-Type of S3 Object with AWSCLI \u00b6 2017-03-04 Discuss aws s3api get-object \\ --bucket my.bucket \\ --key foo/bar/2017-01-26/usage.json \\ usage.json aws s3api copy-object \\ --bucket archive \\ --content-type \"application/rss+xml\" \\ --copy-source archive/test/test.html \\ --key test/test.html \\ --metadata-directive \"REPLACE\"","title":"Checking and Changing Content-Type of S3 Object with AWSCLI"},{"location":"aws/checking-and-changing-content-type-of-s3-object-with-awscli/#checking-and-changing-content-type-of-s3-object-with-awscli","text":"2017-03-04 Discuss aws s3api get-object \\ --bucket my.bucket \\ --key foo/bar/2017-01-26/usage.json \\ usage.json aws s3api copy-object \\ --bucket archive \\ --content-type \"application/rss+xml\" \\ --copy-source archive/test/test.html \\ --key test/test.html \\ --metadata-directive \"REPLACE\"","title":"Checking and Changing Content-Type of S3 Object with AWSCLI"},{"location":"aws/configuring-multiple-interfaces-on-the-same-network-in-ec2/","text":"Configuring Multiple Interfaces on the Same Network in EC2 \u00b6 2014-07-09 Discuss I've been reading a bit lately on Linux policy and source-based routing for the purpose of configuring multiple NICs on the same network in EC2. I found the following links helpful: Multiple IPs and ENIs on EC2 in a VPC Routing for Multiple Uplinks/Providers A Quick Introduction to Linux Policy Routing","title":"Configuring Multiple Interfaces on the Same Network in EC2"},{"location":"aws/configuring-multiple-interfaces-on-the-same-network-in-ec2/#configuring-multiple-interfaces-on-the-same-network-in-ec2","text":"2014-07-09 Discuss I've been reading a bit lately on Linux policy and source-based routing for the purpose of configuring multiple NICs on the same network in EC2. I found the following links helpful: Multiple IPs and ENIs on EC2 in a VPC Routing for Multiple Uplinks/Providers A Quick Introduction to Linux Policy Routing","title":"Configuring Multiple Interfaces on the Same Network in EC2"},{"location":"github/how-i-built-my-site/","text":"How I Built My Site \u00b6 2021-12-01 Discuss I recently rebuilt my GitHub Pages site, switching my tech stack to Python , MkDocs and Material for MkDocs . The previous workflow had too many moving parts and the theme was hard to read. It turns out that MkDocs has some nice automation hooks for GitHub Pages and it looks great on mobile browsers with the Material theme. Since I use MkDocs for site generation at work, I am already pretty familiar with it. Writing documents in Markdown and turning them into web pages with a static site generator is the fastest and easiest way to post articles on your GitHub Pages site. This format allows you to drop down into HTML, when necessary, to enhance your page formatting, but you shouldn't need to do this for most pages. Install Python and MkDocs Packages \u00b6 If you are relying on a system Python, consider following these instructions: Install the Latest Python Versions on Mac OSX . Install Material for MkDocs . This package will bring along all the necessary dependencies, including MkDocs , for a fully functioning site. pip install mkdocs-material Sign Up for Google Analytics \u00b6 Note You must disable ad-blocking software in order to be able to see the Google Analytics page. Navigate to Google Analytics > Admin Property > Create New Property Account Name: $YOUR_ACCOUNT_NAME Website Name: username.github.io Website URL: https://username.github.io Go with the default configuration options - you can change these later. Get Tracking ID Note the Tracking ID (looks like: UA-00000000-0 ) assigned to this property. Build the Site \u00b6 Create a new repository named $USERNAME.github.io , where $USERNAME is your username (or organization name) on GitHub. It must match exactly, or it will not work. Clone the repository locally. Requires SSH keys . git clone git @github . com : $ USERNAME / $ USERNAME . github . io . git cd $ USERNAME . github . com Create a directory structure that looks like the following: $USERNAME.github.io/ \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u251c\u2500\u2500 pr.yml \u2502 \u2514\u2500\u2500 release.yml \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 docs/ \u2502 \u251c\u2500\u2500 css/ \u2502 \u2502 \u2514\u2500\u2500 custom.css \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 requirements.txt Create a GitHub Actions configuration file for pull requests ( ./.github/workflows/pr.yml ). This will build the site for every PR that is submitted. name : pull - request on : [ pull_request ] jobs : build : runs - on : ubuntu - latest steps : - uses : actions / checkout @v2 - uses : actions / setup - python @v2 with : python - version : 3. x - run : pip install - r requirements . txt - run : mkdocs build Create a GitHub Actions configuration file for deploying the site ( ./.github/workflows/release.yml ), replacing $USERNAME with your username. name : release on : push : branches : - main jobs : deploy : if : $ {{ github . repository == '$USERNAME/$USERNAME.github.io' }} runs - on : ubuntu - latest steps : - uses : actions / checkout @ v2 - uses : actions / setup - python @ v2 with : python - version : 3 . x - run : pip install - r requirements . txt - run : git fetch origin gh - pages : gh - pages - run : mkdocs gh - deploy Create a ./mkdocs.yml site configuration file. Choose a Creative Commons license for your site - I chose CC BY-NC-SA 4.0 for my site. Add your Google Analytics property Tracking ID to extra.analytics . If you do not have a Tracking ID, then delete these lines in the configuration file. site_name: My Site site_url: 'http://$USERNAME.github.io/' repo_url: 'https://github.com/username/$USERNAME.github.io' edit_uri: '' site_description: My Site site_author: My Name copyright: <a rel= \"license\" href= \"http://creativecommons.org/licenses/by-nc-sa/4.0/\" ><img alt= \"Creative Commons License\" style= \"border-width:0\" src= \"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a> strict: True pages: - Home: index.md theme: material extra: analytics: provider: google property: UA-00000000-0 extra_css: - css/custom.css markdown_extensions: - admonition - codehilite - pymdownx.tilde - toc: permalink: True Create a ./docs/index.md file. Hello World! Create a ./docs/css/custom.css file. /* Avoid showing first header, i.e., the page title in the sidebar. * * https://github.com/mkdocs/mkdocs/issues/318#issuecomment-98520139 */ li . toctree-l3 : first-child { display : none ; } code { font-family : Menlo , monospace ; } .meta .date { float: left } .meta .discuss { float: right } Create a ./requirements.txt file, so that you can easily reinstall the necessary Python packages with pip install -r requirements.txt . mkdocs-material Create a ./.gitignore file. site/ venv/ Build and serve the site locally, to verify that your changes look good. When MkDocs is up and running, you will see Serving on http://127.0.0.1:8000 . Leave this process running. When you are done developing and testing your site, you can stop this process with ctrl+c . Open a new Terminal to run the second command. mkdocs serve open http://localhost:8000 Push the changes to GitHub to save your progress. The first release build will fail due to the absence of the gh-pages branch. git add --all git commit -m \"first commit\" git push origin Build and deploy the site for the first time. This step is done to establish the gh-pages branch which is used by GitHub Actions and Pages. The static HTML site generated by this process will be pushed to the gh-pages branch at the origin for this repository. Changes will be live within one minute. Navigate to the site url to see your changes, after you have configured the Pages source branch in Repo Settings. mkdocs gh-deploy Repo Configuration \u00b6 Navigate to the repository on GitHub and set some useful configuration options. Code > Description Code > Website Code > Manage topics Settings > Branches > Protected branches: main Check: Protect this branch Check: Include administrators Settings > Pages > Source Branch: gh-pages Folder: / (root) New Post Workflow \u00b6 Create a new branch, so you can check your work in a PR. git checkout -b new-post Start serving the site locally, with file change detection. mkdocs serve open http://localhost:8000 Start a new post by creating a markdown file in the ./docs directory hierarchy. Images can be served from a location such as ./docs/images , with references as follows: ! [ Link Name ]( / images / my - file . png \"Alt Text\" ) Add the new markdown file to the ./mkdocs.yml site configuration and continue editing. See Writing Your Docs for tips on arranging your Markdown files. When editing is complete, commit changes and push. Open a PR on the GitHub site and check the build output. Merge when you are happy with the changes and the release build will deploy the updated site. git add --all git commit -m \"my new post\" git push origin","title":"How I Built My Site"},{"location":"github/how-i-built-my-site/#how-i-built-my-site","text":"2021-12-01 Discuss I recently rebuilt my GitHub Pages site, switching my tech stack to Python , MkDocs and Material for MkDocs . The previous workflow had too many moving parts and the theme was hard to read. It turns out that MkDocs has some nice automation hooks for GitHub Pages and it looks great on mobile browsers with the Material theme. Since I use MkDocs for site generation at work, I am already pretty familiar with it. Writing documents in Markdown and turning them into web pages with a static site generator is the fastest and easiest way to post articles on your GitHub Pages site. This format allows you to drop down into HTML, when necessary, to enhance your page formatting, but you shouldn't need to do this for most pages.","title":"How I Built My Site"},{"location":"github/how-i-built-my-site/#install-python-and-mkdocs-packages","text":"If you are relying on a system Python, consider following these instructions: Install the Latest Python Versions on Mac OSX . Install Material for MkDocs . This package will bring along all the necessary dependencies, including MkDocs , for a fully functioning site. pip install mkdocs-material","title":"Install Python and MkDocs Packages"},{"location":"github/how-i-built-my-site/#sign-up-for-google-analytics","text":"Note You must disable ad-blocking software in order to be able to see the Google Analytics page. Navigate to Google Analytics > Admin Property > Create New Property Account Name: $YOUR_ACCOUNT_NAME Website Name: username.github.io Website URL: https://username.github.io Go with the default configuration options - you can change these later. Get Tracking ID Note the Tracking ID (looks like: UA-00000000-0 ) assigned to this property.","title":"Sign Up for Google Analytics"},{"location":"github/how-i-built-my-site/#build-the-site","text":"Create a new repository named $USERNAME.github.io , where $USERNAME is your username (or organization name) on GitHub. It must match exactly, or it will not work. Clone the repository locally. Requires SSH keys . git clone git @github . com : $ USERNAME / $ USERNAME . github . io . git cd $ USERNAME . github . com Create a directory structure that looks like the following: $USERNAME.github.io/ \u251c\u2500\u2500 .github/ \u2502 \u2514\u2500\u2500 workflows/ \u2502 \u251c\u2500\u2500 pr.yml \u2502 \u2514\u2500\u2500 release.yml \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 docs/ \u2502 \u251c\u2500\u2500 css/ \u2502 \u2502 \u2514\u2500\u2500 custom.css \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 mkdocs.yml \u2514\u2500\u2500 requirements.txt Create a GitHub Actions configuration file for pull requests ( ./.github/workflows/pr.yml ). This will build the site for every PR that is submitted. name : pull - request on : [ pull_request ] jobs : build : runs - on : ubuntu - latest steps : - uses : actions / checkout @v2 - uses : actions / setup - python @v2 with : python - version : 3. x - run : pip install - r requirements . txt - run : mkdocs build Create a GitHub Actions configuration file for deploying the site ( ./.github/workflows/release.yml ), replacing $USERNAME with your username. name : release on : push : branches : - main jobs : deploy : if : $ {{ github . repository == '$USERNAME/$USERNAME.github.io' }} runs - on : ubuntu - latest steps : - uses : actions / checkout @ v2 - uses : actions / setup - python @ v2 with : python - version : 3 . x - run : pip install - r requirements . txt - run : git fetch origin gh - pages : gh - pages - run : mkdocs gh - deploy Create a ./mkdocs.yml site configuration file. Choose a Creative Commons license for your site - I chose CC BY-NC-SA 4.0 for my site. Add your Google Analytics property Tracking ID to extra.analytics . If you do not have a Tracking ID, then delete these lines in the configuration file. site_name: My Site site_url: 'http://$USERNAME.github.io/' repo_url: 'https://github.com/username/$USERNAME.github.io' edit_uri: '' site_description: My Site site_author: My Name copyright: <a rel= \"license\" href= \"http://creativecommons.org/licenses/by-nc-sa/4.0/\" ><img alt= \"Creative Commons License\" style= \"border-width:0\" src= \"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a> strict: True pages: - Home: index.md theme: material extra: analytics: provider: google property: UA-00000000-0 extra_css: - css/custom.css markdown_extensions: - admonition - codehilite - pymdownx.tilde - toc: permalink: True Create a ./docs/index.md file. Hello World! Create a ./docs/css/custom.css file. /* Avoid showing first header, i.e., the page title in the sidebar. * * https://github.com/mkdocs/mkdocs/issues/318#issuecomment-98520139 */ li . toctree-l3 : first-child { display : none ; } code { font-family : Menlo , monospace ; } .meta .date { float: left } .meta .discuss { float: right } Create a ./requirements.txt file, so that you can easily reinstall the necessary Python packages with pip install -r requirements.txt . mkdocs-material Create a ./.gitignore file. site/ venv/ Build and serve the site locally, to verify that your changes look good. When MkDocs is up and running, you will see Serving on http://127.0.0.1:8000 . Leave this process running. When you are done developing and testing your site, you can stop this process with ctrl+c . Open a new Terminal to run the second command. mkdocs serve open http://localhost:8000 Push the changes to GitHub to save your progress. The first release build will fail due to the absence of the gh-pages branch. git add --all git commit -m \"first commit\" git push origin Build and deploy the site for the first time. This step is done to establish the gh-pages branch which is used by GitHub Actions and Pages. The static HTML site generated by this process will be pushed to the gh-pages branch at the origin for this repository. Changes will be live within one minute. Navigate to the site url to see your changes, after you have configured the Pages source branch in Repo Settings. mkdocs gh-deploy","title":"Build the Site"},{"location":"github/how-i-built-my-site/#repo-configuration","text":"Navigate to the repository on GitHub and set some useful configuration options. Code > Description Code > Website Code > Manage topics Settings > Branches > Protected branches: main Check: Protect this branch Check: Include administrators Settings > Pages > Source Branch: gh-pages Folder: / (root)","title":"Repo Configuration"},{"location":"github/how-i-built-my-site/#new-post-workflow","text":"Create a new branch, so you can check your work in a PR. git checkout -b new-post Start serving the site locally, with file change detection. mkdocs serve open http://localhost:8000 Start a new post by creating a markdown file in the ./docs directory hierarchy. Images can be served from a location such as ./docs/images , with references as follows: ! [ Link Name ]( / images / my - file . png \"Alt Text\" ) Add the new markdown file to the ./mkdocs.yml site configuration and continue editing. See Writing Your Docs for tips on arranging your Markdown files. When editing is complete, commit changes and push. Open a PR on the GitHub site and check the build output. Merge when you are happy with the changes and the release build will deploy the updated site. git add --all git commit -m \"my new post\" git push origin","title":"New Post Workflow"},{"location":"linux/apache-deflate-and-cors-headers/","text":"For internal static websites, you may want to configure CORS headers with generous permissions, to improve the cross-site experience. A reasonable deflate configuration is provided which will compress the largest elements of a website. LoadModule deflate_module modules/mod_deflate.so <IfModule mod_deflate.c> # Compress HTML, CSS, JavaScript, Text, XML and fonts AddOutputFilterByType DEFLATE application/javascript AddOutputFilterByType DEFLATE application/rss+xml AddOutputFilterByType DEFLATE application/vnd.ms-fontobject AddOutputFilterByType DEFLATE application/x-font AddOutputFilterByType DEFLATE application/x-font-opentype AddOutputFilterByType DEFLATE application/x-font-otf AddOutputFilterByType DEFLATE application/x-font-truetype AddOutputFilterByType DEFLATE application/x-font-ttf AddOutputFilterByType DEFLATE application/x-javascript AddOutputFilterByType DEFLATE application/xhtml+xml AddOutputFilterByType DEFLATE application/xml AddOutputFilterByType DEFLATE font/opentype AddOutputFilterByType DEFLATE font/otf AddOutputFilterByType DEFLATE font/ttf AddOutputFilterByType DEFLATE image/svg+xml AddOutputFilterByType DEFLATE image/x-icon AddOutputFilterByType DEFLATE text/css AddOutputFilterByType DEFLATE text/html AddOutputFilterByType DEFLATE text/javascript AddOutputFilterByType DEFLATE text/plain AddOutputFilterByType DEFLATE text/xml </IfModule> LoadModule headers_module modules/mod_headers.so # Once a resource becomes stale, caches must not use their stale copy without successful # validation on the origin server. Header always set Cache-Control \"public, must-revalidate, max-age=0\" # Expose the response to frontend JavaScript code, when the request's credentials mode is # include. Credentials are cookies, authorization headers or TLS client certificates. Header always set Access-Control-Allow-Credentials true # When the Origin header is set, copy it from the request to the response. SetEnvIf Origin \"(.+)\" HAVE_origin=1 RewriteCond %{HTTP:Origin} (.+) RewriteRule .* - [E=ORIGIN:%1] Header always set Access-Control-Allow-Origin \"%{ORIGIN}e\" env=HAVE_origin # When the Access-Control-Allow-Methods header is set, replace it with GET,PATCH,POST,PUT,DELETE. SetEnvIf Access-Control-Request-Method \"(.+)\" HAVE_method=1 Header always set Access-Control-Allow-Methods \"GET,PATCH,POST,PUT,DELETE\" env=HAVE_method # When the Access-Control-Request-Headers header is set, copy it from the request to the response. SetEnvIf Access-Control-Request-Headers \"(.+)\" HAVE_headers=1 RewriteCond %{HTTP:Access-Control-Request-Headers} (.+) RewriteRule .* - [E=HEADERS:%1] Header always set Access-Control-Allow-Headers \"%{HEADERS}e\" env=HAVE_headers","title":"Apache Deflate and CORS Headers"},{"location":"linux/linux-ftrace-delivers-dtrace-like-functionality/","text":"Linux ftrace Delivers dtrace-like Functionality \u00b6 2014-07-22 Discuss Brendan Gregg recently released perf-tools , which is a collection of low-level performance observability scripts akin to the DTraceToolkit . He has a new article posted on the tools at LWN.","title":"Linux ftrace Delivers dtrace-like Functionality"},{"location":"linux/linux-ftrace-delivers-dtrace-like-functionality/#linux-ftrace-delivers-dtrace-like-functionality","text":"2014-07-22 Discuss Brendan Gregg recently released perf-tools , which is a collection of low-level performance observability scripts akin to the DTraceToolkit . He has a new article posted on the tools at LWN.","title":"Linux ftrace Delivers dtrace-like Functionality"},{"location":"misc/clone-stash-pull-requests/","text":"Clone Stash Pull Requests \u00b6 2015-11-18 Discuss https://answers.atlassian.com/questions/179848/local-checkout-of-a-pull-request-in-stash [alias] prstash = \"!f() { git fetch $1 refs/pull-requests/$2/from:$3; } ; f\" # where 3 is the 3rd pull request git prstash origin 3 dest-branch git checkout dest-branch # to reclone git checkout master git prstash origin 3 dest-branch git checkout dest-branch","title":"Clone Stash Pull Requests"},{"location":"misc/clone-stash-pull-requests/#clone-stash-pull-requests","text":"2015-11-18 Discuss https://answers.atlassian.com/questions/179848/local-checkout-of-a-pull-request-in-stash [alias] prstash = \"!f() { git fetch $1 refs/pull-requests/$2/from:$3; } ; f\" # where 3 is the 3rd pull request git prstash origin 3 dest-branch git checkout dest-branch # to reclone git checkout master git prstash origin 3 dest-branch git checkout dest-branch","title":"Clone Stash Pull Requests"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/","text":"Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina \u00b6 2015-06-24 Discuss Introduction \u00b6 Why? Some recent benchmarks have shown that Ubuntu can out-perform OSX on Macbook hardware. http://www.phoronix.com/scan.php?page=article&item=osx10_ubuntu1410&num=1 http://www.phoronix.com/scan.php?page=article&item=ubuntu_1404_mba2013gl&num=1 This process was tested on a late 2013 Macbook11,2 A1398 model: 2GHz Intel Core i7 16GB 1600 MHz DDR3 Intel Iris Pro 1536MB You can check your Macbook model from Ubuntu like so: sudo dmidecode --type 1 Issues \u00b6 Thunderbolt Monitor support is not great. It will work if the monitor is connected to the system at boot-up, but it does not support hot-plug. Note See https://blog.jessfraz.com/post/linux-on-mac/ for some new information on this. Apparently, kernel 3.17 has support for hotplugging Thunderbolt connections. See http://ubuntuhandbook.org/index.php/2014/11/how-to-upgrade-to-linux-kernel-3-17-4-in-ubuntu-14-10/ for adding 3.17 to Ubuntu 14. Installing \u00b6 Prepare Macbook for Ubuntu installation. Partition hard disk. Finder > Applications > Utilities > Disk Utility Macintosh HD > Partition Add a \"Free Space\" partition equal to half the drive. Apply and wait for the file system to shrink (30-60 minutes). Download a binary zip and install rEFInd Boot Manager . This will allow you to switch between booting OSX 10.10 and Ubuntu 14.04. unzip refind-bin-0.8.4.zip cd refind-bin-0.8.4 ./install.sh --alldrivers Download Ubuntu and create a bootable USB stick . hdiutil convert - format UDRW - o ~/ path / to / target . img ~/ path / to / ubuntu . iso diskutil list # find usb disk diskutil unmountDisk / dev / diskN sudo dd if =/ path / to / downloaded . img of =/ dev / rdiskN bs = 1 m diskutil eject / dev / diskN Install Ubuntu. Reboot, hold down the Option key and choose Ubuntu USB stick. Grub Boot Menu: Install Ubuntu Continue with the installation, without networking. Installation Type > Something Else This will allow you to create a custom partition configuration and preserve your Mac OSX install. You should see a partition list like the following: Partition Type Size /dev/sda1 efi 209 MB /dev/sda2 hfs+ 125441 MB /dev/sda3 hfs+ 650 MB free space 124699 MB Within the free space, create two new partitions: Swap space, equal to the same size as system RAM. An Ext4 partition for the root mount point, consuming the remaining space. Select the new root partition and continue the installation. Time Zone: Los Angeles Keyboard Layout: English (US) - English (Macintosh) Who Are You? Re-use your existing username and computer name. Reboot and choose Ubuntu installation from rEFInd menu. Configure Ubuntu Environment. Start a Terminal Ubuntu Search > Terminal Right-click Launcher Icon > Lock to Launcher Configure wireless networking . Insert USB stick with Ubuntu installation media. Install dkms and bcmwl-kernel-source packages. cd / media / `whoami` / Ubuntu \\ 14.04.1 \\ LTS \\ amd64 / sudo dpgk - i pool / main / d / dkms / dkms_2 .2.0.3 - 1.1 ubuntu5_all . deb sudo dpgk - i pool / restricted / b / bcmwl / bcmwl - kernel - source_6 .30.223.141 + bdcom - 0ubuntu2_amd64 . deb Create a network manager wakeup script /etc/pm/sleep.d/99_wakeup and make it executable. This will restore wireless networking when resuming from suspend. Bugs 1299282 and 1289884 are tracking this issue. 1 2 3 4 5 6 7 8 9 #!/bin/bash case \" $1 \" in thaw | resume ) nmcli nm sleep false ;; esac exit $? Install binary drivers for Intel Iris Pro Graphics 5200. Download the 64-bit Ubuntu installer from https://01.org/linuxgraphics/ sudo dpkg -i intel-linux-graphics-installer_1.0.7-0intel1_amd64.deb sudo apt-get install -f Disable suspend when closing the lid. sudo vi /etc/systemd/logind.conf HandleLidSwitch=ignore sudo restart systemd-logind Configure mouse behavior. System Settings > Mouse and Touchpad Check: Natural Scrolling Uncheck: Disable while typing Uncheck: Tap to click Disable turning the screen off. System Settings > Brightness & Lock Turn Screen Off When Inactive For: Never Lock Screen After: 10 minutes Add a screensaver. The default gnome-screensaver is just a black screen. sudo apt-get remove gnome-screensaver sudo apt-get install xscreensaver xscreensaver-data-extra xscreensaver-gl-extra Install Applications Oracle Java 7 and Java 8 sudo add-apt-repository ppa:webupd8team/java sudo apt-get update echo oracle-java7-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections sudo apt-get install oracle-java7-installer echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections sudo apt-get install oracle-java8-installer # switching versions sudo update-java-alternatives -s java-7-oracle sudo update-java-alternatives -s java-8-oracle sudo apt-get install oracle-java8-set-default Google Chrome As of version 35, Chrome no longer supports the NPAPI plugin , which is required by the Oracle and OpenJDK Java plugins. If you need to run a Java plugin from a web browser, use Firefox instead. Test Java version. https://www.java.com/en/download/testjava.jsp Test WiFi speed. Download large files. curl - O http : // releases . ubuntu . com / 14.10 / ubuntu - 14.10 - desktop - amd64 . iso curl - O http : // www . wswd . net / testdownloadfiles / 1 GB . zip Uninstalling \u00b6 Reboot into OSX. Uninstall rEFInd. diskutil list diskutil mount disk0s1 sudo su - cd /Volumes/EFI rm -rf BOOT ubuntu Download GParted Live and install it on a USB stick. hdiutil convert - format UDRW - o ~/ path / to / target . img ~/ path / to / gparted . iso diskutil list # find usb disk diskutil unmountDisk / dev / diskN sudo dd if =/ path / to / downloaded . img of =/ dev / rdiskN bs = 1 m diskutil eject / dev / diskN Reboot and hold down the option key. Remove the Linux partitions. Reboot into Mac OSX and extend the partition. Covert the CoreStorage volume back into HFS+ . diskutil cs list diskutil coreStorage revert $VOLUME_UUID Reboot. Delete the recovery partition. Extend the primary partition. Recreate the recovery partition . Convert the HFS+ volume back to CoreStorage . diskutil cs convert \"Macintosh HD\" Reboot.","title":"Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#dual-boot-osx-1010-and-ubuntu-1404-on-a-2013-macbook-pro-retina","text":"2015-06-24 Discuss","title":"Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#introduction","text":"Why? Some recent benchmarks have shown that Ubuntu can out-perform OSX on Macbook hardware. http://www.phoronix.com/scan.php?page=article&item=osx10_ubuntu1410&num=1 http://www.phoronix.com/scan.php?page=article&item=ubuntu_1404_mba2013gl&num=1 This process was tested on a late 2013 Macbook11,2 A1398 model: 2GHz Intel Core i7 16GB 1600 MHz DDR3 Intel Iris Pro 1536MB You can check your Macbook model from Ubuntu like so: sudo dmidecode --type 1","title":"Introduction"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#issues","text":"Thunderbolt Monitor support is not great. It will work if the monitor is connected to the system at boot-up, but it does not support hot-plug. Note See https://blog.jessfraz.com/post/linux-on-mac/ for some new information on this. Apparently, kernel 3.17 has support for hotplugging Thunderbolt connections. See http://ubuntuhandbook.org/index.php/2014/11/how-to-upgrade-to-linux-kernel-3-17-4-in-ubuntu-14-10/ for adding 3.17 to Ubuntu 14.","title":"Issues"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#installing","text":"Prepare Macbook for Ubuntu installation. Partition hard disk. Finder > Applications > Utilities > Disk Utility Macintosh HD > Partition Add a \"Free Space\" partition equal to half the drive. Apply and wait for the file system to shrink (30-60 minutes). Download a binary zip and install rEFInd Boot Manager . This will allow you to switch between booting OSX 10.10 and Ubuntu 14.04. unzip refind-bin-0.8.4.zip cd refind-bin-0.8.4 ./install.sh --alldrivers Download Ubuntu and create a bootable USB stick . hdiutil convert - format UDRW - o ~/ path / to / target . img ~/ path / to / ubuntu . iso diskutil list # find usb disk diskutil unmountDisk / dev / diskN sudo dd if =/ path / to / downloaded . img of =/ dev / rdiskN bs = 1 m diskutil eject / dev / diskN Install Ubuntu. Reboot, hold down the Option key and choose Ubuntu USB stick. Grub Boot Menu: Install Ubuntu Continue with the installation, without networking. Installation Type > Something Else This will allow you to create a custom partition configuration and preserve your Mac OSX install. You should see a partition list like the following: Partition Type Size /dev/sda1 efi 209 MB /dev/sda2 hfs+ 125441 MB /dev/sda3 hfs+ 650 MB free space 124699 MB Within the free space, create two new partitions: Swap space, equal to the same size as system RAM. An Ext4 partition for the root mount point, consuming the remaining space. Select the new root partition and continue the installation. Time Zone: Los Angeles Keyboard Layout: English (US) - English (Macintosh) Who Are You? Re-use your existing username and computer name. Reboot and choose Ubuntu installation from rEFInd menu. Configure Ubuntu Environment. Start a Terminal Ubuntu Search > Terminal Right-click Launcher Icon > Lock to Launcher Configure wireless networking . Insert USB stick with Ubuntu installation media. Install dkms and bcmwl-kernel-source packages. cd / media / `whoami` / Ubuntu \\ 14.04.1 \\ LTS \\ amd64 / sudo dpgk - i pool / main / d / dkms / dkms_2 .2.0.3 - 1.1 ubuntu5_all . deb sudo dpgk - i pool / restricted / b / bcmwl / bcmwl - kernel - source_6 .30.223.141 + bdcom - 0ubuntu2_amd64 . deb Create a network manager wakeup script /etc/pm/sleep.d/99_wakeup and make it executable. This will restore wireless networking when resuming from suspend. Bugs 1299282 and 1289884 are tracking this issue. 1 2 3 4 5 6 7 8 9 #!/bin/bash case \" $1 \" in thaw | resume ) nmcli nm sleep false ;; esac exit $? Install binary drivers for Intel Iris Pro Graphics 5200. Download the 64-bit Ubuntu installer from https://01.org/linuxgraphics/ sudo dpkg -i intel-linux-graphics-installer_1.0.7-0intel1_amd64.deb sudo apt-get install -f Disable suspend when closing the lid. sudo vi /etc/systemd/logind.conf HandleLidSwitch=ignore sudo restart systemd-logind Configure mouse behavior. System Settings > Mouse and Touchpad Check: Natural Scrolling Uncheck: Disable while typing Uncheck: Tap to click Disable turning the screen off. System Settings > Brightness & Lock Turn Screen Off When Inactive For: Never Lock Screen After: 10 minutes Add a screensaver. The default gnome-screensaver is just a black screen. sudo apt-get remove gnome-screensaver sudo apt-get install xscreensaver xscreensaver-data-extra xscreensaver-gl-extra Install Applications Oracle Java 7 and Java 8 sudo add-apt-repository ppa:webupd8team/java sudo apt-get update echo oracle-java7-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections sudo apt-get install oracle-java7-installer echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections sudo apt-get install oracle-java8-installer # switching versions sudo update-java-alternatives -s java-7-oracle sudo update-java-alternatives -s java-8-oracle sudo apt-get install oracle-java8-set-default Google Chrome As of version 35, Chrome no longer supports the NPAPI plugin , which is required by the Oracle and OpenJDK Java plugins. If you need to run a Java plugin from a web browser, use Firefox instead. Test Java version. https://www.java.com/en/download/testjava.jsp Test WiFi speed. Download large files. curl - O http : // releases . ubuntu . com / 14.10 / ubuntu - 14.10 - desktop - amd64 . iso curl - O http : // www . wswd . net / testdownloadfiles / 1 GB . zip","title":"Installing"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#uninstalling","text":"Reboot into OSX. Uninstall rEFInd. diskutil list diskutil mount disk0s1 sudo su - cd /Volumes/EFI rm -rf BOOT ubuntu Download GParted Live and install it on a USB stick. hdiutil convert - format UDRW - o ~/ path / to / target . img ~/ path / to / gparted . iso diskutil list # find usb disk diskutil unmountDisk / dev / diskN sudo dd if =/ path / to / downloaded . img of =/ dev / rdiskN bs = 1 m diskutil eject / dev / diskN Reboot and hold down the option key. Remove the Linux partitions. Reboot into Mac OSX and extend the partition. Covert the CoreStorage volume back into HFS+ . diskutil cs list diskutil coreStorage revert $VOLUME_UUID Reboot. Delete the recovery partition. Extend the primary partition. Recreate the recovery partition . Convert the HFS+ volume back to CoreStorage . diskutil cs convert \"Macintosh HD\" Reboot.","title":"Uninstalling"},{"location":"misc/reset-jenkins-build-number/","text":"Reset Jenkins Build Number \u00b6 2017-03-14 Discuss http://stackoverflow.com/questions/20901791/how-to-reset-build-number-in-jenkins Go to the Jenkins Script Console and enter: item = Jenkins . instance . getItemByFullName ( \"your-job-name-here\" ) // remove all build history item . builds . each () { build -> build . delete () } item . updateNextBuildNumber ( 1 )","title":"Reset Jenkins Build Number"},{"location":"misc/reset-jenkins-build-number/#reset-jenkins-build-number","text":"2017-03-14 Discuss http://stackoverflow.com/questions/20901791/how-to-reset-build-number-in-jenkins Go to the Jenkins Script Console and enter: item = Jenkins . instance . getItemByFullName ( \"your-job-name-here\" ) // remove all build history item . builds . each () { build -> build . delete () } item . updateNextBuildNumber ( 1 )","title":"Reset Jenkins Build Number"},{"location":"misc/switching-jdks-in-osx/","text":"Switching JDKs in OSX \u00b6 2015-02-03 Discuss Add the following to your $HOME/.bash_profile : export JAVA7_HOME = \" $( /usr/libexec/java_home -v 1 .7 ) \" export JAVA8_HOME = \" $( /usr/libexec/java_home -v 1 .8 ) \" export JAVA_HOME = $JAVA8_HOME switch_java () { if echo $JAVA_HOME | grep -q 1 .8 ; then export JAVA_HOME = $JAVA7_HOME else export JAVA_HOME = $JAVA8_HOME fi echo \"JAVA_HOME= $JAVA_HOME \" }","title":"Switching JDKs in OSX"},{"location":"misc/switching-jdks-in-osx/#switching-jdks-in-osx","text":"2015-02-03 Discuss Add the following to your $HOME/.bash_profile : export JAVA7_HOME = \" $( /usr/libexec/java_home -v 1 .7 ) \" export JAVA8_HOME = \" $( /usr/libexec/java_home -v 1 .8 ) \" export JAVA_HOME = $JAVA8_HOME switch_java () { if echo $JAVA_HOME | grep -q 1 .8 ; then export JAVA_HOME = $JAVA7_HOME else export JAVA_HOME = $JAVA8_HOME fi echo \"JAVA_HOME= $JAVA_HOME \" }","title":"Switching JDKs in OSX"},{"location":"python/accessing-google-apis-with-python/","text":"Accessing Google APIs with Python \u00b6 2016-06-03 Discuss Links \u00b6 https://developers.google.com/admin-sdk/directory/v1/quickstart/python#prerequisites https://developers.google.com/admin-sdk/directory/v1/reference/users/list#try-it http://oauth2client.readthedocs.io/en/latest/source/oauth2client.service_account.html https://github.com/google/oauth2client/issues/401 https://github.com/google/oauth2client/issues/418 https://github.com/google/oauth2client/pull/420 Method \u00b6 import httplib2 import secure_storage from apiclient.discovery import build from oauth2client.service_account import ServiceAccountCredentials client_email = '...@developer.gserviceaccount.com' scopes = [ 'https://www.googleapis.com/auth/admin.directory.user.readonly' , 'https://www.googleapis.com/auth/admin.directory.group.readonly' , 'https://www.googleapis.com/auth/apps.groups.settings' ] uname = '...@example.com' fname = '/.../private_key.p12' creds = ServiceAccountCredentials . from_p12_keyfile ( client_email , fname , scopes = scopes ) delegated_creds = creds . create_delegated ( uname ) http_auth = delegated_creds . authorize ( httplib2 . Http ()) service = build ( 'admin' , 'directory_v1' , http = http_auth ) service . users () . list ( domain = 'example.com' , maxResults = 10 , orderBy = 'email' ) . execute () service . groups () . list ( domain = 'example.com' , maxResults = 10 ) . execute ()","title":"Accessing Google APIs with Python"},{"location":"python/accessing-google-apis-with-python/#accessing-google-apis-with-python","text":"2016-06-03 Discuss","title":"Accessing Google APIs with Python"},{"location":"python/accessing-google-apis-with-python/#links","text":"https://developers.google.com/admin-sdk/directory/v1/quickstart/python#prerequisites https://developers.google.com/admin-sdk/directory/v1/reference/users/list#try-it http://oauth2client.readthedocs.io/en/latest/source/oauth2client.service_account.html https://github.com/google/oauth2client/issues/401 https://github.com/google/oauth2client/issues/418 https://github.com/google/oauth2client/pull/420","title":"Links"},{"location":"python/accessing-google-apis-with-python/#method","text":"import httplib2 import secure_storage from apiclient.discovery import build from oauth2client.service_account import ServiceAccountCredentials client_email = '...@developer.gserviceaccount.com' scopes = [ 'https://www.googleapis.com/auth/admin.directory.user.readonly' , 'https://www.googleapis.com/auth/admin.directory.group.readonly' , 'https://www.googleapis.com/auth/apps.groups.settings' ] uname = '...@example.com' fname = '/.../private_key.p12' creds = ServiceAccountCredentials . from_p12_keyfile ( client_email , fname , scopes = scopes ) delegated_creds = creds . create_delegated ( uname ) http_auth = delegated_creds . authorize ( httplib2 . Http ()) service = build ( 'admin' , 'directory_v1' , http = http_auth ) service . users () . list ( domain = 'example.com' , maxResults = 10 , orderBy = 'email' ) . execute () service . groups () . list ( domain = 'example.com' , maxResults = 10 ) . execute ()","title":"Method"},{"location":"python/building-rest-apis-with-python-flask/","text":"Building REST APIs with Python Flask \u00b6 2014-07-15 Discuss Miguel Grinberg , author of Flask Web Development put together an excellent blog post on Designing a RESTful API with Python and Flask .","title":"Building REST APIs with Python Flask"},{"location":"python/building-rest-apis-with-python-flask/#building-rest-apis-with-python-flask","text":"2014-07-15 Discuss Miguel Grinberg , author of Flask Web Development put together an excellent blog post on Designing a RESTful API with Python and Flask .","title":"Building REST APIs with Python Flask"},{"location":"python/install-the-latest-python-versions-on-macosx/","text":"Install the Latest Python Versions on Mac OSX \u00b6 2017-10-12 Discuss Once you have decided that Python is an awesome language to learn and you have heard all about the the cool features awaiting you, you are determined to install the latest versions on your laptop so that you can start developing. What\u2019s New In Python 3.6 - December 23, 2016 What\u2019s New In Python 3.5 - September 13, 2015 What\u2019s New In Python 3.4 - March 16, 2014 What\u2019s New In Python 3.3 - September 29, 2012 What\u2019s New In Python 3.2 - February 20, 2011 What\u2019s New In Python 3.1 - June 27, 2009 What\u2019s New In Python 3.0 - December 3, 2008 What\u2019s New In Python 2.7 - July 3, 2010 What\u2019s New In Python 2.6 - October 1, 2008 You may be a little concerned about the differences between Python 2 and Python 3, so you want to make sure that you have the latest versions of each available. This will enable you to switch between them easily and work on porting Python 2 programs to Python 3. Should I use Python 2 or Python 3 for my development activity? Key Differences between Python 2.7 and Python 3 with Examples Porting Python 2 Code to Python 3 Writing code that runs under both Python2 and 3 OS vendor Python versions lag behind the latest available and can only be updated by installing a major patch or bolting-on an additional Python installation: OS Version System Python Version OSX 10.13 2.7.10 OSX 10.12 2.7.10 OSX 10.11 2.7.10 Ubuntu 16.04 LTS Xenial Xerus 2.7.11 Ubuntu 14.04 LTS Trusty Tahr 2.7.5 Ubuntu 12.04 LTS Precise Pangolin 2.7.3 One way to work around this issue is to use pyenv to install the versions of Python you want to use. This tool provides a simplified build environment for installing the Pythons you want, while providing a set of shell script shims that makes it easy to switch between them. This tool was inspired by and forked from rbenv and ruby-build . Install Python with pyenv \u00b6 Start a Terminal session. Install Homebrew and Xcode Command Line Tools. xcode-select --install /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Install pyenv . brew install pyenv If you already have pyenv installed, you can upgrade it to gain access to the latest Python versions. The pyenv tool is updated periodically with new formulas for the latest releases. brew upgrade pyenv Add configuration to your .bash_profile to initialize pyenv every time you start a new Terminal. The PYENV_ROOT , which is where Python versions and packages will be installed, defaults to $HOME/.pyenv . cat >>$HOME/.bash_profile <<\"EOF\" if which pyenv > /dev/null 2&>1; then eval \"$(pyenv init -)\"; fi EOF Load your .bash_profile configuration changes. source $HOME/.bash_profile List the available Python versions. Notice that you have access to several different distributions: python.org (plain version numbers), anaconda, ironpython, jython, miniconda, pypy and stackless. We will install the standard Python versions released by python.org , otherwise known as CPython, because the interpreter is written in C. pyenv install --list If you are running OSX 10.13, you will need to set the following environment variables, when you install new Python versions. See #988 for more details. export CFLAGS=\"-I$(brew --prefix openssl)/include\" export LDFLAGS=\"-L$(brew --prefix openssl)/lib\" Install Python versions. pyenv install 2.7.14 pyenv install 3.6.3 List the available Python versions. pyenv versions Activate a Python version and verify that it is available. pyenv global 2.7.14 pyenv versions python -V Activate another Python version and verify that it is available. pyenv global 3.6.3 pyenv versions python -V If desired, activate multiple Python versions and verify that they are available. PEP 394 -- The \"python\" Command on Unix-Like Systems explains conventions for naming python binaries. pyenv global 2.7.14 3.6.3 pyenv versions python -V python2 -V python2.7 -V python3 -V python3.6 -V Create new directories for Python projects, add pyenv local version files and verify the Python versions. Your python version will automatically switch when you change into these directories. mkdir python2-project cd python2-project pyenv local 2.7.13 cat .python-version pyenv local python -V cd .. mkdir python3-project cd python3-project pyenv local 3.6.3 cat .python-version pyenv local python -V cd .. Useful Python Packages \u00b6 This is a small collecton of useful packages that will help get you started doing useful things with Python. Install flake8 , which is a static analyzer that enforces good Python coding style and alerts you to coding mistakes. Now that you have Python installed, you can use the pip command to install any additional modules that you want to use. You will need to install modules separately for each version of Python you are actively using. pip install flake8 Install advanced Python Read-Eval-Print-Loop (REPL) packages, which will make it easier to explore Python code, because they grant access to tab completion and fancy editing capabilities. The ipython tool is a command line REPL, which can be exited with ctrl-d . Jupyter Notebook is a browser-based REPL that operates on individual cells of code. The IPython Interactive Computing website has more information on these tools. pip install ipython jupyter ipython jupyter notebook Install Requests: HTTP for Humans , which makes it easy to consume HTTP services. See GitHub's REST API v3 documentation for more details on endpoints that are avaialble. pip install requests python >>> import requests >>> r = requests . get ( 'https://api.github.com/users/copperlight' ) >>> r . ok True >>> r . status_code 200 >>> r . headers [ 'content-type' ] 'application/json; charset=utf8' >>> r . encoding 'utf-8' >>> r . text u '{\"login\":\"copperlight\", ...}' >>> r . json () { u 'public_repos' : 27 , ... } >>> r . json ()[ 'login' ] u 'copperlight' Install Flask: A Python Microframework , which can be used to quickly build small websites that automate everyday tasks. pip install Flask from flask import Flask app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): return \"Hello World!\" FLASK_APP=hello.py flask run curl http://127.0.0.1:5000 If you need Python package isolation on a per-project basis, because you have conflicting sets of packages specified for different projects, you can use virtualenv to set up separate environments. The virtualenv tool establishes a clean room Python installation, either based upon the current version of Python in use, or a version specified on the command line. If you have multiple Python versions activated in pyenv , you can use virtualenv to switch between them easily. pip install virtualenv pip list mkdir python2-project cd python2-project virtualenv -p python2.7 venv source venv/bin/activate python -V which python pip list which pip deactivate cd .. mkdir python3-project cd python3-project virtualenv -p python3.6 venv source venv/bin/activate python -V which python pip list which pip deactivate cd .. Automated Testing with Python \u00b6 While working on Python programs, you may be interested in automating testing. The most common choice for implementing this is tox , which will allow you to run tests with multiple different versions of Python, if needed. Python Packaging \u00b6 After completing a Python program or two, you will be interested in learning how to distribute them effectively. The Python Packaging User Guide has some good advice on this topic. Quickstart Commands \u00b6 # if you need homebrew xcode-select --install /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" # install pyenv and latest major python versions brew install pyenv cat >>$HOME/.bash_profile <<\"EOF\" if which pyenv > /dev/null; then eval \"$(pyenv init -)\"; fi EOF source $HOME/.bash_profile pyenv install 2.7.14 pyenv install 3.6.3 pyenv versions","title":"Install the Latest Python Versions on Mac OSX"},{"location":"python/install-the-latest-python-versions-on-macosx/#install-the-latest-python-versions-on-mac-osx","text":"2017-10-12 Discuss Once you have decided that Python is an awesome language to learn and you have heard all about the the cool features awaiting you, you are determined to install the latest versions on your laptop so that you can start developing. What\u2019s New In Python 3.6 - December 23, 2016 What\u2019s New In Python 3.5 - September 13, 2015 What\u2019s New In Python 3.4 - March 16, 2014 What\u2019s New In Python 3.3 - September 29, 2012 What\u2019s New In Python 3.2 - February 20, 2011 What\u2019s New In Python 3.1 - June 27, 2009 What\u2019s New In Python 3.0 - December 3, 2008 What\u2019s New In Python 2.7 - July 3, 2010 What\u2019s New In Python 2.6 - October 1, 2008 You may be a little concerned about the differences between Python 2 and Python 3, so you want to make sure that you have the latest versions of each available. This will enable you to switch between them easily and work on porting Python 2 programs to Python 3. Should I use Python 2 or Python 3 for my development activity? Key Differences between Python 2.7 and Python 3 with Examples Porting Python 2 Code to Python 3 Writing code that runs under both Python2 and 3 OS vendor Python versions lag behind the latest available and can only be updated by installing a major patch or bolting-on an additional Python installation: OS Version System Python Version OSX 10.13 2.7.10 OSX 10.12 2.7.10 OSX 10.11 2.7.10 Ubuntu 16.04 LTS Xenial Xerus 2.7.11 Ubuntu 14.04 LTS Trusty Tahr 2.7.5 Ubuntu 12.04 LTS Precise Pangolin 2.7.3 One way to work around this issue is to use pyenv to install the versions of Python you want to use. This tool provides a simplified build environment for installing the Pythons you want, while providing a set of shell script shims that makes it easy to switch between them. This tool was inspired by and forked from rbenv and ruby-build .","title":"Install the Latest Python Versions on Mac OSX"},{"location":"python/install-the-latest-python-versions-on-macosx/#install-python-with-pyenv","text":"Start a Terminal session. Install Homebrew and Xcode Command Line Tools. xcode-select --install /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" Install pyenv . brew install pyenv If you already have pyenv installed, you can upgrade it to gain access to the latest Python versions. The pyenv tool is updated periodically with new formulas for the latest releases. brew upgrade pyenv Add configuration to your .bash_profile to initialize pyenv every time you start a new Terminal. The PYENV_ROOT , which is where Python versions and packages will be installed, defaults to $HOME/.pyenv . cat >>$HOME/.bash_profile <<\"EOF\" if which pyenv > /dev/null 2&>1; then eval \"$(pyenv init -)\"; fi EOF Load your .bash_profile configuration changes. source $HOME/.bash_profile List the available Python versions. Notice that you have access to several different distributions: python.org (plain version numbers), anaconda, ironpython, jython, miniconda, pypy and stackless. We will install the standard Python versions released by python.org , otherwise known as CPython, because the interpreter is written in C. pyenv install --list If you are running OSX 10.13, you will need to set the following environment variables, when you install new Python versions. See #988 for more details. export CFLAGS=\"-I$(brew --prefix openssl)/include\" export LDFLAGS=\"-L$(brew --prefix openssl)/lib\" Install Python versions. pyenv install 2.7.14 pyenv install 3.6.3 List the available Python versions. pyenv versions Activate a Python version and verify that it is available. pyenv global 2.7.14 pyenv versions python -V Activate another Python version and verify that it is available. pyenv global 3.6.3 pyenv versions python -V If desired, activate multiple Python versions and verify that they are available. PEP 394 -- The \"python\" Command on Unix-Like Systems explains conventions for naming python binaries. pyenv global 2.7.14 3.6.3 pyenv versions python -V python2 -V python2.7 -V python3 -V python3.6 -V Create new directories for Python projects, add pyenv local version files and verify the Python versions. Your python version will automatically switch when you change into these directories. mkdir python2-project cd python2-project pyenv local 2.7.13 cat .python-version pyenv local python -V cd .. mkdir python3-project cd python3-project pyenv local 3.6.3 cat .python-version pyenv local python -V cd ..","title":"Install Python with pyenv"},{"location":"python/install-the-latest-python-versions-on-macosx/#useful-python-packages","text":"This is a small collecton of useful packages that will help get you started doing useful things with Python. Install flake8 , which is a static analyzer that enforces good Python coding style and alerts you to coding mistakes. Now that you have Python installed, you can use the pip command to install any additional modules that you want to use. You will need to install modules separately for each version of Python you are actively using. pip install flake8 Install advanced Python Read-Eval-Print-Loop (REPL) packages, which will make it easier to explore Python code, because they grant access to tab completion and fancy editing capabilities. The ipython tool is a command line REPL, which can be exited with ctrl-d . Jupyter Notebook is a browser-based REPL that operates on individual cells of code. The IPython Interactive Computing website has more information on these tools. pip install ipython jupyter ipython jupyter notebook Install Requests: HTTP for Humans , which makes it easy to consume HTTP services. See GitHub's REST API v3 documentation for more details on endpoints that are avaialble. pip install requests python >>> import requests >>> r = requests . get ( 'https://api.github.com/users/copperlight' ) >>> r . ok True >>> r . status_code 200 >>> r . headers [ 'content-type' ] 'application/json; charset=utf8' >>> r . encoding 'utf-8' >>> r . text u '{\"login\":\"copperlight\", ...}' >>> r . json () { u 'public_repos' : 27 , ... } >>> r . json ()[ 'login' ] u 'copperlight' Install Flask: A Python Microframework , which can be used to quickly build small websites that automate everyday tasks. pip install Flask from flask import Flask app = Flask ( __name__ ) @app . route ( \"/\" ) def hello (): return \"Hello World!\" FLASK_APP=hello.py flask run curl http://127.0.0.1:5000 If you need Python package isolation on a per-project basis, because you have conflicting sets of packages specified for different projects, you can use virtualenv to set up separate environments. The virtualenv tool establishes a clean room Python installation, either based upon the current version of Python in use, or a version specified on the command line. If you have multiple Python versions activated in pyenv , you can use virtualenv to switch between them easily. pip install virtualenv pip list mkdir python2-project cd python2-project virtualenv -p python2.7 venv source venv/bin/activate python -V which python pip list which pip deactivate cd .. mkdir python3-project cd python3-project virtualenv -p python3.6 venv source venv/bin/activate python -V which python pip list which pip deactivate cd ..","title":"Useful Python Packages"},{"location":"python/install-the-latest-python-versions-on-macosx/#automated-testing-with-python","text":"While working on Python programs, you may be interested in automating testing. The most common choice for implementing this is tox , which will allow you to run tests with multiple different versions of Python, if needed.","title":"Automated Testing with Python"},{"location":"python/install-the-latest-python-versions-on-macosx/#python-packaging","text":"After completing a Python program or two, you will be interested in learning how to distribute them effectively. The Python Packaging User Guide has some good advice on this topic.","title":"Python Packaging"},{"location":"python/install-the-latest-python-versions-on-macosx/#quickstart-commands","text":"# if you need homebrew xcode-select --install /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" # install pyenv and latest major python versions brew install pyenv cat >>$HOME/.bash_profile <<\"EOF\" if which pyenv > /dev/null; then eval \"$(pyenv init -)\"; fi EOF source $HOME/.bash_profile pyenv install 2.7.14 pyenv install 3.6.3 pyenv versions","title":"Quickstart Commands"},{"location":"python/memory-profiling-with-pyrasite-and-heapy/","text":"Memory Profiling with Pyrasite and Heapy \u00b6 2017-09-17 Discuss Build or install Pyrasite from the develop branch in the GitHub repo. The PyPi package does not have the latest code, which allows you to control the IPC timeout. https://github.com/lmacken/pyrasite Install profiling tools. sudo apt-get update sudo apt-get install pyrasite guppy Enable ptrace, so that you can inject into the process. echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope Connect to the running process. export PYRASITE_IPC_TIMEOUT = 10 # default is 5 seconds / apps / python / bin / pyrasite - shell 3640 Pyrasite Shell 2.0 Connected to '...' Python 2.7 . 12 ( default , Nov 19 2016 , 06 : 48 : 10 ) [ GCC 5.4 . 0 20160609 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . ( DistantInteractiveConsole ) >>> Enumerate the threads in the current process. import sys , traceback for thread , frame in sys . _current_frames () . items (): print ( 'Thread 0x %x ' % thread ) traceback . print_stack ( frame ) print () Profile process memory with heapy. from guppy import hpy hp = hpy () hp . heap () Debug the pyrasite process, if needed. /apps/python/bin/pyrasite --verbose <pid> helloworld.py","title":"Memory Profiling with Pyrasite and Heapy"},{"location":"python/memory-profiling-with-pyrasite-and-heapy/#memory-profiling-with-pyrasite-and-heapy","text":"2017-09-17 Discuss Build or install Pyrasite from the develop branch in the GitHub repo. The PyPi package does not have the latest code, which allows you to control the IPC timeout. https://github.com/lmacken/pyrasite Install profiling tools. sudo apt-get update sudo apt-get install pyrasite guppy Enable ptrace, so that you can inject into the process. echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope Connect to the running process. export PYRASITE_IPC_TIMEOUT = 10 # default is 5 seconds / apps / python / bin / pyrasite - shell 3640 Pyrasite Shell 2.0 Connected to '...' Python 2.7 . 12 ( default , Nov 19 2016 , 06 : 48 : 10 ) [ GCC 5.4 . 0 20160609 ] on linux2 Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information . ( DistantInteractiveConsole ) >>> Enumerate the threads in the current process. import sys , traceback for thread , frame in sys . _current_frames () . items (): print ( 'Thread 0x %x ' % thread ) traceback . print_stack ( frame ) print () Profile process memory with heapy. from guppy import hpy hp = hpy () hp . heap () Debug the pyrasite process, if needed. /apps/python/bin/pyrasite --verbose <pid> helloworld.py","title":"Memory Profiling with Pyrasite and Heapy"},{"location":"python/stop-using-print-for-debugging/","text":"Stop Using \"print\" for Debugging \u00b6 2017-10-12 Discuss My favorite quickstart guide to the Python logging module, by Al Sweigart : Stop Using \"print\" for Debugging: A 5 Minute Quickstart Guide to Python\u2019s logging Module This is a slight tweak to that pattern, which adds the name of the logging source and an example of disabling selected log sources. import logging logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s') logging.getLogger('urllib3').setLevel(logging.CRITICAL) logging.debug('This is a log message.')","title":"Stop Using \"print\" for Debugging"},{"location":"python/stop-using-print-for-debugging/#stop-using-print-for-debugging","text":"2017-10-12 Discuss My favorite quickstart guide to the Python logging module, by Al Sweigart : Stop Using \"print\" for Debugging: A 5 Minute Quickstart Guide to Python\u2019s logging Module This is a slight tweak to that pattern, which adds the name of the logging source and an example of disabling selected log sources. import logging logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s') logging.getLogger('urllib3').setLevel(logging.CRITICAL) logging.debug('This is a log message.')","title":"Stop Using \"print\" for Debugging"},{"location":"recipes/galaxys-best-margarita/","text":"I don't recall where this came from, but the recipe is amazing and the original site is no longer on the Internet. Not a difficult one, but formulated during the great Margarita consumption in Phoenix AZ during the late 80's. Important Stuff \u00b6 Put these ingredients in the blender. Mix the ingredients for a short period, but do not \"blend\". A pulsating action with four or five jolts of the blender works the best. This recipe makes 3 and a half 12 ounce margaritas. Tequila - one cup (8 ounces) Triple Sec - a quarter cup Limeade Frozen Concentrate - half can (6 ounces) Lemon Juice - one fresh lemon Ice - two cups large cubes Water - a half cup Toss 4 ice cubes into a 12 ounce, salt rimmed glass, and then pour the margarita! Details \u00b6 Water \u00b6 Adjust the amount of water to alter the strength of the margarita mix. Do not adjust the alcohol measurements. If they are too strong, use one cup of water in the next batch instead of a half cup. Ice \u00b6 The most important thing is - DO NOT CRUSH the ice. Use large cubes in the blender. They will lose some size in the mixing process. Do not allow the ice to become a puree. Many people do not like their margaritas with crushed ice because it will freeze the roof of your mouth and can cause a headache. Lemon Juice \u00b6 Fresh lemon juice is a must. The bottled crap should be used only as a last resort, and only if the local grocery store is closed or more than 5 miles away. Make sure when you are juicing the lemon that no seeds get into the mix. Lime Juice Concentrate \u00b6 Minute Maid is a common brand but the only problem with Minute Maid is that there is too much pulp. I used to not recommend this brand but I am changing my tune. I have had quality problems with the lower-priced brands. Go with the Minute Maid . Tequila \u00b6 Any tequila will work in this recipe. Most people think they need the most expensive tequila. Not so with this mix. Cheap white tequila works great. My favorite is Sauza Giro Gold tequila because it is smooth and the price is low (about $9 US for 750 ml). I do not recommend Cuervo Gold because it is overpriced (about $14) and has a rough flavor. It still turns out OK in the margarita mix, but the Sauza is cheaper and better. Triple Sec \u00b6 Many triple sec brands have an orange flavor. I believe the best margarita is made with \"neutral\" triple sec. Try to use triple sec like Chateaux which is plain, and about $7 for 750 ml. It's a little more potent than the orange stuff. The problem with the orange triple sec is that it makes a margarita that is a little on the sweet side, and mixing orange flavor with lemon and lime just doesn't sound right to me. Salt Rim \u00b6 To create a salt rimmed glass, take one of the lemon rinds from above and rub the drinking surface of the glass so it is barely moist. Dip the edge of the glass into salt. Table salt will work, but larger size margarita (or kosher) salt is better. Garnish with a lime wedge, which is merely a decoration, although some folks like to squeeze the lime juice into their drink as it goes down the hatch. Some people do not like salt on the glass. Although 8 out of 10 like the salt, I have found that women are generally the ones who don't. You will make points with these folks if you can remember their preference in advance. This recipe is featured on the Bacardi web site and averages 4000 quality hits per month! I am a party-er and enjoy drinking, and sharing that experience with fellow party-ers like you. Let's Party! A Few Editorial Comments \u00b6 Some people think that you should use the best (and most expensive) liquor. This recipe is very inexpensive to make, but if you want to splurge for the better tequila, go for it! Expensive liquor makes the margarita only marginally better because of the neutralizing effect of the other ingredients. Since a margarita is a blend of several flavors, changing the tequila has limited affect. I realize it is presumptuous to call this recipe \"The Galaxy's Best.\" But I registered the name in Yahoo without much thought two years ago, and there it stays. It really should be named \"A Good Marg!\" - I'm sure everyone could agree to that. About Dreaded Hangovers \u00b6 There are two types: first there is the \"in-a-daze mild headache\" kind. This hangover is solved by drinking orange juice on an empty stomach in the morning. I am serious dammit - make sure you use real orange juice. Sixteen ounces will do. The bad kind of hangover cannot be cured - you know - barfing etc.","title":"Galaxy's Best Margarita"},{"location":"recipes/galaxys-best-margarita/#important-stuff","text":"Put these ingredients in the blender. Mix the ingredients for a short period, but do not \"blend\". A pulsating action with four or five jolts of the blender works the best. This recipe makes 3 and a half 12 ounce margaritas. Tequila - one cup (8 ounces) Triple Sec - a quarter cup Limeade Frozen Concentrate - half can (6 ounces) Lemon Juice - one fresh lemon Ice - two cups large cubes Water - a half cup Toss 4 ice cubes into a 12 ounce, salt rimmed glass, and then pour the margarita!","title":"Important Stuff"},{"location":"recipes/galaxys-best-margarita/#details","text":"","title":"Details"},{"location":"recipes/galaxys-best-margarita/#water","text":"Adjust the amount of water to alter the strength of the margarita mix. Do not adjust the alcohol measurements. If they are too strong, use one cup of water in the next batch instead of a half cup.","title":"Water"},{"location":"recipes/galaxys-best-margarita/#ice","text":"The most important thing is - DO NOT CRUSH the ice. Use large cubes in the blender. They will lose some size in the mixing process. Do not allow the ice to become a puree. Many people do not like their margaritas with crushed ice because it will freeze the roof of your mouth and can cause a headache.","title":"Ice"},{"location":"recipes/galaxys-best-margarita/#lemon-juice","text":"Fresh lemon juice is a must. The bottled crap should be used only as a last resort, and only if the local grocery store is closed or more than 5 miles away. Make sure when you are juicing the lemon that no seeds get into the mix.","title":"Lemon Juice"},{"location":"recipes/galaxys-best-margarita/#lime-juice-concentrate","text":"Minute Maid is a common brand but the only problem with Minute Maid is that there is too much pulp. I used to not recommend this brand but I am changing my tune. I have had quality problems with the lower-priced brands. Go with the Minute Maid .","title":"Lime Juice Concentrate"},{"location":"recipes/galaxys-best-margarita/#tequila","text":"Any tequila will work in this recipe. Most people think they need the most expensive tequila. Not so with this mix. Cheap white tequila works great. My favorite is Sauza Giro Gold tequila because it is smooth and the price is low (about $9 US for 750 ml). I do not recommend Cuervo Gold because it is overpriced (about $14) and has a rough flavor. It still turns out OK in the margarita mix, but the Sauza is cheaper and better.","title":"Tequila"},{"location":"recipes/galaxys-best-margarita/#triple-sec","text":"Many triple sec brands have an orange flavor. I believe the best margarita is made with \"neutral\" triple sec. Try to use triple sec like Chateaux which is plain, and about $7 for 750 ml. It's a little more potent than the orange stuff. The problem with the orange triple sec is that it makes a margarita that is a little on the sweet side, and mixing orange flavor with lemon and lime just doesn't sound right to me.","title":"Triple Sec"},{"location":"recipes/galaxys-best-margarita/#salt-rim","text":"To create a salt rimmed glass, take one of the lemon rinds from above and rub the drinking surface of the glass so it is barely moist. Dip the edge of the glass into salt. Table salt will work, but larger size margarita (or kosher) salt is better. Garnish with a lime wedge, which is merely a decoration, although some folks like to squeeze the lime juice into their drink as it goes down the hatch. Some people do not like salt on the glass. Although 8 out of 10 like the salt, I have found that women are generally the ones who don't. You will make points with these folks if you can remember their preference in advance. This recipe is featured on the Bacardi web site and averages 4000 quality hits per month! I am a party-er and enjoy drinking, and sharing that experience with fellow party-ers like you. Let's Party!","title":"Salt Rim"},{"location":"recipes/galaxys-best-margarita/#a-few-editorial-comments","text":"Some people think that you should use the best (and most expensive) liquor. This recipe is very inexpensive to make, but if you want to splurge for the better tequila, go for it! Expensive liquor makes the margarita only marginally better because of the neutralizing effect of the other ingredients. Since a margarita is a blend of several flavors, changing the tequila has limited affect. I realize it is presumptuous to call this recipe \"The Galaxy's Best.\" But I registered the name in Yahoo without much thought two years ago, and there it stays. It really should be named \"A Good Marg!\" - I'm sure everyone could agree to that.","title":"A Few Editorial Comments"},{"location":"recipes/galaxys-best-margarita/#about-dreaded-hangovers","text":"There are two types: first there is the \"in-a-daze mild headache\" kind. This hangover is solved by drinking orange juice on an empty stomach in the morning. I am serious dammit - make sure you use real orange juice. Sixteen ounces will do. The bad kind of hangover cannot be cured - you know - barfing etc.","title":"About Dreaded Hangovers"},{"location":"running/garmin-935-navigation/","text":"Introduction \u00b6 Out in the SF Bay Area, there are tons of trails which are fantastic for running. The Trailstompers site is an excellent guide, covering six distinct regions split into four difficulty levels. Descriptions of the trails, with some turn navigation advice and GPX tracks are provided for each of the trails. The tracks can be imported into Garmin watches and you can use the Navigation feature to help stay on course. There are two issues with using these tracks directly on the watch: When loading a GPX track for navigation, it takes awhile to process the course on the watch and it ends up having far too many points, throwing out many thousands of them. Garmin will make some guesses about what turn-by-turn navigation hints to display, but the ones that you get end up being either obvious or they do not occur when you need them. This page is a guide to a track processing process which allows you to add custom turn-by-turn waypoints to tracks and post-processes them so that they appear correctly on your Garmin watch. The following sites and tools are used: Trailstompers GPSies CourseTCX2Fit conversion tool from @CTCX2Fit Process \u00b6 Find a trail you like on Trailstompers and download the GPX track. Upload the GPX track to GPSies . Edit the track and then modify the track. The course title will display as-is on the Garmin. You can add a cue sheet, which will populate turn-by-turn hints on the track. The cue sheet feature is capable of adding waypoints to the course points. Add or delete waypoints, as desired. It is best to keep waypoints for forks in the trail. Waypoints cannot be manually added to course points - they must be placed alongside course points. Download the track and waypoints as a Garmin Course TCX file. Run CourseTCX2FIT and convert the TCX file to a FIT file. Connect to your Garmin via USB. Access the Garmin storage device and copy the FIT file to the GARMIN/COURSES directory. Eject the Garmin device. On your Garmin, load the course. Start > Select: Trail Run > Start Hold Up > Select: Navigation Select: Courses > Select: Named Course > Start Do Course > Start The Garmin has 15 characters to display course titles, so choose wisely.","title":"Garmin 935 Navigation"},{"location":"running/garmin-935-navigation/#introduction","text":"Out in the SF Bay Area, there are tons of trails which are fantastic for running. The Trailstompers site is an excellent guide, covering six distinct regions split into four difficulty levels. Descriptions of the trails, with some turn navigation advice and GPX tracks are provided for each of the trails. The tracks can be imported into Garmin watches and you can use the Navigation feature to help stay on course. There are two issues with using these tracks directly on the watch: When loading a GPX track for navigation, it takes awhile to process the course on the watch and it ends up having far too many points, throwing out many thousands of them. Garmin will make some guesses about what turn-by-turn navigation hints to display, but the ones that you get end up being either obvious or they do not occur when you need them. This page is a guide to a track processing process which allows you to add custom turn-by-turn waypoints to tracks and post-processes them so that they appear correctly on your Garmin watch. The following sites and tools are used: Trailstompers GPSies CourseTCX2Fit conversion tool from @CTCX2Fit","title":"Introduction"},{"location":"running/garmin-935-navigation/#process","text":"Find a trail you like on Trailstompers and download the GPX track. Upload the GPX track to GPSies . Edit the track and then modify the track. The course title will display as-is on the Garmin. You can add a cue sheet, which will populate turn-by-turn hints on the track. The cue sheet feature is capable of adding waypoints to the course points. Add or delete waypoints, as desired. It is best to keep waypoints for forks in the trail. Waypoints cannot be manually added to course points - they must be placed alongside course points. Download the track and waypoints as a Garmin Course TCX file. Run CourseTCX2FIT and convert the TCX file to a FIT file. Connect to your Garmin via USB. Access the Garmin storage device and copy the FIT file to the GARMIN/COURSES directory. Eject the Garmin device. On your Garmin, load the course. Start > Select: Trail Run > Start Hold Up > Select: Navigation Select: Courses > Select: Named Course > Start Do Course > Start The Garmin has 15 characters to display course titles, so choose wisely.","title":"Process"},{"location":"scala/intellij-configuration-tips/","text":"IntelliJ Configuration Tips \u00b6 2017-10-05 Discuss This page provides a few tips on configuring IntelliJ for working with Scala code. sbt Projects \u00b6 Sometimes, IntelliJ struggles to import sbt projects - the build phase will last forever (20 min is not uncommon), when it should complete in 3-4 minutes. The solution is to switch the sbt build process to use sbt shell. IntelliJ IDEA > Preferences Build, Execution, Deployment > Build Tools > sbt sbt projects > sbt shell > use for: project reload, builds Optimize Imports \u00b6 The preference for organizing imports is to keep them on a single line per import, rather than bunching them together with curly braces. You can enforce this behavior with IntelliJ by modifying the following configuration: IntelliJ IDEA > Preferences Code Style > Scala > Imports Uncheck: Collect imports with the same prefix into one import When you run Code > Optimize Imports on a source file, it will unbundle imports and leave them with a single one per line. Code Style \u00b6 Navigate to these configuration options as follows: IntelliJ IDEA > Preferences Editor > Code Style > Scala Method declaration parameters \u00b6 Uncheck: Align when multiline This prevents Intellij from indenting parameter lists when cutting and pasting code within a file. Method Signature Warnings \u00b6 Navigate to these configuration options as follows: IntelliJ IDEA > Preferences Editor > Inspections > Scala > Method Signature Method with accessor-like name has Unit result type \u00b6 The intent of this warning is to cover the following case: Methods that follow JavaBean naming contract for accessors are expected to have no side effects. However, methods with a result type of Unit are only executed for their side effects. Refer to Programming in Scala, 2.3 Define some functions This indicates poor naming of the method involved, but depending on the code you are working with, there may be multiple examples of this present. You may want to set this to be a weak warning until you can update method names to be more descriptive. Method with Unit result type is defined like procedure \u00b6 This inspection is disabled by default. It should be enabled. It is not recommended to use procedure-like syntax for methods with Unit return type. It is inconsistent, may lead to errors and will be deprecated in future versions of Scala. Reference: The talk \"Scala with Style\" by Martin Odersky at ScalaDays 2013 Hint: You can use Analyze / Run Inspection by Name (Ctrl+Alt+Shift+I) to apply this inspection to the whole project Scala Worksheet \u00b6 This is a good way to gain access to an interactive repl-style experience for evaluating Scala code. As long as the worksheet file is established within your main directory, you will have access to import all the dependencies of your project. However, the default configuration of the Scala Worksheet will not be able to find Akka actor resources ( SCL-9229 ). You will receive the following error: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka' The work-around for this issue is as follows: IntelliJ IDEA > Preferences Languages & Frameworks > Scala > Worksheet Uncheck: Run worksheet in the compiler process Zero-Latency Typing \u00b6 See Experimental Zero-latency Typing in IntelliJ IDEA 15 EAP for an explanation of this feature. To implement it, follow these steps: Help > Edit Custom Properties... Add editor.zero.latency.typing=true Restart IntelliJ At some point, this will become the default.","title":"IntelliJ Configuration Tips"},{"location":"scala/intellij-configuration-tips/#intellij-configuration-tips","text":"2017-10-05 Discuss This page provides a few tips on configuring IntelliJ for working with Scala code.","title":"IntelliJ Configuration Tips"},{"location":"scala/intellij-configuration-tips/#sbt-projects","text":"Sometimes, IntelliJ struggles to import sbt projects - the build phase will last forever (20 min is not uncommon), when it should complete in 3-4 minutes. The solution is to switch the sbt build process to use sbt shell. IntelliJ IDEA > Preferences Build, Execution, Deployment > Build Tools > sbt sbt projects > sbt shell > use for: project reload, builds","title":"sbt Projects"},{"location":"scala/intellij-configuration-tips/#optimize-imports","text":"The preference for organizing imports is to keep them on a single line per import, rather than bunching them together with curly braces. You can enforce this behavior with IntelliJ by modifying the following configuration: IntelliJ IDEA > Preferences Code Style > Scala > Imports Uncheck: Collect imports with the same prefix into one import When you run Code > Optimize Imports on a source file, it will unbundle imports and leave them with a single one per line.","title":"Optimize Imports"},{"location":"scala/intellij-configuration-tips/#code-style","text":"Navigate to these configuration options as follows: IntelliJ IDEA > Preferences Editor > Code Style > Scala","title":"Code Style"},{"location":"scala/intellij-configuration-tips/#method-declaration-parameters","text":"Uncheck: Align when multiline This prevents Intellij from indenting parameter lists when cutting and pasting code within a file.","title":"Method declaration parameters"},{"location":"scala/intellij-configuration-tips/#method-signature-warnings","text":"Navigate to these configuration options as follows: IntelliJ IDEA > Preferences Editor > Inspections > Scala > Method Signature","title":"Method Signature Warnings"},{"location":"scala/intellij-configuration-tips/#method-with-accessor-like-name-has-unit-result-type","text":"The intent of this warning is to cover the following case: Methods that follow JavaBean naming contract for accessors are expected to have no side effects. However, methods with a result type of Unit are only executed for their side effects. Refer to Programming in Scala, 2.3 Define some functions This indicates poor naming of the method involved, but depending on the code you are working with, there may be multiple examples of this present. You may want to set this to be a weak warning until you can update method names to be more descriptive.","title":"Method with accessor-like name has Unit result type"},{"location":"scala/intellij-configuration-tips/#method-with-unit-result-type-is-defined-like-procedure","text":"This inspection is disabled by default. It should be enabled. It is not recommended to use procedure-like syntax for methods with Unit return type. It is inconsistent, may lead to errors and will be deprecated in future versions of Scala. Reference: The talk \"Scala with Style\" by Martin Odersky at ScalaDays 2013 Hint: You can use Analyze / Run Inspection by Name (Ctrl+Alt+Shift+I) to apply this inspection to the whole project","title":"Method with Unit result type is defined like procedure"},{"location":"scala/intellij-configuration-tips/#scala-worksheet","text":"This is a good way to gain access to an interactive repl-style experience for evaluating Scala code. As long as the worksheet file is established within your main directory, you will have access to import all the dependencies of your project. However, the default configuration of the Scala Worksheet will not be able to find Akka actor resources ( SCL-9229 ). You will receive the following error: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'akka' The work-around for this issue is as follows: IntelliJ IDEA > Preferences Languages & Frameworks > Scala > Worksheet Uncheck: Run worksheet in the compiler process","title":"Scala Worksheet"},{"location":"scala/intellij-configuration-tips/#zero-latency-typing","text":"See Experimental Zero-latency Typing in IntelliJ IDEA 15 EAP for an explanation of this feature. To implement it, follow these steps: Help > Edit Custom Properties... Add editor.zero.latency.typing=true Restart IntelliJ At some point, this will become the default.","title":"Zero-Latency Typing"},{"location":"scala/jmh-testing-with-scala/","text":"2018-02-22 Discuss JMH is the Java Microbenchmark Harness provided by the OpenJDK project. It can be a useful tool for validating assumptions about the performance of small code sections subject to many iterations. See the following links for more details: OpenJDK Code Tools: jmh JMH Tutorial Gradle \u00b6 A Gradle plugin is available for JMH. Project layout: . \u251c\u2500\u2500 build.gradle \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 wrapper \u2502 \u251c\u2500\u2500 gradle-wrapper.jar \u2502 \u2514\u2500\u2500 gradle-wrapper.properties \u251c\u2500\u2500 gradlew \u251c\u2500\u2500 gradlew.bat \u2514\u2500\u2500 src \u2514\u2500\u2500 jmh \u2514\u2500\u2500 scala \u2514\u2500\u2500 io \u2514\u2500\u2500 github \u2514\u2500\u2500 brharrington \u251c\u2500\u2500 AtomicLongUpdate.scala \u251c\u2500\u2500 CurrentTimeMillis.scala \u251c\u2500\u2500 NanoTime.scala \u2514\u2500\u2500 TimeLimiting.scala The build.gradle configuration: plugins { id 'scala' id 'idea' id 'me.champeau.gradle.jmh' version '0.4.5' } repositories { jcenter() } dependencies { compile 'com.netflix.servo:servo-core:0.12.17' compile 'org.scala-lang:scala-library:2.12.4' } idea { module { testSourceDirs += sourceSets.jmh.scala.srcDirs } } Run the tests: ./gradlew jmh sbt \u00b6 An sbt plugin is available for JMH. Example project layout : . \u251c\u2500\u2500 build.sbt \u251c\u2500\u2500 project \u2502 \u251c\u2500\u2500 build.properties \u2502 \u251c\u2500\u2500 plugins.sbt \u2502 \u251c\u2500\u2500 sbt \u2502 \u2514\u2500\u2500 sbt-launch-1.0.0.jar \u2514\u2500\u2500 src \u2514\u2500\u2500 main \u2514\u2500\u2500 scala \u2514\u2500\u2500 io \u2514\u2500\u2500 github \u2514\u2500\u2500 brharrington \u251c\u2500\u2500 AtomicLongUpdate.scala \u251c\u2500\u2500 CurrentTimeMillis.scala \u251c\u2500\u2500 NanoTime.scala \u2514\u2500\u2500 TimeLimiting.scala The build.sbt configuration: name := \"misc-jmh\" organization := \"io.github.brharrington\" scalaVersion := \"2.12.4\" enablePlugins(JmhPlugin) libraryDependencies ++= Seq( \"com.netflix.servo\" % \"servo-core\" % \"0.12.17\" ) The plugins.sbt configuration: addSbtPlugin(\"pl.project13.scala\" % \"sbt-jmh\" % \"0.2.27\") Run the tests: ./project/sbt jmh:run Example Test Class \u00b6 package io.github.brharrington import java.util.concurrent.atomic.AtomicLong import org.openjdk.jmh.annotations.Benchmark import org.openjdk.jmh.annotations.Scope import org.openjdk.jmh.annotations.State import org.openjdk.jmh.infra.Blackhole @State(Scope.Thread) class AtomicLongUpdate { private val value = new AtomicLong() @Benchmark def incrementAndGet(bh: Blackhole): Unit = { bh.consume(value.incrementAndGet()) } @Benchmark def addAndGet(bh: Blackhole): Unit = { bh.consume(value.addAndGet(42)) } @Benchmark def get(bh: Blackhole): Unit = { bh.consume(value.get()) } }","title":"JMH Testing with Scala"},{"location":"scala/jmh-testing-with-scala/#gradle","text":"A Gradle plugin is available for JMH. Project layout: . \u251c\u2500\u2500 build.gradle \u251c\u2500\u2500 gradle \u2502 \u2514\u2500\u2500 wrapper \u2502 \u251c\u2500\u2500 gradle-wrapper.jar \u2502 \u2514\u2500\u2500 gradle-wrapper.properties \u251c\u2500\u2500 gradlew \u251c\u2500\u2500 gradlew.bat \u2514\u2500\u2500 src \u2514\u2500\u2500 jmh \u2514\u2500\u2500 scala \u2514\u2500\u2500 io \u2514\u2500\u2500 github \u2514\u2500\u2500 brharrington \u251c\u2500\u2500 AtomicLongUpdate.scala \u251c\u2500\u2500 CurrentTimeMillis.scala \u251c\u2500\u2500 NanoTime.scala \u2514\u2500\u2500 TimeLimiting.scala The build.gradle configuration: plugins { id 'scala' id 'idea' id 'me.champeau.gradle.jmh' version '0.4.5' } repositories { jcenter() } dependencies { compile 'com.netflix.servo:servo-core:0.12.17' compile 'org.scala-lang:scala-library:2.12.4' } idea { module { testSourceDirs += sourceSets.jmh.scala.srcDirs } } Run the tests: ./gradlew jmh","title":"Gradle"},{"location":"scala/jmh-testing-with-scala/#sbt","text":"An sbt plugin is available for JMH. Example project layout : . \u251c\u2500\u2500 build.sbt \u251c\u2500\u2500 project \u2502 \u251c\u2500\u2500 build.properties \u2502 \u251c\u2500\u2500 plugins.sbt \u2502 \u251c\u2500\u2500 sbt \u2502 \u2514\u2500\u2500 sbt-launch-1.0.0.jar \u2514\u2500\u2500 src \u2514\u2500\u2500 main \u2514\u2500\u2500 scala \u2514\u2500\u2500 io \u2514\u2500\u2500 github \u2514\u2500\u2500 brharrington \u251c\u2500\u2500 AtomicLongUpdate.scala \u251c\u2500\u2500 CurrentTimeMillis.scala \u251c\u2500\u2500 NanoTime.scala \u2514\u2500\u2500 TimeLimiting.scala The build.sbt configuration: name := \"misc-jmh\" organization := \"io.github.brharrington\" scalaVersion := \"2.12.4\" enablePlugins(JmhPlugin) libraryDependencies ++= Seq( \"com.netflix.servo\" % \"servo-core\" % \"0.12.17\" ) The plugins.sbt configuration: addSbtPlugin(\"pl.project13.scala\" % \"sbt-jmh\" % \"0.2.27\") Run the tests: ./project/sbt jmh:run","title":"sbt"},{"location":"scala/jmh-testing-with-scala/#example-test-class","text":"package io.github.brharrington import java.util.concurrent.atomic.AtomicLong import org.openjdk.jmh.annotations.Benchmark import org.openjdk.jmh.annotations.Scope import org.openjdk.jmh.annotations.State import org.openjdk.jmh.infra.Blackhole @State(Scope.Thread) class AtomicLongUpdate { private val value = new AtomicLong() @Benchmark def incrementAndGet(bh: Blackhole): Unit = { bh.consume(value.incrementAndGet()) } @Benchmark def addAndGet(bh: Blackhole): Unit = { bh.consume(value.addAndGet(42)) } @Benchmark def get(bh: Blackhole): Unit = { bh.consume(value.get()) } }","title":"Example Test Class"},{"location":"scala/overview-of-debugging-tools/","text":"Overview of Debugging Tools \u00b6 2016-10-13 Discuss Tool Use Cases Java Flight Recorder first choice for GC allocation pauses and heap pressure first choice for thread contention Java Flame Graphs first choice for CPU contention VisualVM second choice for CPU contention YourKit Profiler third choice for CPU contention, other options are more accessible not as good as flight recorder for heap pressure Java Micro Harness (JMH) first choice for writing tiny benchmarks to evaluate code samples","title":"Overview of Debugging Tools"},{"location":"scala/overview-of-debugging-tools/#overview-of-debugging-tools","text":"2016-10-13 Discuss Tool Use Cases Java Flight Recorder first choice for GC allocation pauses and heap pressure first choice for thread contention Java Flame Graphs first choice for CPU contention VisualVM second choice for CPU contention YourKit Profiler third choice for CPU contention, other options are more accessible not as good as flight recorder for heap pressure Java Micro Harness (JMH) first choice for writing tiny benchmarks to evaluate code samples","title":"Overview of Debugging Tools"},{"location":"scala/scalafmt-configuration-tips/","text":"Scalafmt Configuration Tips \u00b6 2018-02-22 Discuss Scalafmt is a code formatter for Scala. It is available as an IntelliJ plugin and a CLI version can be installed using Homebrew . brew install --HEAD olafurpg/scalafmt/scalafmt Configuration \u00b6 The following .scalafmt.conf configuration is recommended for use with Scala projects. It should be created in the project's root directory. style = defaultWithAlign align.openParenCallSite = false align.openParenDefnSite = false align.tokens = [{code = \"->\"}, {code = \"<-\"}, {code = \"=>\", owner = \"Case\"}] continuationIndent.callSite = 2 continuationIndent.defnSite = 2 danglingParentheses = true indentOperator = spray maxColumn = 100 newlines.alwaysBeforeTopLevelStatements = true project.excludeFilters = [\".*\\\\.sbt\"] rewrite.rules = [RedundantParens, SortImports] spaces.inImportCurlyBraces = false unindentTopLevelOperators = true The maxColumn value is chosen so that it is easy to do side-by-side views of code and tests, while also avoiding wrapping in diffs on GitHub and Stash. Alignment is allowed for case statements => and map assignments -> and it is disallowed for assignment = . This option taken together with the openParen.*Site configuration minimizes the amount of whitespace fiddling that will occur in code changes, which should lead to more readable diffs. Usage Tips \u00b6 Blank lines separate alignment blocks. For sections of code that require formatting which does not follow the scalafmt conventions, use format:off blocks to disable the formatter. When formatter changes include altering whitespace to align operators, it can make diffs more difficult to read. You can use git diff -w to ignore whitespace changes. To enable formatting on file save in IntelliJ: IntelliJ IDEA > Preferences > Tools > Scalafmt > check: Format on file save. Enforce Formatting \u00b6 Gradle \u00b6 A Scalafmt Gradle plugin is available, which can be used to enforce scalafmt conventions for your project. It expects the configuration file to be located in the root of your project. Add the following to your build.gradle file: buildscript { dependencies { classpath 'cz.alenkacz:gradle-scalafmt:1.6.0' } } apply plugin: 'scalafmt' This adds a task checkScalafmtAll to your project, which can be used to verify that your project files adhere to the established configuration. Sbt \u00b6 A Scalafmt sbt plugin is available, which can be used to enforce scalafmt conventions for your project. It expects the configuration file to be located in the root of your project. scalafmt::test test:scalafmt::test","title":"Scalafmt Configuration Tips"},{"location":"scala/scalafmt-configuration-tips/#scalafmt-configuration-tips","text":"2018-02-22 Discuss Scalafmt is a code formatter for Scala. It is available as an IntelliJ plugin and a CLI version can be installed using Homebrew . brew install --HEAD olafurpg/scalafmt/scalafmt","title":"Scalafmt Configuration Tips"},{"location":"scala/scalafmt-configuration-tips/#configuration","text":"The following .scalafmt.conf configuration is recommended for use with Scala projects. It should be created in the project's root directory. style = defaultWithAlign align.openParenCallSite = false align.openParenDefnSite = false align.tokens = [{code = \"->\"}, {code = \"<-\"}, {code = \"=>\", owner = \"Case\"}] continuationIndent.callSite = 2 continuationIndent.defnSite = 2 danglingParentheses = true indentOperator = spray maxColumn = 100 newlines.alwaysBeforeTopLevelStatements = true project.excludeFilters = [\".*\\\\.sbt\"] rewrite.rules = [RedundantParens, SortImports] spaces.inImportCurlyBraces = false unindentTopLevelOperators = true The maxColumn value is chosen so that it is easy to do side-by-side views of code and tests, while also avoiding wrapping in diffs on GitHub and Stash. Alignment is allowed for case statements => and map assignments -> and it is disallowed for assignment = . This option taken together with the openParen.*Site configuration minimizes the amount of whitespace fiddling that will occur in code changes, which should lead to more readable diffs.","title":"Configuration"},{"location":"scala/scalafmt-configuration-tips/#usage-tips","text":"Blank lines separate alignment blocks. For sections of code that require formatting which does not follow the scalafmt conventions, use format:off blocks to disable the formatter. When formatter changes include altering whitespace to align operators, it can make diffs more difficult to read. You can use git diff -w to ignore whitespace changes. To enable formatting on file save in IntelliJ: IntelliJ IDEA > Preferences > Tools > Scalafmt > check: Format on file save.","title":"Usage Tips"},{"location":"scala/scalafmt-configuration-tips/#enforce-formatting","text":"","title":"Enforce Formatting"},{"location":"scala/scalafmt-configuration-tips/#gradle","text":"A Scalafmt Gradle plugin is available, which can be used to enforce scalafmt conventions for your project. It expects the configuration file to be located in the root of your project. Add the following to your build.gradle file: buildscript { dependencies { classpath 'cz.alenkacz:gradle-scalafmt:1.6.0' } } apply plugin: 'scalafmt' This adds a task checkScalafmtAll to your project, which can be used to verify that your project files adhere to the established configuration.","title":"Gradle"},{"location":"scala/scalafmt-configuration-tips/#sbt","text":"A Scalafmt sbt plugin is available, which can be used to enforce scalafmt conventions for your project. It expects the configuration file to be located in the root of your project. scalafmt::test test:scalafmt::test","title":"Sbt"},{"location":"scala/starting-scala-repls-with-gradle-and-sbt/","text":"Starting Scala REPLs with Gradle and SBT \u00b6 2015-07-04 Discuss These examples are tailored for usage with the Atomic Scala exercises, but they demonstrate the principles. build.gradle apply plugin: 'scala' repositories { mavenCentral () } ext { versions = [ commons_math3: '3.5' , jline: '2.12.1' , scala: '2.11.6' ] } dependencies { compile \"org.apache.commons:commons-math3:${versions.commons_math3}\" compile \"org.scala-lang:scala-library:${versions.scala}\" runtime \"jline:jline:${versions.jline}\" runtime \"org.scala-lang:scala-compiler:${versions.scala}\" } sourceSets { main { scala { srcDirs = [ '.' ] include 'AtomicTest.scala' } } } scalaConsole . dependsOn ( build ) scalaConsole . classpath += sourceSets . main . runtimeClasspath // usage: gradlew scalaConsole -q build.sbt scalaVersion := \"2.11.6\" sources in Compile <<= ( sources in Compile ). map ( _ filter ( _ . name == \"AtomicTest.scala\" )) libraryDependencies += \"org.apache.commons\" % \"commons-math3\" % \"3.5\" initialCommands in console := \"\"\" |import org.apache.commons.math3._ |import com.atomicscala.AtomicTest._ |\"\"\" . stripMargin // usage: sbt console","title":"Starting Scala REPLs with Gradle and SBT"},{"location":"scala/starting-scala-repls-with-gradle-and-sbt/#starting-scala-repls-with-gradle-and-sbt","text":"2015-07-04 Discuss These examples are tailored for usage with the Atomic Scala exercises, but they demonstrate the principles. build.gradle apply plugin: 'scala' repositories { mavenCentral () } ext { versions = [ commons_math3: '3.5' , jline: '2.12.1' , scala: '2.11.6' ] } dependencies { compile \"org.apache.commons:commons-math3:${versions.commons_math3}\" compile \"org.scala-lang:scala-library:${versions.scala}\" runtime \"jline:jline:${versions.jline}\" runtime \"org.scala-lang:scala-compiler:${versions.scala}\" } sourceSets { main { scala { srcDirs = [ '.' ] include 'AtomicTest.scala' } } } scalaConsole . dependsOn ( build ) scalaConsole . classpath += sourceSets . main . runtimeClasspath // usage: gradlew scalaConsole -q build.sbt scalaVersion := \"2.11.6\" sources in Compile <<= ( sources in Compile ). map ( _ filter ( _ . name == \"AtomicTest.scala\" )) libraryDependencies += \"org.apache.commons\" % \"commons-math3\" % \"3.5\" initialCommands in console := \"\"\" |import org.apache.commons.math3._ |import com.atomicscala.AtomicTest._ |\"\"\" . stripMargin // usage: sbt console","title":"Starting Scala REPLs with Gradle and SBT"},{"location":"scala/testing-aws-clients-with-iam-assumerole-credentials-in-scala/","text":"Testing AWS Clients with IAM AssumeRole Credentials in Scala \u00b6 2020-01-06 Discuss This technique is meant to be used with IntelliJ Scala worksheets or similar scratch code, so you can test clients and validate their behavior. Don't check-in secrets \u2013 use on-instance credentials with the DefaultAWSCredentialsProviderChain in production code. import com . amazonaws . auth . AWSStaticCredentialsProvider import com . amazonaws . auth . BasicSessionCredentials import com . amazonaws . regions . Regions import com . amazonaws . services . autoscaling . AmazonAutoScalingClientBuilder import com . amazonaws . services . securitytoken . AWSSecurityTokenServiceClientBuilder import com . amazonaws . services . securitytoken . model . AssumeRoleRequest // config values val accessKeyId = \"\" val secretAccessKey = \"\" val token = \"\" val role : Option [ String ] = None val accountId = \"\" val region = Regions . US_WEST_1 // client configuration val staticProvider = { role . fold { val basic = new BasicSessionCredentials ( accessKeyId , secretAccessKey , token ) new AWSStaticCredentialsProvider ( basic ) } { role => val instanceProvider = { val basic = new BasicSessionCredentials ( accessKeyId , secretAccessKey , token ) new AWSStaticCredentialsProvider ( basic ) } val stsClient = AWSSecurityTokenServiceClientBuilder . standard () . withCredentials ( instanceProvider ) . withRegion ( region ) . build () val req = new AssumeRoleRequest () . withRoleSessionName ( s\" $ role -testing\" ) . withRoleArn ( s\"arn:aws:iam:: $ accountId :role/ $ role \" ) val assumedCreds = stsClient . assumeRole ( req ). getCredentials val basic = new BasicSessionCredentials ( assumedCreds . getAccessKeyId , assumedCreds . getSecretAccessKey , assumedCreds . getSessionToken ) new AWSStaticCredentialsProvider ( basic ) } } val client = AmazonAutoScalingClientBuilder . standard () . withCredentials ( staticProvider ) . withRegion ( region ) . build ()","title":"Testing AWS Clients with IAM AssumeRole Credentials in Scala"},{"location":"scala/testing-aws-clients-with-iam-assumerole-credentials-in-scala/#testing-aws-clients-with-iam-assumerole-credentials-in-scala","text":"2020-01-06 Discuss This technique is meant to be used with IntelliJ Scala worksheets or similar scratch code, so you can test clients and validate their behavior. Don't check-in secrets \u2013 use on-instance credentials with the DefaultAWSCredentialsProviderChain in production code. import com . amazonaws . auth . AWSStaticCredentialsProvider import com . amazonaws . auth . BasicSessionCredentials import com . amazonaws . regions . Regions import com . amazonaws . services . autoscaling . AmazonAutoScalingClientBuilder import com . amazonaws . services . securitytoken . AWSSecurityTokenServiceClientBuilder import com . amazonaws . services . securitytoken . model . AssumeRoleRequest // config values val accessKeyId = \"\" val secretAccessKey = \"\" val token = \"\" val role : Option [ String ] = None val accountId = \"\" val region = Regions . US_WEST_1 // client configuration val staticProvider = { role . fold { val basic = new BasicSessionCredentials ( accessKeyId , secretAccessKey , token ) new AWSStaticCredentialsProvider ( basic ) } { role => val instanceProvider = { val basic = new BasicSessionCredentials ( accessKeyId , secretAccessKey , token ) new AWSStaticCredentialsProvider ( basic ) } val stsClient = AWSSecurityTokenServiceClientBuilder . standard () . withCredentials ( instanceProvider ) . withRegion ( region ) . build () val req = new AssumeRoleRequest () . withRoleSessionName ( s\" $ role -testing\" ) . withRoleArn ( s\"arn:aws:iam:: $ accountId :role/ $ role \" ) val assumedCreds = stsClient . assumeRole ( req ). getCredentials val basic = new BasicSessionCredentials ( assumedCreds . getAccessKeyId , assumedCreds . getSecretAccessKey , assumedCreds . getSessionToken ) new AWSStaticCredentialsProvider ( basic ) } } val client = AmazonAutoScalingClientBuilder . standard () . withCredentials ( staticProvider ) . withRegion ( region ) . build ()","title":"Testing AWS Clients with IAM AssumeRole Credentials in Scala"},{"location":"scala/using-gradle-to-run-scala-projects-locally/","text":"Using Gradle to Run Scala Projects Locally \u00b6 2017-10-12 Discuss Running projects locally in a minimal fashion is often useful for understanding the code, using a debugger and performing interactive integration testing. You can run REPLs with ./gradlew , but due to the input fiddling that is going on, it's very distracting and not effective, especially if you need to build up anything more than simple state. You will be better served by using the IntelliJ Scala Worksheet , which makes it easy to gain access to the libraries you have included in your build. buildscript { dependencies { classpath \"gradle.plugin.com.github.maiflai:gradle-scalatest:0.14\" classpath \"org.akhikhl.gretty:gretty:2.0.0\" } } apply plugin: 'com.github.maiflai.scalatest' apply plugin: 'org.akhikhl.gretty' tasks . withType ( ScalaCompile ) { scalaCompileOptions . setAdditionalParameters ([ '-deprecation' , '-unchecked' , '-Xexperimental' , '-Xlint:_,-infer-any' , '-feature' , '-Ydelambdafy:method' ]) } // for servlet applications gretty { httpPort = 7101 contextPath = '/' servletContainer = 'jetty7' jvmArgs = [ \"-Dlog4j.configurationFile=\" + System . getProperty ( \"user.dir\" ) + \"/src/main/resources/log4j_dev.xml\" ] } // for main class applications task runService ( dependsOn: 'classes' , type: JavaExec ) { main = 'com.example.app.Main' classpath sourceSets . main . runtimeClasspath } ./gradlew appRun ./gradlew runService","title":"Using Gradle to Run Scala Projects Locally"},{"location":"scala/using-gradle-to-run-scala-projects-locally/#using-gradle-to-run-scala-projects-locally","text":"2017-10-12 Discuss Running projects locally in a minimal fashion is often useful for understanding the code, using a debugger and performing interactive integration testing. You can run REPLs with ./gradlew , but due to the input fiddling that is going on, it's very distracting and not effective, especially if you need to build up anything more than simple state. You will be better served by using the IntelliJ Scala Worksheet , which makes it easy to gain access to the libraries you have included in your build. buildscript { dependencies { classpath \"gradle.plugin.com.github.maiflai:gradle-scalatest:0.14\" classpath \"org.akhikhl.gretty:gretty:2.0.0\" } } apply plugin: 'com.github.maiflai.scalatest' apply plugin: 'org.akhikhl.gretty' tasks . withType ( ScalaCompile ) { scalaCompileOptions . setAdditionalParameters ([ '-deprecation' , '-unchecked' , '-Xexperimental' , '-Xlint:_,-infer-any' , '-feature' , '-Ydelambdafy:method' ]) } // for servlet applications gretty { httpPort = 7101 contextPath = '/' servletContainer = 'jetty7' jvmArgs = [ \"-Dlog4j.configurationFile=\" + System . getProperty ( \"user.dir\" ) + \"/src/main/resources/log4j_dev.xml\" ] } // for main class applications task runService ( dependsOn: 'classes' , type: JavaExec ) { main = 'com.example.app.Main' classpath sourceSets . main . runtimeClasspath } ./gradlew appRun ./gradlew runService","title":"Using Gradle to Run Scala Projects Locally"},{"location":"scala/using-the-scala-repl-to-configure-amazon-ses-notifications/","text":"Using the Scala REPL to Configure Amazon SES Notifications \u00b6 2014-06-30 Discuss Let's say you want to configure bounce notifications for SES emails using the Scala REPL. How do you go about accessing the AWS API to make it happen? Make sure to have the AWS SDK for Java API Reference available for additional details. Install Scala and SBT: brew install scala sbt Create an AWS credentials file at ~/.aws/credentials that is formatted like so: [default] aws_access_key_id = ... aws_secret_access_key = ... aws_session_token = ... Create a build.sbt file that looks like: name := \"aws-sdk-client\" version := \"1.0\" scalaVersion := \"2.11.0\" libraryDependencies ++= Seq ( \"commons-logging\" % \"commons-logging\" % \"1.1.3\" , \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\" ) You can then engage the Scala REPL to talk to the AWS API like so (Scala will download the AWS SDK from Maven Central automatically): sbt console import com . amazonaws . auth . _ import com . amazonaws . auth . profile . _ import com . amazonaws . services . simpleemail . _ import com . amazonaws . services . simpleemail . model . _ import com . amazonaws . services . sns . _ import com . amazonaws . services . sns . model . _ val c = new AmazonSNSClient ( new AWSCredentialsProviderChain ( // attempt on-instance credentials first new InstanceProfileCredentialsProvider (), // fallback to aws credentials file new ProfileCredentialsProvider () ) ) c . setEndpoint ( \"sns.us-east-1.amazonaws.com\" ) c . createTopic ( \"ses-email-bounce\" ) res0 : com . amazonaws . services . sns . model . CreateTopicResult = { TopicArn : arn : aws : sns : us - east - 1 : 000000000000 : ses - email - bounce } val c = new AmazonSimpleEmailServiceClient ( new AWSCredentialsProviderChain ( // attempt on-instance credentials first new InstanceProfileCredentialsProvider (), // fallback to aws credentials file new ProfileCredentialsProvider () ) ) c . setEndpoint ( \"email.us-east-1.amazonaws.com\" ) c . getSendQuota () res1 : com . amazonaws . services . simpleemail . model . GetSendQuotaResult = { Max24HourSend : 100.0 , MaxSendRate : 10.0 , SentLast24Hours : 1.0 } c . getSendStatistics () res2 : com . amazonaws . services . simpleemail . model . GetSendStatisticsResult = { SendDataPoints : [{ Timestamp : ..., DeliveryAttempts : 0 , Bounces : 0 , Complaints : 0 , Rejects : 0 }, ... c . getIdentityVerificationAttributes ( new GetIdentityVerificationAttributesRequest () . withIdentities ( \"some.email@example.com\" ) ) res3 : com . amazonaws . services . simpleemail . model . GetIdentityVerificationAttributesResult = { VerificationAttributes : { some . email @mydomain . com = { VerificationStatus : Success ,}}} c . setIdentityNotificationTopic ( new SetIdentityNotificationTopicRequest () . withIdentity ( \"some.email@example.com\" ) . withNotificationType ( \"Bounce\" ) . withSnsTopic ( s\"arn:aws:sns:us-east-1: $ accountId :ses-email-bounce\" ) ) c . getIdentityNotificationAttributes ( new GetIdentityNotificationAttributesRequest () . withIdentities ( \"some.email@example.com\" ) ) One of the nice things about using the Scala REPL is that you get some useful tab completions (SES object here): scala> c. addRequestHandler sendEmail asInstanceOf sendRawEmail deleteIdentity setConfiguration deleteVerifiedEmailAddress setEndpoint getCachedResponseMetadata setIdentityDkimEnabled getIdentityDkimAttributes setIdentityFeedbackForwardingEnabled getIdentityNotificationAttributes setIdentityNotificationTopic getIdentityVerificationAttributes setRegion getRequestMetricsCollector setServiceNameIntern getSendQuota setSignerRegionOverride getSendStatistics setTimeOffset getServiceName shutdown getSignerByURI toString getSignerRegionOverride verifyDomainDkim getTimeOffset verifyDomainIdentity isInstanceOf verifyEmailAddress listIdentities verifyEmailIdentity listVerifiedEmailAddresses withTimeOffset removeRequestHandler Once you have a feel for how the Scala REPL works with a library, you can create alternate entry point for sbt called scalas which allows you to write full Scala scripts with library dependencies. See Scripts, REPL, and Dependencies for additional details. In brief, create a scalas script and make it available on your PATH : #!/bin/sh test -f ~/.sbtconfig && . ~/.sbtconfig exec java \\ -Xms512M \\ -Xmx1536M \\ -Xss1M \\ -XX:+CMSClassUnloadingEnabled \\ -XX:MaxPermSize = 256M \\ ${ SBT_OPTS } \\ -jar /usr/local/Cellar/sbt/0.13.2/libexec/sbt-launch.jar \\ -Dsbt.main.class = sbt.ScriptMain \\ \" $@ \" You can then write Scala scripts like so: #!/usr/bin/env scalas /*** scalaVersion := \"2.11.0\" libraryDependencies ++= Seq( \"commons-logging\" % \"commons-logging\" % \"1.1.3\", \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\" ) */ import com . amazonaws . auth . _ import com . amazonaws . auth . profile . _ import com . amazonaws . services . simpleemail . _ import com . amazonaws . services . simpleemail . model . _ val c = new AmazonSimpleEmailServiceClient ( new AWSCredentialsProviderChain ( // attempt on-instance credentials first new InstanceProfileCredentialsProvider (), // fallback to aws credentials file new ProfileCredentialsProvider () ) ) c . setEndpoint ( \"email.us-east-1.amazonaws.com\" ) println ( c . getSendQuota ()) println ( c . getSendStatistics (). getSendDataPoints . get ( 0 )) println ( c . getSendStatistics (). getSendDataPoints . size ) Execute the script as follows: $ ./demo.scala . . . {Timestamp: Sat Jul 05 04:01:00 PDT 2014,DeliveryAttempts: 3,Bounces: 0,Complaints: 0,Rejects: 0} 1301","title":"Using the Scala REPL to Configure Amazon SES Notifications"},{"location":"scala/using-the-scala-repl-to-configure-amazon-ses-notifications/#using-the-scala-repl-to-configure-amazon-ses-notifications","text":"2014-06-30 Discuss Let's say you want to configure bounce notifications for SES emails using the Scala REPL. How do you go about accessing the AWS API to make it happen? Make sure to have the AWS SDK for Java API Reference available for additional details. Install Scala and SBT: brew install scala sbt Create an AWS credentials file at ~/.aws/credentials that is formatted like so: [default] aws_access_key_id = ... aws_secret_access_key = ... aws_session_token = ... Create a build.sbt file that looks like: name := \"aws-sdk-client\" version := \"1.0\" scalaVersion := \"2.11.0\" libraryDependencies ++= Seq ( \"commons-logging\" % \"commons-logging\" % \"1.1.3\" , \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\" ) You can then engage the Scala REPL to talk to the AWS API like so (Scala will download the AWS SDK from Maven Central automatically): sbt console import com . amazonaws . auth . _ import com . amazonaws . auth . profile . _ import com . amazonaws . services . simpleemail . _ import com . amazonaws . services . simpleemail . model . _ import com . amazonaws . services . sns . _ import com . amazonaws . services . sns . model . _ val c = new AmazonSNSClient ( new AWSCredentialsProviderChain ( // attempt on-instance credentials first new InstanceProfileCredentialsProvider (), // fallback to aws credentials file new ProfileCredentialsProvider () ) ) c . setEndpoint ( \"sns.us-east-1.amazonaws.com\" ) c . createTopic ( \"ses-email-bounce\" ) res0 : com . amazonaws . services . sns . model . CreateTopicResult = { TopicArn : arn : aws : sns : us - east - 1 : 000000000000 : ses - email - bounce } val c = new AmazonSimpleEmailServiceClient ( new AWSCredentialsProviderChain ( // attempt on-instance credentials first new InstanceProfileCredentialsProvider (), // fallback to aws credentials file new ProfileCredentialsProvider () ) ) c . setEndpoint ( \"email.us-east-1.amazonaws.com\" ) c . getSendQuota () res1 : com . amazonaws . services . simpleemail . model . GetSendQuotaResult = { Max24HourSend : 100.0 , MaxSendRate : 10.0 , SentLast24Hours : 1.0 } c . getSendStatistics () res2 : com . amazonaws . services . simpleemail . model . GetSendStatisticsResult = { SendDataPoints : [{ Timestamp : ..., DeliveryAttempts : 0 , Bounces : 0 , Complaints : 0 , Rejects : 0 }, ... c . getIdentityVerificationAttributes ( new GetIdentityVerificationAttributesRequest () . withIdentities ( \"some.email@example.com\" ) ) res3 : com . amazonaws . services . simpleemail . model . GetIdentityVerificationAttributesResult = { VerificationAttributes : { some . email @mydomain . com = { VerificationStatus : Success ,}}} c . setIdentityNotificationTopic ( new SetIdentityNotificationTopicRequest () . withIdentity ( \"some.email@example.com\" ) . withNotificationType ( \"Bounce\" ) . withSnsTopic ( s\"arn:aws:sns:us-east-1: $ accountId :ses-email-bounce\" ) ) c . getIdentityNotificationAttributes ( new GetIdentityNotificationAttributesRequest () . withIdentities ( \"some.email@example.com\" ) ) One of the nice things about using the Scala REPL is that you get some useful tab completions (SES object here): scala> c. addRequestHandler sendEmail asInstanceOf sendRawEmail deleteIdentity setConfiguration deleteVerifiedEmailAddress setEndpoint getCachedResponseMetadata setIdentityDkimEnabled getIdentityDkimAttributes setIdentityFeedbackForwardingEnabled getIdentityNotificationAttributes setIdentityNotificationTopic getIdentityVerificationAttributes setRegion getRequestMetricsCollector setServiceNameIntern getSendQuota setSignerRegionOverride getSendStatistics setTimeOffset getServiceName shutdown getSignerByURI toString getSignerRegionOverride verifyDomainDkim getTimeOffset verifyDomainIdentity isInstanceOf verifyEmailAddress listIdentities verifyEmailIdentity listVerifiedEmailAddresses withTimeOffset removeRequestHandler Once you have a feel for how the Scala REPL works with a library, you can create alternate entry point for sbt called scalas which allows you to write full Scala scripts with library dependencies. See Scripts, REPL, and Dependencies for additional details. In brief, create a scalas script and make it available on your PATH : #!/bin/sh test -f ~/.sbtconfig && . ~/.sbtconfig exec java \\ -Xms512M \\ -Xmx1536M \\ -Xss1M \\ -XX:+CMSClassUnloadingEnabled \\ -XX:MaxPermSize = 256M \\ ${ SBT_OPTS } \\ -jar /usr/local/Cellar/sbt/0.13.2/libexec/sbt-launch.jar \\ -Dsbt.main.class = sbt.ScriptMain \\ \" $@ \" You can then write Scala scripts like so: #!/usr/bin/env scalas /*** scalaVersion := \"2.11.0\" libraryDependencies ++= Seq( \"commons-logging\" % \"commons-logging\" % \"1.1.3\", \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\" ) */ import com . amazonaws . auth . _ import com . amazonaws . auth . profile . _ import com . amazonaws . services . simpleemail . _ import com . amazonaws . services . simpleemail . model . _ val c = new AmazonSimpleEmailServiceClient ( new AWSCredentialsProviderChain ( // attempt on-instance credentials first new InstanceProfileCredentialsProvider (), // fallback to aws credentials file new ProfileCredentialsProvider () ) ) c . setEndpoint ( \"email.us-east-1.amazonaws.com\" ) println ( c . getSendQuota ()) println ( c . getSendStatistics (). getSendDataPoints . get ( 0 )) println ( c . getSendStatistics (). getSendDataPoints . size ) Execute the script as follows: $ ./demo.scala . . . {Timestamp: Sat Jul 05 04:01:00 PDT 2014,DeliveryAttempts: 3,Bounces: 0,Complaints: 0,Rejects: 0} 1301","title":"Using the Scala REPL to Configure Amazon SES Notifications"},{"location":"shell/measuring-transfer-speed-over-time-with-curl/","text":"Measuring Transfer Speed Over Time with cURL \u00b6 2014-07-02 Discuss Ordinarily when you run the cURL command to download a file, you see a progress meter that updates every second. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 346 100 346 0 0 422 0 --:--:-- --:--:-- --:--:-- 422 4 635M 4 29.8M 0 0 1793k 0 0:06:02 0:00:17 0:05:45 2394k This progress meter is written to stderr and if you were to redirect both stderr and stdout to a file and then run tail -f on that file, you would see the exact same progress meter being updated once per second, with no running log of download speed. The reason that this output updates in place is because the program is writing a carriage return \\r at the end of the progress line instead of a newline \\n . This causes the cursor to return to the beginning of the line without advancing. With the knowledge of how this operates, it is possible to alter the output of the cURL command to save the per-second speed of a download. If you further send the results of a large file download to /dev/null , then you have a reasonable approximation of of a speedtest tool and you can graph the download speed over time. The command below uses tr to rewrite carriage returns as newlines in an unbuffered manner, so that data is instantly available in the output file. As an aside on the power of the tr command , the More Shell, Less Egg blog post by Dr. Drang discusses a programming challenge proposed to Donald Knuth , who solved it with ~10 pages of literate Pascal, and Doug McIlroy who critiqued the solution and provided an alternative solution in six shell commands. URL = \"http://cdimage.debian.org/debian-cd/7.5.0/amd64/iso-cd/debian-7.5.0-amd64-CD-1.iso\" curl -L -o /dev/null \" $URL \" 2 > & 1 \\ | tr -u '\\r' '\\n' > curl.out This results in an output file that looks like this: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 346 100 346 0 0 295 0 0:00:01 0:00:01 --:--:-- 295 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:02 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:04 --:--:-- 0 0 635M 0 70871 0 0 12988 0 14:14:26 0:00:05 14:14:21 17260 0 635M 0 608k 0 0 97534 0 1:53:46 0:00:06 1:53:40 120k 0 635M 0 1489k 0 0 201k 0 0:53:41 0:00:07 0:53:34 296k 0 635M 0 2742k 0 0 328k 0 0:33:00 0:00:08 0:32:52 548k 0 635M 0 4297k 0 0 456k 0 0:23:43 0:00:09 0:23:34 849k 0 635M 0 6015k 0 0 580k 0 0:18:40 0:00:10 0:18:30 1210k 1 635M 1 8014k 0 0 701k 0 0:15:27 0:00:11 0:15:16 1471k 1 635M 1 10.0M 0 0 827k 0 0:13:05 0:00:12 0:12:53 1749k 1 635M 1 11.0M 0 0 841k 0 0:12:52 0:00:13 0:12:39 1682k . . . Write a Python script plot_curl_data.py to process the data to convert it into a format useful for gnuplot and render a plot: #!/usr/bin/env python import os, sys def readCurlData(fname): lines = [] with open(fname) as f: for line in f: lines.append(line.split()) return lines[3:] def convertUnits(lines): converted = [] for line in lines: if len(line) == 12 and not \"--\" in line[9]: # curl reports speed in bytes per second if 'k' in line[11]: line[11] = str(float(line[11].replace('k','')) * 8 * 1024) elif 'M' in line[11]: line[11] = str(float(line[11].replace('M','')) * 8 * 1048576) elif 'G' in line[11]: line[11] = str(float(line[11].replace('G','')) * 8 * 1073741824) converted.append([line[9], line[11]]) return converted def writeGnuplotData(fname, lines): fname = fname + \".gnuplot.data\" with open(fname, 'w') as f: for line in lines: f.write(','.join(line) + '\\n') def plot(fname): gp_fname = fname + \".gp\" gpdata_fname = fname + \".gnuplot.data\" png_fname = fname + \".png\" f = open(gp_fname, \"w\") f.write('set output \"%s\"\\n' % png_fname) f.write('set datafile separator \",\"\\n') f.write('set terminal png size 1400,800\\n') f.write('set title \"Download Speed\"\\n') f.write('set ylabel \"Speed (Mbits/s)\"\\n') f.write('set xlabel \"Time (seconds)\"\\n') f.write('set xdata time\\n') f.write('set timefmt \"%H:%M:%S\"\\n') f.write('set key outside\\n') f.write('set grid\\n') f.write('plot \\\\\\n') f.write('\"%s\" using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \"speed\"\\n' % gpdata_fname) f.close() os.system(\"gnuplot %s\" % gp_fname) if len(sys.argv) < 2: print \"Usage: %s [curl_data_filename]\" % sys.argv[0] exit(1) else: lines = readCurlData(sys.argv[1]) lines = convertUnits(lines) writeGnuplotData(sys.argv[1], lines) plot(sys.argv[1]) Run this script like so: ./plot_curl_data.py curl.out You will end up with data ( curl.out.gp.data ) and configuration files ( curl.out.gp ) like so: 0:00:01,295 0:00:01,0 0:00:02,0 0:00:03,0 0:00:04,0 0:00:05,17260 0:00:06,983040 0:00:07,2424832 0:00:08,4489216 0:00:09,6955008 0:00:10,9912320 0:00:11,12050432 0:00:12,14327808 0:00:13,13778944 0:00:14,12173312 . . . set output \"curl.out.png\" set datafile separator \",\" set terminal png size 1400,800 set title \"Download Speed\" set ylabel \"Speed (Mbits/s)\" set xlabel \"Time (seconds)\" set xdata time set timefmt \"%H:%M:%S\" set key outside set grid plot \\ \"curl.out.gp.data\" using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \"speed\" The graph will be rendered in PNG format:","title":"Measuring Transfer Speed Over Time with cURL"},{"location":"shell/measuring-transfer-speed-over-time-with-curl/#measuring-transfer-speed-over-time-with-curl","text":"2014-07-02 Discuss Ordinarily when you run the cURL command to download a file, you see a progress meter that updates every second. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 346 100 346 0 0 422 0 --:--:-- --:--:-- --:--:-- 422 4 635M 4 29.8M 0 0 1793k 0 0:06:02 0:00:17 0:05:45 2394k This progress meter is written to stderr and if you were to redirect both stderr and stdout to a file and then run tail -f on that file, you would see the exact same progress meter being updated once per second, with no running log of download speed. The reason that this output updates in place is because the program is writing a carriage return \\r at the end of the progress line instead of a newline \\n . This causes the cursor to return to the beginning of the line without advancing. With the knowledge of how this operates, it is possible to alter the output of the cURL command to save the per-second speed of a download. If you further send the results of a large file download to /dev/null , then you have a reasonable approximation of of a speedtest tool and you can graph the download speed over time. The command below uses tr to rewrite carriage returns as newlines in an unbuffered manner, so that data is instantly available in the output file. As an aside on the power of the tr command , the More Shell, Less Egg blog post by Dr. Drang discusses a programming challenge proposed to Donald Knuth , who solved it with ~10 pages of literate Pascal, and Doug McIlroy who critiqued the solution and provided an alternative solution in six shell commands. URL = \"http://cdimage.debian.org/debian-cd/7.5.0/amd64/iso-cd/debian-7.5.0-amd64-CD-1.iso\" curl -L -o /dev/null \" $URL \" 2 > & 1 \\ | tr -u '\\r' '\\n' > curl.out This results in an output file that looks like this: % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 100 346 100 346 0 0 295 0 0:00:01 0:00:01 --:--:-- 295 0 0 0 0 0 0 0 0 --:--:-- 0:00:01 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:02 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:03 --:--:-- 0 0 0 0 0 0 0 0 0 --:--:-- 0:00:04 --:--:-- 0 0 635M 0 70871 0 0 12988 0 14:14:26 0:00:05 14:14:21 17260 0 635M 0 608k 0 0 97534 0 1:53:46 0:00:06 1:53:40 120k 0 635M 0 1489k 0 0 201k 0 0:53:41 0:00:07 0:53:34 296k 0 635M 0 2742k 0 0 328k 0 0:33:00 0:00:08 0:32:52 548k 0 635M 0 4297k 0 0 456k 0 0:23:43 0:00:09 0:23:34 849k 0 635M 0 6015k 0 0 580k 0 0:18:40 0:00:10 0:18:30 1210k 1 635M 1 8014k 0 0 701k 0 0:15:27 0:00:11 0:15:16 1471k 1 635M 1 10.0M 0 0 827k 0 0:13:05 0:00:12 0:12:53 1749k 1 635M 1 11.0M 0 0 841k 0 0:12:52 0:00:13 0:12:39 1682k . . . Write a Python script plot_curl_data.py to process the data to convert it into a format useful for gnuplot and render a plot: #!/usr/bin/env python import os, sys def readCurlData(fname): lines = [] with open(fname) as f: for line in f: lines.append(line.split()) return lines[3:] def convertUnits(lines): converted = [] for line in lines: if len(line) == 12 and not \"--\" in line[9]: # curl reports speed in bytes per second if 'k' in line[11]: line[11] = str(float(line[11].replace('k','')) * 8 * 1024) elif 'M' in line[11]: line[11] = str(float(line[11].replace('M','')) * 8 * 1048576) elif 'G' in line[11]: line[11] = str(float(line[11].replace('G','')) * 8 * 1073741824) converted.append([line[9], line[11]]) return converted def writeGnuplotData(fname, lines): fname = fname + \".gnuplot.data\" with open(fname, 'w') as f: for line in lines: f.write(','.join(line) + '\\n') def plot(fname): gp_fname = fname + \".gp\" gpdata_fname = fname + \".gnuplot.data\" png_fname = fname + \".png\" f = open(gp_fname, \"w\") f.write('set output \"%s\"\\n' % png_fname) f.write('set datafile separator \",\"\\n') f.write('set terminal png size 1400,800\\n') f.write('set title \"Download Speed\"\\n') f.write('set ylabel \"Speed (Mbits/s)\"\\n') f.write('set xlabel \"Time (seconds)\"\\n') f.write('set xdata time\\n') f.write('set timefmt \"%H:%M:%S\"\\n') f.write('set key outside\\n') f.write('set grid\\n') f.write('plot \\\\\\n') f.write('\"%s\" using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \"speed\"\\n' % gpdata_fname) f.close() os.system(\"gnuplot %s\" % gp_fname) if len(sys.argv) < 2: print \"Usage: %s [curl_data_filename]\" % sys.argv[0] exit(1) else: lines = readCurlData(sys.argv[1]) lines = convertUnits(lines) writeGnuplotData(sys.argv[1], lines) plot(sys.argv[1]) Run this script like so: ./plot_curl_data.py curl.out You will end up with data ( curl.out.gp.data ) and configuration files ( curl.out.gp ) like so: 0:00:01,295 0:00:01,0 0:00:02,0 0:00:03,0 0:00:04,0 0:00:05,17260 0:00:06,983040 0:00:07,2424832 0:00:08,4489216 0:00:09,6955008 0:00:10,9912320 0:00:11,12050432 0:00:12,14327808 0:00:13,13778944 0:00:14,12173312 . . . set output \"curl.out.png\" set datafile separator \",\" set terminal png size 1400,800 set title \"Download Speed\" set ylabel \"Speed (Mbits/s)\" set xlabel \"Time (seconds)\" set xdata time set timefmt \"%H:%M:%S\" set key outside set grid plot \\ \"curl.out.gp.data\" using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \"speed\" The graph will be rendered in PNG format:","title":"Measuring Transfer Speed Over Time with cURL"},{"location":"shell/parsing-json-on-the-command-line/","text":"Parsing JSON on the Command Line \u00b6 2014-07-16 Discuss With more APIs moving to JSON, being able to parse it at the command line allows you to write more sophisticated shell scripts that can interact with your favorite services. Most first attempts at JSON parsing using some variation of the following to get the job done, which initially seems reasonable: curl -s http://endpoint.info \\ | python -mjson.tool \\ | grep foo \\ | cut -d: -f2 \\ | sed -e 's/\"//g' However, this can rapidly get out of hand, if you have key duplication, complex nested structures or you need to pull in all of the elements of a list. For awhile, underscore-cli was my favorite fully-featured JSON parser, but I found its documentation somewhat lacking and it hasn't seen serious development since November 2012. Since then, I found jq which has a beautiful, well-written manual with many usage examples and it is under active development. It also has the benefit of being written in C, which helps speed and it has a fairly concise descriptor language. To install: brew install jq Then you can do things like flattening a complex JSON structure into a simple CSV: PAYLOADS = $( curl -s $URL | jq '.payloads' ) if [ \" $PAYLOADS \" ! = \"[]\" ] ; then echo $PAYLOADS \\ | jq -r '.[] | .minutes[].payload.items[] | [.actions.actionTime, (.actions | {actions} | .actions.email.state), .sourceInstance, (.actions | {actions} | .actions.email.info )] | @csv' fi As a bonus feature, if you have to deal in XML rather than JSON, then xmlstarlet is a good choice for handling it. Naturally, installation: brew install xmlstarlet Once you have xmlstarlet, you can do things like pull the build number out of an Atlassian Bamboo HTML page, which was necessary when they did not have an API call available to report the version number of the last known good build for a project: curl -s --insecure https://bamboo.local/browse/ ${ BUILDKEY }${ BUILD } \\ | tidy -asxhtml -numeric --force-output true 2 >/dev/null \\ | xmlstarlet sel -N x = \"http://www.w3.org/1999/xhtml\" -t -m \"//x:div[@id='sr-build']/x:h2/x:a\" -v \".\" \\ | sed -e \"s/#//g\"","title":"Parsing JSON on the Command Line"},{"location":"shell/parsing-json-on-the-command-line/#parsing-json-on-the-command-line","text":"2014-07-16 Discuss With more APIs moving to JSON, being able to parse it at the command line allows you to write more sophisticated shell scripts that can interact with your favorite services. Most first attempts at JSON parsing using some variation of the following to get the job done, which initially seems reasonable: curl -s http://endpoint.info \\ | python -mjson.tool \\ | grep foo \\ | cut -d: -f2 \\ | sed -e 's/\"//g' However, this can rapidly get out of hand, if you have key duplication, complex nested structures or you need to pull in all of the elements of a list. For awhile, underscore-cli was my favorite fully-featured JSON parser, but I found its documentation somewhat lacking and it hasn't seen serious development since November 2012. Since then, I found jq which has a beautiful, well-written manual with many usage examples and it is under active development. It also has the benefit of being written in C, which helps speed and it has a fairly concise descriptor language. To install: brew install jq Then you can do things like flattening a complex JSON structure into a simple CSV: PAYLOADS = $( curl -s $URL | jq '.payloads' ) if [ \" $PAYLOADS \" ! = \"[]\" ] ; then echo $PAYLOADS \\ | jq -r '.[] | .minutes[].payload.items[] | [.actions.actionTime, (.actions | {actions} | .actions.email.state), .sourceInstance, (.actions | {actions} | .actions.email.info )] | @csv' fi As a bonus feature, if you have to deal in XML rather than JSON, then xmlstarlet is a good choice for handling it. Naturally, installation: brew install xmlstarlet Once you have xmlstarlet, you can do things like pull the build number out of an Atlassian Bamboo HTML page, which was necessary when they did not have an API call available to report the version number of the last known good build for a project: curl -s --insecure https://bamboo.local/browse/ ${ BUILDKEY }${ BUILD } \\ | tidy -asxhtml -numeric --force-output true 2 >/dev/null \\ | xmlstarlet sel -N x = \"http://www.w3.org/1999/xhtml\" -t -m \"//x:div[@id='sr-build']/x:h2/x:a\" -v \".\" \\ | sed -e \"s/#//g\"","title":"Parsing JSON on the Command Line"},{"location":"shell/pet-cli-snippet-manager/","text":"pet - CLI Snippet Manager \u00b6 2017-10-13 Discuss https://github.com/knqyf263/pet This shell utility that makes it easy to save, search and recall shell commands. It uses GitHub Gists as the backend storage, so your work is backed up and accessible from multiple locations. Of course, since you are storing data publicly, make sure that you are sanitizing the data, so no secrets are shared. Install and Configure \u00b6 Install with Homebrew: brew install pet Create a new access token on GitHub that allows only the gist scope. Create a pet configuration file and set the access_token . pet configure Usage \u00b6 Create new snippets: pet new List existing snippets: pet list Search for snippets: pet search Warning The pet command does not have version validation associated with snippets uploaded to Gist, so it is possible to wipe out your snippets in two separate ways: Downloading empty snippets from your Gist, which overwrite your local snippets. Uploading empty local snippets to your Gist. Since GitHub stores revisions, you can recover from this. Make sure that you have the latest snippets locally before updating your Gist. Upload snippets to Gist: pet sync -u Download snippets from Gist: pet sync","title":"pet - CLI Snippet Manager"},{"location":"shell/pet-cli-snippet-manager/#pet-cli-snippet-manager","text":"2017-10-13 Discuss https://github.com/knqyf263/pet This shell utility that makes it easy to save, search and recall shell commands. It uses GitHub Gists as the backend storage, so your work is backed up and accessible from multiple locations. Of course, since you are storing data publicly, make sure that you are sanitizing the data, so no secrets are shared.","title":"pet - CLI Snippet Manager"},{"location":"shell/pet-cli-snippet-manager/#install-and-configure","text":"Install with Homebrew: brew install pet Create a new access token on GitHub that allows only the gist scope. Create a pet configuration file and set the access_token . pet configure","title":"Install and Configure"},{"location":"shell/pet-cli-snippet-manager/#usage","text":"Create new snippets: pet new List existing snippets: pet list Search for snippets: pet search Warning The pet command does not have version validation associated with snippets uploaded to Gist, so it is possible to wipe out your snippets in two separate ways: Downloading empty snippets from your Gist, which overwrite your local snippets. Uploading empty local snippets to your Gist. Since GitHub stores revisions, you can recover from this. Make sure that you have the latest snippets locally before updating your Gist. Upload snippets to Gist: pet sync -u Download snippets from Gist: pet sync","title":"Usage"},{"location":"shell/schedule-ad-hoc-jobs-for-later-on-an-instance-with-atd/","text":"Schedule Ad-Hoc Jobs for Later on an Instance with atd \u00b6 2017-08-15 Discuss https://www.computerhope.com/unix/uat.htm The purpose of this technique is to allow you to disconnect from the instance, while leaving the job running (i.e. not dependent on a shell fork). (root) # at now + 1 minute warning: commands will be executed using /bin/sh at> /apps/something/bin/upload_to_s3.sh at> ^d job 2 at Wed Aug 9 20:37:00 2017 (root) # atq 2 Wed Aug 9 20:37:00 2017 a root If you need to remove and reschedule a job, you can do the following: (root) # atrm 2 The following are examples of casual times that can be used with at: expression time noon 12:00 PM October 18 2014 midnight 12:00 AM October 19 2014 teatime 4:00 PM October 18 2014 tomorrow 10:00 AM October 19 2014 noon tomorrow 12:00 PM October 19 2014 next week 10:00 AM October 25 2014 next monday 10:00 AM October 24 2014 fri 10:00 AM October 21 2014 NOV 10:00 AM November 18 2014 9:00 AM 9:00 AM October 19 2014 2:30 PM 2:30 PM October 18 2014 1430 2:30 PM October 18 2014 2:30 PM tomorrow 2:30 PM October 19 2014 2:30 PM next month 2:30 PM November 18 2014 2:30 PM Fri 2:30 PM October 21 2014 2:30 PM 10/21 2:30 PM October 21 2014 2:30 PM Oct 21 2:30 PM October 21 2014 2:30 PM 10/21/2014 2:30 PM October 21 2014 2:30 PM 21.10.14 2:30 PM October 21 2014 now + 30 minutes 10:30 AM October 18 2014 now + 1 hour 11:00 AM October 18 2014 now + 2 days 10:00 AM October 20 2014 4 PM + 2 days 4:00 PM October 20 2014 now + 3 weeks 10:00 AM November 8 2014 now + 4 months 10:00 AM February 18 2015 now + 5 years 10:00 AM October 18 2019","title":"Schedule Ad-Hoc Jobs for Later on an Instance with atd"},{"location":"shell/schedule-ad-hoc-jobs-for-later-on-an-instance-with-atd/#schedule-ad-hoc-jobs-for-later-on-an-instance-with-atd","text":"2017-08-15 Discuss https://www.computerhope.com/unix/uat.htm The purpose of this technique is to allow you to disconnect from the instance, while leaving the job running (i.e. not dependent on a shell fork). (root) # at now + 1 minute warning: commands will be executed using /bin/sh at> /apps/something/bin/upload_to_s3.sh at> ^d job 2 at Wed Aug 9 20:37:00 2017 (root) # atq 2 Wed Aug 9 20:37:00 2017 a root If you need to remove and reschedule a job, you can do the following: (root) # atrm 2 The following are examples of casual times that can be used with at: expression time noon 12:00 PM October 18 2014 midnight 12:00 AM October 19 2014 teatime 4:00 PM October 18 2014 tomorrow 10:00 AM October 19 2014 noon tomorrow 12:00 PM October 19 2014 next week 10:00 AM October 25 2014 next monday 10:00 AM October 24 2014 fri 10:00 AM October 21 2014 NOV 10:00 AM November 18 2014 9:00 AM 9:00 AM October 19 2014 2:30 PM 2:30 PM October 18 2014 1430 2:30 PM October 18 2014 2:30 PM tomorrow 2:30 PM October 19 2014 2:30 PM next month 2:30 PM November 18 2014 2:30 PM Fri 2:30 PM October 21 2014 2:30 PM 10/21 2:30 PM October 21 2014 2:30 PM Oct 21 2:30 PM October 21 2014 2:30 PM 10/21/2014 2:30 PM October 21 2014 2:30 PM 21.10.14 2:30 PM October 21 2014 now + 30 minutes 10:30 AM October 18 2014 now + 1 hour 11:00 AM October 18 2014 now + 2 days 10:00 AM October 20 2014 4 PM + 2 days 4:00 PM October 20 2014 now + 3 weeks 10:00 AM November 8 2014 now + 4 months 10:00 AM February 18 2015 now + 5 years 10:00 AM October 18 2019","title":"Schedule Ad-Hoc Jobs for Later on an Instance with atd"},{"location":"shell/zsh-keyboard-shortcuts/","text":"Introduction \u00b6 Default Z shell keyboard shortcuts, tailored for MacOS and Kitty . I have been on a minimal configuration kick lately and don't want to customize things heavily. Use bindkey to see current configuration settings. More details available in the zshzle man page. Kitty Windows & Tabs \u00b6 Description Shortcut Switch between enabled window layouts ctrl-shift-l New window cmd-enter Close window cmd-shift-d Next window ctrl-shift-] Previous window ctrl-shift-[ Move window forward ctrl-shift-f Move window backward ctrl-shift-b New tab cmd-t Close tab cmd-w Next tab cmd-shift-] Previous tab cmd-shift-[ Move tab forward ctrl-shift-. Move tab backward ctrl-shift-, Moving within a Line \u00b6 Description Mapping Shortcut Move one character backwards backward-char leftArrow ctrl-b Move one character forwards forward-char rightArrow ctrl-f Move one word backwards backward-word ctrl-[b Move one word forwards forward-word ctrl-[f Move to the beginning of the line beginning-of-line ctrl-a Move to the end of the line end-of-line ctrl-e Editing a Line \u00b6 Description Mapping Shortcut Delete the character before the cursor backward-delete-char delete ctrl-h Delete the character under the cursor delete-char-or-list ctrl-d Delete the word before the cursor backward-kill-word ctrl-w ctrl-[,ctrl-h Delete the word after the cursor kill-word ctrl-[d Delete the line after the cursor kill-line ctrl-k Delete the whole line kill-whole-line ctrl-u Transpose the two characters before the cursor transpose-chars ctrl-t Transpose the two words before the cursor transpose-words ctrl-[t Make a word lowercase down-case-word ctrl-[l Make a word uppercase up-case-word ctrl-[u Quote line quote-line ctrl-[' Push line onto buffer push-line ctrl-[q Get line from buffer get-line ctrl-[g Delete buffer kill-buffer ctrl-x,ctrl-k Undo the last change undo ctrl-xu ctrl-x,ctrl-u Execute line accept-line enter ctrl-j ctrl-m Screen Management \u00b6 Description Mapping Shortcut Clear screen, leaving current line intact clear-screen ctrl-l Halt output to screen ? ctrl-s Resume output to screen ? ctrl-q Process Management \u00b6 Description Shortcut Terminate/kill current foreground process ctrl-c Suspend/stop current foreground process ctrl-z Execute last command in history !! Execute last command in history that starts with abc !abc Print last command in history beginning with abc !abc:p History \u00b6 Description Mapping Shortcut Previous history line up-line-or-history upArrow ctrl-p Next history line down-line-or-history downArrow ctrl-n Search history backwards history-incremental-search-backward ctrl-r Search history forwards history-incremental-search-forward ctrl-s Exit history search send-break ctrl-g","title":"Z Shell Keyboard Shortcuts"},{"location":"shell/zsh-keyboard-shortcuts/#introduction","text":"Default Z shell keyboard shortcuts, tailored for MacOS and Kitty . I have been on a minimal configuration kick lately and don't want to customize things heavily. Use bindkey to see current configuration settings. More details available in the zshzle man page.","title":"Introduction"},{"location":"shell/zsh-keyboard-shortcuts/#kitty-windows-tabs","text":"Description Shortcut Switch between enabled window layouts ctrl-shift-l New window cmd-enter Close window cmd-shift-d Next window ctrl-shift-] Previous window ctrl-shift-[ Move window forward ctrl-shift-f Move window backward ctrl-shift-b New tab cmd-t Close tab cmd-w Next tab cmd-shift-] Previous tab cmd-shift-[ Move tab forward ctrl-shift-. Move tab backward ctrl-shift-,","title":"Kitty Windows &amp; Tabs"},{"location":"shell/zsh-keyboard-shortcuts/#moving-within-a-line","text":"Description Mapping Shortcut Move one character backwards backward-char leftArrow ctrl-b Move one character forwards forward-char rightArrow ctrl-f Move one word backwards backward-word ctrl-[b Move one word forwards forward-word ctrl-[f Move to the beginning of the line beginning-of-line ctrl-a Move to the end of the line end-of-line ctrl-e","title":"Moving within a Line"},{"location":"shell/zsh-keyboard-shortcuts/#editing-a-line","text":"Description Mapping Shortcut Delete the character before the cursor backward-delete-char delete ctrl-h Delete the character under the cursor delete-char-or-list ctrl-d Delete the word before the cursor backward-kill-word ctrl-w ctrl-[,ctrl-h Delete the word after the cursor kill-word ctrl-[d Delete the line after the cursor kill-line ctrl-k Delete the whole line kill-whole-line ctrl-u Transpose the two characters before the cursor transpose-chars ctrl-t Transpose the two words before the cursor transpose-words ctrl-[t Make a word lowercase down-case-word ctrl-[l Make a word uppercase up-case-word ctrl-[u Quote line quote-line ctrl-[' Push line onto buffer push-line ctrl-[q Get line from buffer get-line ctrl-[g Delete buffer kill-buffer ctrl-x,ctrl-k Undo the last change undo ctrl-xu ctrl-x,ctrl-u Execute line accept-line enter ctrl-j ctrl-m","title":"Editing a Line"},{"location":"shell/zsh-keyboard-shortcuts/#screen-management","text":"Description Mapping Shortcut Clear screen, leaving current line intact clear-screen ctrl-l Halt output to screen ? ctrl-s Resume output to screen ? ctrl-q","title":"Screen Management"},{"location":"shell/zsh-keyboard-shortcuts/#process-management","text":"Description Shortcut Terminate/kill current foreground process ctrl-c Suspend/stop current foreground process ctrl-z Execute last command in history !! Execute last command in history that starts with abc !abc Print last command in history beginning with abc !abc:p","title":"Process Management"},{"location":"shell/zsh-keyboard-shortcuts/#history","text":"Description Mapping Shortcut Previous history line up-line-or-history upArrow ctrl-p Next history line down-line-or-history downArrow ctrl-n Search history backwards history-incremental-search-backward ctrl-r Search history forwards history-incremental-search-forward ctrl-s Exit history search send-break ctrl-g","title":"History"},{"location":"stocks/nflx-quarterly-subscriber-growth/","text":"Netflix Quarterly Subscriber Growth \u00b6 All of the information presented here is taken from quarterly earnings reports publicly posted on the Netflix Investors website. The subscriber numbers can be found on the Segment Information tab of the Quarterly Earnings Financial Statements spreadsheets. The Domestic and International streaming paid memberships were first broken out separately in 2011-Q3. The spreadsheet with the source data that generated these charts is available for review, so you can see that it is a straight copy from the source.","title":"Netflix Quarterly Growth"},{"location":"stocks/nflx-quarterly-subscriber-growth/#netflix-quarterly-subscriber-growth","text":"All of the information presented here is taken from quarterly earnings reports publicly posted on the Netflix Investors website. The subscriber numbers can be found on the Segment Information tab of the Quarterly Earnings Financial Statements spreadsheets. The Domestic and International streaming paid memberships were first broken out separately in 2011-Q3. The spreadsheet with the source data that generated these charts is available for review, so you can see that it is a straight copy from the source.","title":"Netflix Quarterly Subscriber Growth"},{"location":"talks/favorite/","text":"Favorite Talks \u00b6 C \u00b6 Handmade Hero Day 001: Setting Up the Windows Build - Casey Muratori | 2014 Career \u00b6 Rethinking the Developer Career Path \u2013 Randall Koutnik | The Lead Developer UK 2017 Clojure \u00b6 Are We There Yet? - Rich Hickey | JVM Language Summit 2009 Boot Can Built It - Alan Dipert, Micha Niskin | Clojure/west 2015 Bottom Up vs Top Down Design in Clojure - Mark Bastian | Clojure/conj 2015 Clojure Concurrency - Rich Hickey | Western Mass. Developers Group Clojure core.async - Rich Hickey | Strange Loop 2013 Clojure Made Simple - Rich Hickey | Oracle Developers 2015 Clojure Parallelism Beyond Futures - Leon Barrett | Clojure/west 2015 clojure.spec - Rich Hickey | LispNYC 2017 Composing Interactive Apps With Zelkova - James MacAulay | Clojure/west 2015 Dagobah, a Data centric Meta scheduler - Matt Bossenbroek | Clojure/conj 2015 Debugging Clojure Code with Cursive - Colin Fleming | Clojure/west 2015 Design and Prototype a Language In Clojure - Jeanine Adkisson | Clojure/west 2015 Hammock Driven Development - Rich Hickey | Clojure/conj 2010 Parallel Programming, Fork Join, and Reducers - Daniel Higginbotham | Clojure/west 2016 Parsing Text with a Virtual Machine - Ghadi Shayban | Clojure/west 2016 REPL-Based Development Demo - Valentin Waeselynck | 2017 (from What makes a good REPL? ) Simple Made Easy - Rich Hickey | Strange Loop 2011 Spec-ulation - Rich Hickey | Clojure/conj 2016 The Joys and Perils of Interactive Development - Stuart Sierra | Clojure/west 2016 The ReactJS Landscape - Luke VanderHart | Clojure/west 2015 Types are Like the Weather, Type Systems are Like Weathermen - Matthias Felleisen | Clojure/west 2016 ClojureScript \u00b6 ClojureScript for Skeptics - Derek Slager | Clojure/conj 2015 Developing ClojureScript With Figwheel - Bruce Hauman | Clojure/west 2015 From 0 to Prototype Using ClojureScript, Re-Frame and Friends - Martin Clausen | Dutch Clojure Days 2017 Interactive Programming Flappy Bird in ClojureScript - Bruce Hauman | 2014 Netflix Atlas \u00b6 Netflix: Amazon S3 & Amazon Elastic MapReduce to Monitor at Gigascale - Roy Rapoport | AWS re:Invent 2013 Monitoring Monitoring Systems at Netflix - Roy Rapoport | Monitorama PDX 2017 Netflix Atlas Telemetry - A Platform Begets an Ecosystem Python \u00b6 Automating Your Browser and Desktop Apps - Al Sweigart | PyBay2016 Beyond PEP 8: Best Practices for Beautiful Intelligible Code - Raymond Hettinger | PyCon 2015 Built in Super Heroes - David Beazley | PyData Chicago 2016 Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream - David Beazley | PyOhio 2016 Logging and Testing and Debugging, Oh My! - Albert Sweigart | PyBay2017 Machete-Mode Debugging: Hacking Your Way Out of a Tight Spot - Ned Batchelder | PyCon 2016 Python as a Configuration Language - Guillermo P\u00e9rez | PyCon 2016 Python Concurrency From the Ground Up: LIVE! - David Beazley | PyCon 2015 Python Language - Guido van Rossum | PyCon 2016 Python vs Ruby: A Battle to The Death - Gary Bernhardt | NWPD 2010 Refactoring Python: Why and How to Restructure Your Code - Brett Slatkin | PyCon 2016 Removing Python's GIL: The Gilectomy - Larry Hastings | PyCon 2016 Stop Writing Classes - Jack Diederich | PyCon 2012 The Packaging Gradient - Mahmoud Hashemi | PyBay 2017 Think Like a Pythonista - Luciano Ramalho | PyBay 2017 Thinking In Coroutines - \u0141ukasz Langa | PyCon 2016 To Mock, or Not to Mock, That is the Question - Ana Balica | PyCon 2016 Writing Awesome Command-Line Programs in Python - Mark Smith | EuroPython 2014 Scala \u00b6 A Better Scala REPL - Li Haoyi | SBTB 2015 Unix \u00b6 The Unix Chainsaw - Gary Bernhardt | Cascadia Ruby Conf 2011","title":"Favorite"},{"location":"talks/favorite/#favorite-talks","text":"","title":"Favorite Talks"},{"location":"talks/favorite/#c","text":"Handmade Hero Day 001: Setting Up the Windows Build - Casey Muratori | 2014","title":"C"},{"location":"talks/favorite/#career","text":"Rethinking the Developer Career Path \u2013 Randall Koutnik | The Lead Developer UK 2017","title":"Career"},{"location":"talks/favorite/#clojure","text":"Are We There Yet? - Rich Hickey | JVM Language Summit 2009 Boot Can Built It - Alan Dipert, Micha Niskin | Clojure/west 2015 Bottom Up vs Top Down Design in Clojure - Mark Bastian | Clojure/conj 2015 Clojure Concurrency - Rich Hickey | Western Mass. Developers Group Clojure core.async - Rich Hickey | Strange Loop 2013 Clojure Made Simple - Rich Hickey | Oracle Developers 2015 Clojure Parallelism Beyond Futures - Leon Barrett | Clojure/west 2015 clojure.spec - Rich Hickey | LispNYC 2017 Composing Interactive Apps With Zelkova - James MacAulay | Clojure/west 2015 Dagobah, a Data centric Meta scheduler - Matt Bossenbroek | Clojure/conj 2015 Debugging Clojure Code with Cursive - Colin Fleming | Clojure/west 2015 Design and Prototype a Language In Clojure - Jeanine Adkisson | Clojure/west 2015 Hammock Driven Development - Rich Hickey | Clojure/conj 2010 Parallel Programming, Fork Join, and Reducers - Daniel Higginbotham | Clojure/west 2016 Parsing Text with a Virtual Machine - Ghadi Shayban | Clojure/west 2016 REPL-Based Development Demo - Valentin Waeselynck | 2017 (from What makes a good REPL? ) Simple Made Easy - Rich Hickey | Strange Loop 2011 Spec-ulation - Rich Hickey | Clojure/conj 2016 The Joys and Perils of Interactive Development - Stuart Sierra | Clojure/west 2016 The ReactJS Landscape - Luke VanderHart | Clojure/west 2015 Types are Like the Weather, Type Systems are Like Weathermen - Matthias Felleisen | Clojure/west 2016","title":"Clojure"},{"location":"talks/favorite/#clojurescript","text":"ClojureScript for Skeptics - Derek Slager | Clojure/conj 2015 Developing ClojureScript With Figwheel - Bruce Hauman | Clojure/west 2015 From 0 to Prototype Using ClojureScript, Re-Frame and Friends - Martin Clausen | Dutch Clojure Days 2017 Interactive Programming Flappy Bird in ClojureScript - Bruce Hauman | 2014","title":"ClojureScript"},{"location":"talks/favorite/#netflix-atlas","text":"Netflix: Amazon S3 & Amazon Elastic MapReduce to Monitor at Gigascale - Roy Rapoport | AWS re:Invent 2013 Monitoring Monitoring Systems at Netflix - Roy Rapoport | Monitorama PDX 2017 Netflix Atlas Telemetry - A Platform Begets an Ecosystem","title":"Netflix Atlas"},{"location":"talks/favorite/#python","text":"Automating Your Browser and Desktop Apps - Al Sweigart | PyBay2016 Beyond PEP 8: Best Practices for Beautiful Intelligible Code - Raymond Hettinger | PyCon 2015 Built in Super Heroes - David Beazley | PyData Chicago 2016 Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream - David Beazley | PyOhio 2016 Logging and Testing and Debugging, Oh My! - Albert Sweigart | PyBay2017 Machete-Mode Debugging: Hacking Your Way Out of a Tight Spot - Ned Batchelder | PyCon 2016 Python as a Configuration Language - Guillermo P\u00e9rez | PyCon 2016 Python Concurrency From the Ground Up: LIVE! - David Beazley | PyCon 2015 Python Language - Guido van Rossum | PyCon 2016 Python vs Ruby: A Battle to The Death - Gary Bernhardt | NWPD 2010 Refactoring Python: Why and How to Restructure Your Code - Brett Slatkin | PyCon 2016 Removing Python's GIL: The Gilectomy - Larry Hastings | PyCon 2016 Stop Writing Classes - Jack Diederich | PyCon 2012 The Packaging Gradient - Mahmoud Hashemi | PyBay 2017 Think Like a Pythonista - Luciano Ramalho | PyBay 2017 Thinking In Coroutines - \u0141ukasz Langa | PyCon 2016 To Mock, or Not to Mock, That is the Question - Ana Balica | PyCon 2016 Writing Awesome Command-Line Programs in Python - Mark Smith | EuroPython 2014","title":"Python"},{"location":"talks/favorite/#scala","text":"A Better Scala REPL - Li Haoyi | SBTB 2015","title":"Scala"},{"location":"talks/favorite/#unix","text":"The Unix Chainsaw - Gary Bernhardt | Cascadia Ruby Conf 2011","title":"Unix"},{"location":"talks/netflix-atlas-telemetry-a-platform-begets-an-ecosystem/","text":"Netflix Atlas Telemetry - A Platform Begets an Ecosystem \u00b6","title":"Netflix Atlas Telemetry - A Platform Begets an Ecosystem"},{"location":"talks/netflix-atlas-telemetry-a-platform-begets-an-ecosystem/#netflix-atlas-telemetry-a-platform-begets-an-ecosystem","text":"","title":"Netflix Atlas Telemetry - A Platform Begets an Ecosystem"}]}