{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Copperlight Writes","text":"2021-12-01 Discuss <p>The purpose of this site is to share notes and neat ways of solving problems. If you have questions or comments on this site, file a GitHub issue and discussion will take place there.</p>"},{"location":"#top-10-popular-pages-nov-2021","title":"Top-10 Popular Pages (Nov 2021)","text":"<ul> <li>Ansible Vault and SSH Key Distribution</li> <li>Measuring Transfer Speed Over Time with cURL</li> <li>Running Ansible Playbooks on Windows</li> <li>Testing AWS Clients with IAM AssumeRole Credentials in Scala</li> <li>Scalafmt Configuration Tips</li> <li>AWS Credential Files for Java and Python</li> <li>Build a Fake Instance Metadata Server for Ubuntu on Vagrant </li> <li>JMH Testing with Scala</li> <li>Reset Jenkins Build Number</li> <li>Garmin 935 Navigation</li> <li>Galaxy's Best Margarita</li> </ul>"},{"location":"ansible/ansible-vault-and-ssh-key-distribution/","title":"Ansible Vault and SSH Key Distribution","text":"2014-06-30 Discuss <p>There are two types of SSH key distribution discussed in this post: private keys on local hosts and public keys on remote hosts.  SSH private key distribution is best used for setting up your own workstation or possibly an Ansible Tower server.  In general, you should not be distributing private keys widely; with a good SSH tunneling configuration and SSH public key distribution, there should be no need for the private keys to be installed in more than few places.  This configuration will show off a technique for configuring an SSH jump host bastion that allows you to keep your private key on your own workstation; there is no need to have the SSH private key on the bastion host.</p> <p>For the purpose of this post, I have generated a new SSH key pair to demonstrate this technique; this keypair is used nowhere.  Part of the trick to making this work is that the private key needs to be base64 encoded so that line breaks are preserved when it is stored as a yaml string in the <code>vars_files</code>.  Template files are created for the public and private keys to preserve file change detection.</p> <p>Generate a new key pair:</p> <pre><code>$ ssh-keygen -b 2048 -f junk_key -C junk\nGenerating public/private rsa key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in junk_key.\nYour public key has been saved in junk_key.pub.\nThe key fingerprint is:\n94:94:ae:ac:c3:5e:ee:7d:fa:2c:cb:0f:ae:19:c8:99 junk\nThe key's randomart image is:\n+--[ RSA 2048]----+\n|        ..       |\n|       ...       |\n|       .o        |\n|       ..        |\n|     . .S        |\n|   . +o          |\n|   .E.o .        |\n|    +o *.o.      |\n|   ..o=.*B+      |\n+-----------------+\n&lt;/pre&gt;\n</code></pre> <p>Base64 encode the private key:</p> <pre><code>base64 -i junk_key &gt; junk_key.b64\n</code></pre> <p>Create an Ansible vars_files yaml data file named <code>ssh_keys/ssh_key_vault.yml</code>.  The <code>ssh_private_key</code> variable should contain the base64 encoded private key and the <code>ssh_public_key</code> variable should contain the public key. Encrypt the file with <code>ansible-vault</code>:</p> <pre><code>ssh_private_key: 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\n\nssh_public_key: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\n</code></pre> <pre><code>ansible-vault encrypt ssh_keys/ssh_key_vault.yml\nVault password:\nConfirm Vault password:\nEncryption successful\n</code></pre> <p>Create an inventory file named <code>inventory</code>, showing off the SSH jump host connection capability:</p> <pre><code>[localhost]\nlocalhost ansible_connection=local\n\n[group-all:children]\ngroup-01\ngroup-02\n\n[group-01]\ni-00000001 ansible_ssh_host=bastion+192.168.1.1 ansible_ssh_user=remoteuser\ni-00000002 ansible_ssh_host=bastion+192.168.1.2 ansible_ssh_user=remoteuser\n\n[group-02]\ni-00000003 ansible_ssh_host=bastion+192.168.1.3 ansible_ssh_user=remoteuser\ni-00000004 ansible_ssh_host=bastion+192.168.1.4 ansible_ssh_user=remoteuser\n</code></pre> <p>Create a template file for the private key named <code>templates/HOME_.ssh_junk</code>:</p> <pre><code>{{ssh_private_key_decoded.stdout}}\n</code></pre> <p>Create a template file for the public key named <code>templates/HOME_.ssh_junk.pub</code>:</p> <pre><code>{{ssh_public_key}}\n</code></pre> <p>Create a template file for the SSH jump host configuration named <code>templates/HOME_.ssh_config</code>:</p> <pre><code>Host *\n    ServerAliveInterval 30\n    ServerAliveCountMax 5\n\nHost bastion\n    User {{remote_user}}\n    IdentityFile ~/.ssh/junk\n    Hostname bastion\n\nHost bastion+*\n    User {{remote_user}}\n    IdentityFile ~/.ssh/junk\n    ProxyCommand ssh -T -a bastion nc $(echo %h |cut -d+ -f2) %p 2&gt;/dev/null\n    StrictHostKeyChecking no\n</code></pre> <p>This configuration assumes that you have a consistent remote username defined on the bastion server and your protected hosts.</p> <p>Write a playbook to install the SSH key and configuration on your local workstation named <code>config_local-ssh.yml</code>:</p> <pre><code>- name: configure local ssh\n  hosts:\n  - localhost\n  gather_facts: false\n  sudo: false\n  vars:\n    local_home: \"{{ lookup('env','HOME') }}\"\n    local_user: \"{{ lookup('env','USER') }}\"\n    remote_user: remoteuser\n  vars_files:\n  - ssh_keys/ssh_key_vault.yml\n  tasks:\n  - file: path={{local_home}}/.ssh state=directory mode=0700 owner={{local_user}}\n\n  - template: src=templates/HOME_.ssh_config dest={{local_home}}/.ssh/config mode=0644 owner={{local_user}} backup=yes\n\n  - shell: echo {{ssh_private_key}} |base64 --decode\n    register: ssh_private_key_decoded\n\n  - template: src=templates/HOME_.ssh_junk dest={{local_home}}/.ssh/junk mode=0600 owner={{local_user}}\n\n  - template: src=templates/HOME_.ssh_junk.pub dest={{local_home}}/.ssh/junk.pub mode=0644 owner={{local_user}}\n</code></pre> <p>Run the playbook to setup your local workstation with SSH keys and configuration:</p> <pre><code>ansible-playbook -i inventory config_local-ssh.yml --ask-vault-pass\nVault password:\n</code></pre> <p>Test your SSH tunneling access to a remote host behind the bastion server:</p> <pre><code>ssh bastion+192.168.1.1\nssh bastion+192.168.1.1 \"date; date &gt; /tmp/date.out\"\nscp bastion+192.168.1.1:/tmp/date.out .\n</code></pre> <p>Notice that the hosts behind the bastion server are referenced in the SSH command the same way that they are referenced in the Ansible inventory file.  The \"+\" character used as a separator was selected explicitly for its ability to be used interchangeably at the command line and in the Ansible inventory.  IP addresses are being used on the right hand side of the expression since the secondary connection to the protected host relies on the name resolution capabilities of the first host in the tunnel.  If you had a reliable dynamic DNS service that was keeping up with changes to the protected hosts and was accessible to the bastion host, then you could use host names instead, such as <code>bastion+webserver01</code>.  This host selection technique can be extended to an Ansible dynamic inventory script, if you were running instances at a cloud provider such as AWS.  When you write a dynamic inventory script, the data format should look like this:</p> <pre><code>{\n    \"group-01\": {\n        \"hosts\": [\n            \"i-00000001\",\n            \"i-00000002\"\n        ]\n    },\n    \"group-02\": {\n        \"hosts\": [\n            \"i-00000003\",\n            \"i-00000004\"\n        ]\n    },\n    \"group-all\": {\n        \"children\": [\n            \"group-01\",\n            \"group-02\"\n        ]\n    },\n    \"localhost\": [\n        \"localhost\"\n    ],\n    \"_meta\": {\n        \"hostvars\": {\n            \"i-00000001\": {\n                \"ansible_ssh_host\": \"bastion+192.168.1.1\",\n                \"ansible_ssh_user\": \"remoteuser\"\n            },\n            \"i-00000002\": {\n                \"ansible_ssh_host\": \"bastion+192.168.1.2\",\n                \"ansible_ssh_user\": \"remoteuser\"\n            },\n            \"i-00000003\": {\n                \"ansible_ssh_host\": \"bastion+192.168.1.3\",\n                \"ansible_ssh_user\": \"remoteuser\"\n            },\n            \"i-00000004\": {\n                \"ansible_ssh_host\": \"bastion+192.168.1.4\",\n                \"ansible_ssh_user\": \"remoteuser\"\n            },\n            \"localhost\": {\n                \"ansible_connection\": \"local\"\n            },\n        }\n    }\n}\n</code></pre> <p>The nice thing about this style of SSH configuration is that you can have multiple bastion hosts in different locations and target the hosts behind each of them, provided that you give your bastion hosts different names.  The method of accessing them is the same between direct SSH connections and Ansible execution.  Once you have this infrastructure in place, you can start distributing public SSH keys to your protected hosts.</p> <p>Write a <code>templates/etc_sudoers</code> file that grants NOPASSWD access to the sudo group:</p> <pre><code>Defaults    env_reset\nDefaults    mail_badpass\nDefaults    secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n\n# Host alias specification\n\n# User alias specification\n\n# Cmnd alias specification\n\n# User privilege specification\nroot    ALL=(ALL:ALL) ALL\n\n# Allow members of group sudo to execute any command\n%sudo   ALL=NOPASSWD: ALL\n\n#includedir /etc/sudoers.d\n</code></pre> <p>Write a playbook <code>update_remote-ssh.yml</code> to configure NOPASSWD sudo access for your remote user and distribute SSH public keys on your remote hosts.  This will allow subsequent playbook execution to operate more easily against your remote hosts.  In order for this to work, the paramiko connection type must be used initially, so that the password can be requested once and re-used across all hosts.</p> <pre><code>- name: update remote ssh\n  hosts:\n  - group-all\n  gather_facts: false\n  sudo: true\n  connection: paramiko\n  vars_files:\n  - ssh_keys/ssh_key_vault.yml\n  tasks:\n  - copy: src=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root\n\n  - user: name=remoteuser groups=sudo shell=/bin/bash state=present\n\n  - authorized_key: user=remoteuser state=present key={{ssh_public_key}}\n</code></pre> <p>Run an SSH configuration playbook against remote hosts through the SSH tunnel, providing the SSH password, sudo password and vault password:</p> <pre><code>ansible-playbook -i inventory update_remote-ssh.yml --ask-pass --ask-sudo-pass --ask-vault-pass\nSSH password:\nsudo password [defaults to SSH password]:\nVault password:\n\nPLAY [update ssh] *************************************************************\n\nTASK: [copy src=templates/etc_sudoers dest=/etc/sudoers mode=0440 owner=root group=root] ***\nok: [i-00000001]\nok: [i-00000002]\nok: [i-00000003]\nok: [i-00000004]\n\nTASK: [user name=remoteuser groups=sudo shell=/bin/bash state=present] ***\nok: [i-00000001]\nok: [i-00000002]\nok: [i-00000003]\nok: [i-00000004]\n\nTASK: [authorized_key user=remoteuser state=present key=\"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDIocVcldGnLIFZqGVrXdv0qS6GKkex1UPG3xl+uTWOPBKDgcFrffSAGmo9jkkSrEo5YDBh97EumC4SnLiCP0pGp2VKgiVthWUplG/wWFAf9s7If7Rb4igSyA7PbjNJU8Q6uZ7PC3ffe32G3jWVg8AvOycRnOihRrx7kmxOw4XcLt+IfDgAi9L35VQnLajRG8/t8sEqZYADuChg/vBXOJI+USRREcNrbmlQ11JJioLbpwbhBw4zEK43LAQUrguLIKaymxYnLdal0JBcIjkIG0gRQ23NxCItP/AWmjiIXPHgMOcOV6C8br9EbNCPVGvJ0S35LlYbOzdsEaXzX1z8mM7l junk\"] ***\nok: [i-00000001]\nok: [i-00000002]\nok: [i-00000003]\nok: [i-00000004]\n\n\nPLAY RECAP ********************************************************************\ni-00000001               : ok=3    changed=0    unreachable=0    failed=0\ni-00000002               : ok=3    changed=0    unreachable=0    failed=0\ni-00000003               : ok=3    changed=0    unreachable=0    failed=0\ni-00000004               : ok=3    changed=0    unreachable=0    failed=0\n</code></pre> <p>The best way to keep Ansible output concise is to run without verbosity -- only crank this up if you need it to diagnose a problem.</p> <p></p>"},{"location":"ansible/running-ansible-playbooks-on-windows/","title":"Running Ansible Playbooks on Windows","text":"2014-06-29 Discuss"},{"location":"ansible/running-ansible-playbooks-on-windows/#but-first-some-history","title":"But First, Some History","text":"<p>In early 2006, running almost a thousand servers for Blackboard Product Development that were evenly distributed across Windows, Linux and Solaris, we needed an automation tool that would allow us to quickly deploy and configure the Blackboard Learning Management System (LMS).  The real sticking point for us was managing the Windows ecosystem.</p> <p>The first commit of PuppetLabs Puppet occurred in April 2005 and the first tagged release was on Jan 3, 2006.  The first commit of OpsCode Chef occurred in March 2008 and the first tagged release was on Jan 31, 2009.  Needless to say, the modern configuration management ecosystem was much sparser in 2006 then as compared to today.  Puppet was still very new and in the process of gaining mind share and adding functionality; it did not support Windows at first.  CFEngine was available, but it also did not support Windows in the open source version.</p> <p>I worked with Dave Carter at the beginning of 2006 to develop our own in-house configuration management system that we called Fusion.  It was based on Ant and Ant-Contrib.  Since Blackboard was a Java based application, we always had one or more versions of JDKs installed on our systems as a part of the imaging or virtual machine cloning process, so picking a tool that ran on the JDK made sense and offered us the platform independence we needed.  Dave developed a state machine with a socket listener that would accept XML-formatted messages and then kick off various tasks.  I developed a library and property inheritance hierarchy for the the system, along with a parallel job execution client and added the set of scripts that deployed and configured the Blackboard LMS.  I figured out that by cherry-picking a few key utilities out of the UnxUtils distribution, I could write Windows batch scripts in a manner similar to Linux bash scripts and this lent to a somewhat manageable level of consistency between the disparate operating systems without completely abandoning the hooks we needed for Windows.</p>"},{"location":"ansible/running-ansible-playbooks-on-windows/#motivation","title":"Motivation","text":"<p>Anyone who has spoken with me in the past year about my work knows that Ansible is hands-down my favorite piece of software tooling.  Using Ansible, I was able to effectively manage 500-odd Ubuntu Linux systems at Blackboard, half of which were deployed in a VPC at AWS and half of which were deployed in the Blackboard Managed Hosting data centers.  Part of the challenge that we had at Blackboard in the Product Development department is that 30-40% of the several thousand servers used for development and testing were Windows, which made it difficult to choose a single configuration management system to rule them all.  For the better part of a year, I kept saying that it was a terrible shame that Ansible did not support Windows and that Opscode Chef would probably be the best choice for configuration management of all systems, since it arguably had the best support for that platform in 2013.  After meeting with Michael DeHaan, creator and CTO of Ansible, at Blackboard headquarters, we talked through their rough plans for Windows support.  In short, it was something they wanted to be thoughtful about.  Some time later, we had a hack day organized at Blackboard and I decided that I would attempt to develop an Ansible playbook that could install and uninstall a JDK on Windows, using my previous experience with building the Fusion configuration management system.</p> <p>It turns out that this works. Quite well. Even though it wasn't intended to.</p>"},{"location":"ansible/running-ansible-playbooks-on-windows/#ansible-roadmap-update","title":"Ansible Roadmap Update","text":"<p>On June 19, 2014, Michael DeHaan announced Windows Is Coming. PowerShell remoting is a far cleaner solution and I am looking forward to seeing it hit the release branch, although I don't have to worry about Windows machines so much these days.  I learned this nifty fact from Ansible Weekly Issue 38; this is not a bad way to keep up on the latest Ansible news.</p>"},{"location":"ansible/running-ansible-playbooks-on-windows/#pre-requisites","title":"Pre-Requisites","text":"<p>Windows + Cygwin + SSHd + Python</p>"},{"location":"ansible/running-ansible-playbooks-on-windows/#playbooks","title":"Playbooks","text":"<p>Ansible inventory file <code>hosts</code>:</p> <pre><code>[w7x64-jf]\nw7x64-jf.pd.local\n</code></pre> <p>JDK7 installation playbook <code>jdk7_install.yml</code>:</p> <pre><code>- name: install oracle jdk7\n  hosts:\n  - w7x64-jf\n  user: administrator\n  gather_facts: false\n  vars:\n    version: 7u45\n    build: b18\n    version_padded: 1.7.0_45\n    dodrootca2: c:\\\\jdk\\{{version_padded}}\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem\n    cacerts: c:\\\\jdk{{version_padded}}\\\\jre\\\\lib\\\\security\\\\cacerts\n  tasks:\n  - command: wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/{{version}}-{{build}}/jdk-{{version}}-windows-x64.exe creates=/usr/local/src/jdk-{{version}}-windows-x64.exe\n\n  - file: path=/usr/local/src/jdk-{{version}}-windows-x64.exe mode=0755\n\n  - shell: /usr/local/src/jdk-{{version}}-windows-x64.exe /s INSTALLDIR=c:\\\\jdk{{version_padded}} /INSTALLDIRPUBJRE=c:\\\\jre{{version_padded}} REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk{{version_padded}}\n\n  - copy: src=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/dod.root-ca-2.pem\n\n  - command: /cygdrive/c/jdk{{version_padded}}/bin/keytool -import -trustcacerts -alias dodrootca2 -file {{dodrootca2}} -keystore $cacerts -storepass changeit -noprompt\n    ignore_errors: true\n\n  - copy: src=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/US_export_policy.jar\n\n  - copy: src=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk{{version_padded}}/jre/lib/security/local_policy.jar\n\n  - shell: /cygdrive/c/jdk{{version_padded}}/bin/java -version 2&gt;&amp;1 |head -1 |awk '{print $3}' |sed -e 's/\"//g'\n    register: java_version\n\n  - fail: msg=\"The Java version does not match the expected value {{ version_padded }}.\"\n    when: \"'{{ java_version.stdout }}' != '{{ version_padded }}'\"\n</code></pre> <p>JDK7 uninstallation playbook <code>jdk7_uninstall.yml</code>:</p> <pre><code>- name: uninstall oracle jdk7\n  hosts:\n  - w7x64-jf\n  user: administrator\n  gather_facts: false\n  vars:\n    version: 7u45\n    version_padded: 1.7.0_45\n    version_text: \"7 Update 45\"\n  tasks:\n  - template: src=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs\n\n  - shell: cscript remove-programs.vbs |grep \"Java {{version_text}} (64-bit)\" chdir=/usr/local/src\n    register: result\n    ignore_errors: true\n\n  - command: cscript remove-programs.vbs /uninstall \"Java {{version_text}} (64-bit)\" chdir=/usr/local/src\n    when: result|success\n\n  - shell: cscript remove-programs.vbs |grep \"Java SE Development Kit {{version_text}} (64-bit)\" chdir=/usr/local/src\n    register: result\n    ignore_errors: true\n\n  - command: cscript remove-programs.vbs /uninstall \"Java SE Development Kit {{version_text}} (64-bit)\" chdir=/usr/local/src\n    when: result|success\n\n  - file: path={{item}} state=absent\n    with_items:\n    - /usr/local/src/jdk-{{version}}-windows-x64.exe\n    - /usr/local/src/jdk-install-log.txt\n    - /usr/local/src/remove-programs.vbs\n</code></pre> <p>The <code>remove-programs.vbs</code> helper script:</p> <pre><code>If Wscript.Arguments.Count = 0 Then\n  inventory_software()\nElseIf Wscript.Arguments.Count = 2 Then\n  If Wscript.Arguments(0) = \"/uninstall\" Then\n    'Expecting: cscript remove-programs.vbs /uninstall \"Java(TM) 6 Update 26\"\n    'Expecting: cscript remove-programs.vbs /uninstall \"Java(TM) SE Development Kit 6 Update 26\"\n    uninstall_software(Wscript.Arguments(1))\n  Else\n    Wscript.Echo \"Usage: remove-programs.vbs [/uninstall \"\"software\"\"]\"\n  End If\nElse\n  Wscript.Echo \"Usage: remove-programs.vbs [/uninstall \"\"software\"\"]\"\nEnd If\n\nSub inventory_software()\n  strComputer = \".\"\n\n  Set objWMIService = GetObject(\"winmgmts:\" _\n    &amp; \"{impersonationLevel=impersonate}!\\\\\" _\n    &amp; strComputer &amp; \"\\root\\cimv2\")\n  Set colSoftware = objWMIService.ExecQuery _\n    (\"Select * from Win32_Product\")\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo \"Name: \" &amp; objSoftware.Name\n    'Wscript.Echo \"Version: \" &amp; objSoftware.Version\n  Next\nEnd Sub\n\nSub inventory_java_software()\n  strComputer = \".\"\n\n  Set objWMIService = GetObject(\"winmgmts:\" _\n    &amp; \"{impersonationLevel=impersonate}!\\\\\" _\n    &amp; strComputer &amp; \"\\root\\cimv2\")\n  Set colSoftware = objWMIService.ExecQuery _\n    (\"Select * from Win32_Product \" _\n        &amp; \"Where Name Like 'Java%'\")\n\n  For Each objSoftware in colSoftware\n    Wscript.Echo \"Name: \" &amp; objSoftware.Name\n    Wscript.Echo \"Version: \" &amp; objSoftware.Version\n  Next\nEnd Sub\n\nSub uninstall_software(strApplicationName)\n  'Make sure to run this with Administrator privileges\n  strComputer = \".\"\n\n  Set objWMIService = GetObject(\"winmgmts:\" _\n    &amp; \"{impersonationLevel=impersonate}!\\\\\" _\n    &amp; strComputer &amp; \"\\root\\cimv2\")\n  Set colSoftware = objWMIService.ExecQuery _\n    (\"Select * From Win32_Product Where Name = '\" _\n    &amp; strApplicationName &amp; \"'\")\n\n  For Each objSoftware in colSoftware\n    objSoftware.Uninstall()\n  Next\nEnd Sub\n</code></pre>"},{"location":"ansible/running-ansible-playbooks-on-windows/#demonstration","title":"Demonstration","text":"<p>File system state before install:</p> <pre><code>administrator@W7X64-JF~\n$ ls -l /cygdrive/c |egrep \"jdk|jre\"\ndrwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5\ndrwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6\ndrwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7\n</code></pre> <p>Install JDK7 on Windows:</p> <pre><code>copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_install.yml\n\nPLAY [install oracle jdk7] ****************************************************\n\nTASK: [command wget -P /usr/local/src --no-cookies --no-check-certificate --header Cookie:gpw_e24=http%3A%2F%2Fwww.oracle.com http://download.oracle.com/otn-pub/java/jdk/7u45-b18/jdk-7u45-windows-x64.exe creates=/usr/local/src/jdk-7u45-windows-x64.exe] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [file path=/usr/local/src/jdk-7u45-windows-x64.exe mode=0755] ***********\nchanged: [w7x64-jf.pd.local]\n\nTASK: [shell /usr/local/src/jdk-7u45-windows-x64.exe /s INSTALLDIR=c:\\\\jdk1.7.0_45 /INSTALLDIRPUBJRE=c:\\\\jre1.7.0_45 REBOOT=Suppress ADDLOCAL=ToolsFeature,SourceFeature,PublicjreFeature AUTOUPDATE=0 SYSTRAY=0 SYSTRAY=0 /L c:\\\\cygwin\\\\usr\\\\local\\\\src\\\\jdk-install-log.txt creates=/cygdrive/c/jdk1.7.0_45] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [copy src=../certs/dod.root-ca-2.pem dest=/cygdrive/c/jdk1.7.0_45/jre/lib/security/dod.root-ca-2.pem] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [command /cygdrive/c/jdk1.7.0_45/bin/keytool -import -trustcacerts -alias dodrootca2 -file c:\\\\jdk1.7.0_45\\\\jre\\\\lib\\\\security\\\\dod.root-ca-2.pem -keystore c:\\\\jdk1.7.0_45\\\\jre\\\\lib\\\\security\\\\cacerts -storepass changeit -noprompt] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [copy src=files/cygdrive_c_java_jre_lib_security_US_export_policy.jar dest=/cygdrive/c/jdk1.7.0_45/jre/lib/security/US_export_policy.jar] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [copy src=files/cygdrive_c_java_jre_lib_security_local_policy.jar dest=/cygdrive/c/jdk1.7.0_45/jre/lib/security/local_policy.jar] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [shell /cygdrive/c/jdk1.7.0_45/bin/java -version 2&gt;&amp;1 |head -1 |awk '{print $3}' |sed -e 's/\"//g'] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [fail msg=\"The Java version does not match the expected value 1.7.0_45.\"] ***\nskipping: [w7x64-jf.pd.local]\n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local : ok=8 changed=8 unreachable=0 failed=0\n</code></pre> <p>File system state after install and before uninstall:</p> <pre><code>administrator@W7X64-JF ~\n$ ls -l /cygdrive/c |egrep \"jdk|jre\"\ndrwx------+ 1 SYSTEM SYSTEM 0 Dec 12 23:17 jdk1.7.0_45\ndrwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5\ndrwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6\ndrwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7\ndrwx------+ 1 SYSTEM SYSTEM 0 Dec 12 23:17 jre1.7.0_45\n</code></pre> <p>Uninstall JDK7 on Windows:</p> <pre><code>copperpro:windows-hack mjohnson$ ansible-playbook -i hosts jdk7_uninstall.yml\n\nPLAY [uninstall oracle jdk7] **************************************************\n\nTASK: [template src=templates/usr_local_src_remove-programs.vbs dest=/usr/local/src/remove-programs.vbs] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [shell cscript remove-programs.vbs |grep \"Java 7 Update 45 (64-bit)\" chdir=/usr/local/src] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [command cscript remove-programs.vbs /uninstall \"Java 7 Update 45 (64-bit)\" chdir=/usr/local/src] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [shell cscript remove-programs.vbs |grep \"Java SE Development Kit 7 Update 45 (64-bit)\" chdir=/usr/local/src] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [command cscript remove-programs.vbs /uninstall \"Java SE Development Kit 7 Update 45 (64-bit)\" chdir=/usr/local/src] ***\nchanged: [w7x64-jf.pd.local]\n\nTASK: [file path=$item state=absent] ******************************************\nchanged: [w7x64-jf.pd.local] =&gt; (item=/usr/local/src/jdk-7u45-windows-x64.exe)\nchanged: [w7x64-jf.pd.local] =&gt; (item=/usr/local/src/jdk-install-log.txt)\nchanged: [w7x64-jf.pd.local] =&gt; (item=/usr/local/src/remove-programs.vbs)\n\nPLAY RECAP ********************************************************************\nw7x64-jf.pd.local : ok=6 changed=6 unreachable=0 failed=0\n</code></pre> <p>File system state after uninstall:</p> <pre><code>administrator@W7X64-JF ~\n$ ls -l /cygdrive/c |egrep \"jdk|jre\"\ndrwx------+ 1 SYSTEM SYSTEM 0 Jun 24 2010 jdk5\ndrwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk6\ndrwx------+ 1 SYSTEM SYSTEM 0 Oct 4 2012 jdk7\n</code></pre> <p></p>"},{"location":"ansible/testing-ansible-galaxy-roles-with-docker/","title":"Testing Ansible Galaxy Roles with Docker","text":"2014-10-14 Discuss <p>I learned a neat trick for testing Ansible Galaxy roles at AnsibleFest 2014.</p> <p>To get started with Docker on your Mac, install VirtualBox and then install boot2docker, using Homebrew. The boot2docker package will install docker as a dependency and it runs a small (24MB) Linux VirtualBox virtual machine that provides a platform for running Docker images:</p> <pre><code>brew install boot2docker\nboot2docker init\nboot2docker up\n$(boot2docker shellinit)\n</code></pre> <p>The Ansible Team (thanks, Toshio!) has released Docker images that are included in the Docker Hub Registry, which can be used for testing:</p> <pre><code>docker search ansible |grep ^ansible\nansible/ubuntu14.04-ansible                    Ubuntu 14.04 LTS with ansible                   12\nansible/centos7-ansible                        Ansible on Centos7                              11\n</code></pre> <p>There are two tags associated with each of these images: latest and devel.  The latest contains a layered Ansible stable and devel contains a layered Ansible HEAD.  Pull one of these images from the Docker Hub:</p> <pre><code>docker pull ansible/ubuntu14.04-ansible\n</code></pre> <p>This technique will allow you to rinse and repeat installs with different roles, which will help you figure out which ones deliver the most suitable functionality for your use cases.</p> <p>Launch the docker image and leave a shell process running:</p> <pre><code>docker run -i -t ansible/ubuntu14.04-ansible:stable /bin/bash\n</code></pre> <p>Download a role from Ansible Galaxy:</p> <pre><code>ansible-galaxy install geerlingguy.memcached\n</code></pre> <p>Create a local site.yml playbook to run the role:</p> <pre><code>- hosts: localhost\n  roles:\n      - role: geerlingguy.memcached\n</code></pre> <p>Execute the playbook:</p> <pre><code>root@ccca9e7f365f:/# ansible-playbook site.yml -c local\n\nPLAY [localhost] **************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nskipping: [localhost]\n\nTASK: [geerlingguy.memcached | Update apt cache.] *****************************\nok: [localhost]\n\nTASK: [geerlingguy.memcached | Install Memcached.] ****************************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Copy Memcached configuration.] *****************\nchanged: [localhost]\n\nTASK: [geerlingguy.memcached | Ensure Memcached is started and set to run on startup.] ***\nchanged: [localhost]\n\nNOTIFIED: [geerlingguy.memcached | restart memcached] *************************\nchanged: [localhost]\n\nPLAY RECAP ********************************************************************\nlocalhost                  : ok=6    changed=4    unreachable=0    failed=0\n\nroot@ccca9e7f365f:/#\n</code></pre> <p></p>"},{"location":"aws/assume-role-with-awscli/","title":"Assume Role with AWSCLI","text":"2016-05-19 Discuss <p>If you have either static or instance profile credentials that grant you STS permissions, then you can gather a set of time-limited role credentials as follows:</p> <pre><code>#!/bin/bash\n\nTEST_CREDENTIALS=$( \\\n  aws sts assume-role \\\n  --role-arn arn:aws:iam::$AWS_ACCOUNT_ID_1:role/$ROLE_NAME \\\n  --role-session-name $USER \\\n  |jq '.Credentials'\n)\n\nPROD_CREDENTIALS=$( \\\n  aws sts assume-role \\\n  --role-arn arn:aws:iam::$AWS_ACCOUNT_ID_2:role/$ROLE_NAME \\\n  --role-session-name $USER \\\n  |jq '.Credentials'\n)\n\ncat &gt;&gt;$HOME/.aws/credentials &lt;&lt;EOF\n[test-$ROLE_NAME]\naws_access_key_id=$(echo $TEST_CREDENTIALS |jq -r '.AccessKeyId')\naws_secret_access_key=$(echo $TEST_CREDENTIALS |jq -r '.SecretAccessKey')\naws_session_token=$(echo $TEST_CREDENTIALS |jq -r '.SessionToken')\nexpiration=$(echo $TEST_CREDENTIALS |jq -r '.Expiration')\n\n[prod-$ROLE_NAME]\naws_access_key_id=$(echo $PROD_CREDENTIALS |jq -r '.AccessKeyId')\naws_secret_access_key=$(echo $PROD_CREDENTIALS |jq -r '.SecretAccessKey')\naws_session_token=$(echo $PROD_CREDENTIALS |jq -r '.SessionToken')\nexpiration=$(echo $PROD_CREDENTIALS |jq -r '.Expiration')\nEOF\n</code></pre> <p></p>"},{"location":"aws/aws-credential-files-for-java-and-python/","title":"AWS Credential Files for Java and Python","text":"2014-07-14 Discuss <p>Each of the AWS tools has slightly different expectations about the location and naming of the credentials file and the various properties within it.  It seems like the Python tools are moving closer to the Java standard as they iterate through releases, but it is still necessary to use a patchwork solution to be able to have a unified credentials file.</p>"},{"location":"aws/aws-credential-files-for-java-and-python/#java-sdk","title":"Java SDK","text":"<p> Version: \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\"      Installation: sbt      Link: AWS Java SDK Class ProfilesConfigFile <p>The standard location for the credentials file is <code>~/.aws/credentials</code>, which can be overridden with the <code>AWS_CREDENTIAL_PROFILES_FILE</code> environment variable or by specifying an alternate file location in the constructor.  The format of this file is described below:</p> <pre><code>[default]\naws_access_key_id=testAccessKey\naws_secret_access_key=testSecretKey\naws_session_token=testSessionToken\n\n[test-user]\naws_access_key_id=testAccessKey\naws_secret_access_key=testSecretKey\naws_session_token=testSessionToken\n</code></pre>"},{"location":"aws/aws-credential-files-for-java-and-python/#java-command-line-tools","title":"Java Command Line Tools","text":"Version: 1.6.13.0      Installation: brew install ec2-api-tools      Link: Setting Up the Amazon EC2 Command Line Interface Tools on Linux/Unix and Mac OS X <p>The standard configuration is to use environment variables, since these tools have not been updated to read the standard AWS credentials file.  Add the following to your <code>~/.bash_profile</code>, to link the required data to the standard credentials file and allow for session tokens:</p> <pre><code>export AWS_CREDENTIAL_FILE=\"$HOME/.aws/credentials\"\nexport AWS_ACCESS_KEY=\"$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\"\nexport AWS_SECRET_KEY=\"$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\"\nexport AWS_DELEGATION_TOKEN=\"$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\"\n</code></pre>"},{"location":"aws/aws-credential-files-for-java-and-python/#python-boto","title":"Python Boto","text":"Version: boto==2.31.1  botocore==0.56.0      Installation: pip install boto      Link: Boto Config <p>The latest version of boto needs to have <code>aws_security_token</code> defined, rather than <code>aws_session_token</code>, in the credentials file.  The simplest solution for this is to duplicate the token between both names; the Java SDK will throw the following log message when reading the extra property, but will work as expected: <code>INFO: Skip unsupported property name aws_security_token in profile [default].</code>  Boto will not throw log messages about the existence of the <code>aws_session_token</code> property.</p>"},{"location":"aws/aws-credential-files-for-java-and-python/#aws-cli","title":"AWS CLI","text":"Version: aws-cli/1.3.22      Installation: pip install awscli      Link: Configuring the AWS Command Line Interface <p>The standard location for the credentials file is <code>~/.aws/config</code>, which can be overridden with the <code>AWS_CREDENTIAL_FILE</code> environment variable.  The latest version of this tool accepts the Java SDK credential file format as-is, including the use of <code>aws_session_token</code>, whereas previous versions wanted <code>aws_security_token</code> instead.  When you have multiple profiles in the credentials file, you can select a profile with the tool like so:</p> <pre><code>aws --profile test-user s3 ls\n</code></pre>"},{"location":"aws/aws-credential-files-for-java-and-python/#unified-solution","title":"Unified Solution","text":"<p>The best approach for creating a unified credentials file is to follow the Java credentials file format as closely as possible, while redirecting the Python tools to that file and adding properties to cover the corner cases.</p> <p>To do this, create a <code>~/.aws/credentials</code> file that duplicates the necessary properties:</p> <pre><code>[default]\naws_access_key_id=testAccessKey\naws_secret_access_key=testSecretKey\naws_session_token=testSessionToken\naws_security_token=testSessionToken\n\n[test-user]\naws_access_key_id=testAccessKey\naws_secret_access_key=testSecretKey\naws_session_token=testSessionToken\naws_security_token=testSessionToken\n\n[prod-user]\naws_access_key_id=testAccessKey\naws_secret_access_key=testSecretKey\naws_session_token=testSessionToken\naws_security_token=testSessionToken\n</code></pre> <p>And add a section to your <code>~/.bash_profile</code>:</p> <pre><code>export AWS_CREDENTIAL_FILE=\"$HOME/.aws/credentials\"\nexport AWS_ACCESS_KEY=\"$(grep aws_access_key_id $AWS_CREDENTIAL_FILE |cut -d= -f2)\"\nexport AWS_SECRET_KEY=\"$(grep aws_secret_access_key $AWS_CREDENTIAL_FILE |cut -d= -f2)\"\nexport AWS_DELEGATION_TOKEN=\"$(grep aws_session_token $AWS_CREDENTIAL_FILE |cut -d= -f2)\"\n</code></pre> <p>With this configuration, you should be able to move seamlessly between the various Java and Python tools available for AWS.</p>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/","title":"Build a Fake Instance Metadata Server for Ubuntu on Vagrant","text":"2014-07-22 Discuss <p>Let's say that you have a proper build/bake/deploy pipeline in place for running appications in AWS. This is a reliable way to deploy applications at scale and move traffic between different versions of applications as a part of the deployment pipeline.  However, the whole process may take 20-40 minutes or so to complete for any particular build.  If you want to iterate more rapidly on your development efforts, you could skip the full process with a quickpatch ssh/rsync deployment to a single server or you could stand up a local Vagrant base image and iterate on that.  Now let's say that the application you are working on is intended to work with instance metadata, particularly for the purpose of obtaining a rotating set of access and secret keys.  It might be nice to have a fake metadata service running on your local Vagrant image so that you can test your application in a manner similar to how it will be running in the cloud.  In this post, I describe how to build and configure a fake metadata service for an Ubuntu image running on Vagrant.</p>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#package-layout","title":"Package Layout","text":"<p>This layout assumes that you will be installing custom packages to the <code>/apps</code> directory, and there is a daemontools service hierarchy located at <code>/service</code>.  The <code>fake-metadata/service/run</code> file is a script suitable for use with daemontools.</p> <pre><code>root/\n\u251c\u2500\u2500 apps\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 app.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 service\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 run\n\u2514\u2500\u2500 etc\n    \u251c\u2500\u2500 init.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u251c\u2500\u2500 logrotate.d\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 fake-metadata\n    \u2514\u2500\u2500 network\n        \u2514\u2500\u2500 iptables.rules\n</code></pre>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#building-and-packaging","title":"Building and Packaging","text":"<p>For cross-platform build packaging, it will be easiest to use the nebula ospackage plugin with Gradle. With this plugin available, your build script will look something like this:</p> <pre><code>apply plugin: 'nebula-ospackage'\n\nospackage {\n    version='1.0'\n    packageName='fake-metadata'\n\n    requires('python-flask')\n\n    link('/apps/fake-metadata/logs', '/mnt/logs/fake-metadata')\n    link('/service/fake-metadata', '/apps/fake-metadata/service')\n}\n\nbuildDeb {\n    postInstall file('scripts/postInstall.sh')\n    preUninstall 'svc -d /service/fake-metadata'\n    postUninstall file('scripts/postUninstall.sh')\n}\n\ntask build(dependsOn: ['buildDeb'])\n</code></pre>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#fake-metadata-application","title":"Fake Metadata Application","text":"<p>The simplest approach to building the service is to create a Flask service and have it run bare on the Vagrant instance.  Given how small it will be and limited amount of traffic it will need to serve, there is no need to run this behind a dedicated static webserver like nginx or Apache.  The nice thing about using Flask and having a basic structure in place is that it is then easy to extend the application to add other endpoints when needed.</p> <pre><code>#!/usr/bin/env python\n\nfrom flask import Flask, jsonify, abort, make_response, request\nimport os\nimport sys\n\n\napp = Flask(__name__)\n\n\nBaseIAMRole = {\n  'Code': 'Success',\n  'LastUpdated' : '',\n  'Type': 'AWS-HMAC',\n  'AccessKeyId': '',\n  'SecretAccessKey': '',\n  'Token': '',\n  'Expiration': ''\n}\n\n\n@app.route('/', methods = ['GET'])\ndef index():\n    return 'latest'\n\n\n@app.route('/latest/', methods = ['GET'])\ndef latest():\n    return 'meta-data'\n\n\n@app.route('/latest/meta-data/', methods = ['GET'])\ndef meta_data():\n    endpoints = [\n        'iam',\n        'public-hostname',\n        'public-ipv4'\n    ]\n    return ('\\n').join(endpoints)\n\n\n@app.route('/latest/meta-data/iam/', methods = ['GET'])\ndef iam():\n    return 'security-credentials'\n\n\n@app.route('/latest/meta-data/iam/security-credentials/', methods = ['GET'])\ndef security_credentials():\n    return 'BaseIAMRole'\n\n\n@app.route('/latest/meta-data/iam/security-credentials/BaseIAMRole', methods = ['GET'])\ndef base_iam_role():\n    # update the payload to contain a current set of accesss and secrey keys\n    return jsonify(BaseIAMRole)\n\n\n@app.route('/latest/meta-data/public-hostname', methods = ['GET'])\ndef public_hostname():\n    return os.environ['EC2_LOCAL_HOSTNAME']\n\n\n@app.route('/latest/meta-data/public-ipv4', methods = ['GET'])\ndef public_ipv4():\n    return os.environ['EC2_LOCAL_IPV4']\n\n\n@app.errorhandler(400)\ndef not_found(error):\n    return make_response(jsonify( { 'error': 'bad request' } ), 400)\n\n\n@app.errorhandler(404)\ndef not_found(error):\n    return make_response(jsonify( { 'error': 'not found' } ), 404)\n\n\n@app.errorhandler(500)\ndef not_found(error):\n    return make_response(jsonify( { 'error': 'internal server error' } ), 500)\n\n\nif __name__ == '__main__':\n    if len(sys.argv) &gt; 1:\n        if ':' in sys.argv[1]:\n            host=sys.argv[1].split(':')[0]\n            port=int(sys.argv[1].split(':')[1])\n            app.run(host=host, port=port)\n        else:\n            app.run(host=sys.argv[1])\n    else:\n        app.run(debug=True)\n</code></pre>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#daemontools-run-script","title":"Daemontools Run Script","text":"<p>This script is watched by the supervise process, which then starts (or restarts) the application if it is not running.  Switching to a non-root user and redirecting output to the log file occurs here.</p> <pre><code>#!/bin/bash\n\nulimit -n 32768\n\nsource /etc/profile.d/environment.sh\n\nexport PATH=/command:/usr/local/bin:/usr/local/sbin:/bin:/sbin:/usr/bin:/usr/sbin:/usr/local/scripts\n\nif [ ! -d \"/mnt/logs/fake-metadata\" ]; then\n  mkdir -p /mnt/logs/fake-metadata\n  chmod 0777 /mnt/logs/fake-metadata\nfi\n\nUSER=someuser\nPYTHON=/usr/bin/python\nAPP=/apps/fake-metadata/app.py\nOPTS=127.0.0.1:8000\nLOG=/mnt/logs/fake-metadata/server.log\n\necho \"starting fake-metadata\"\nexec setuidgid $USER $PYTHON $APP $OPTS &gt;&gt; $LOG 2&gt;&amp;1\n</code></pre>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postinstall-script","title":"PostInstall Script","text":"<p>This is where most of the trickiness occurs.</p> <p>The post install script is responsible for modifying the <code>/etc/network/interfaces</code> file, adding the metadata server IP address and configuring iptables in an idempotent manner.  When the post install script is packaged by the nebula ospackage plugin into a Debian package, it gets a <code>#!/bin/sh -e</code> shebang invocation, which means that the script will halt execution at any point where it evaluates a non-zero return code.  This means that the script needs to be written such a way that the environment state testing being done always returns a true value so that the script does not fail, hence the <code>||true</code> constructs.</p> <p>We are attaching an extra IP address to the loopback interface, so we need to redirect traffic from 169.254.169.254:80 to the location where the fake metadata server is running.  We are dealing with the loopback interface, which means that the PREROUTING nat table is never hit and we must use the OUTPUT table instead.  You cannot DNAT packets destined for the loopback interface, because the kernel will treat them as martians and drop them, so you must REDIRECT the packets to the desired port.  When performing the redirection from port 80 to 8000 on the loopback interface, it sends the packets to 127.0.0.1:8000, not 169.254.169.254:8000, so the fake metadata server must be listening on localhost port 8000.</p> <p>If for some reason, you need to troubleshoot the post install script, it can be found at <code>/var/lib/dpkg/info/fake-metadata.postinst</code> following an attempted package installation.</p> <pre><code>LINE=$( grep 169.254.169.254/32 /etc/network/interfaces || true )\nif [[ ! \"$LINE\" == *169.254.169.254/32*  ]]; then\n    sed -i '/iface lo inet loopback/a up ip addr add 169.254.169.254/32 dev lo scope host' /etc/network/interfaces\nfi\n\nLINE=$( /sbin/ip addr |grep 169.254.169.254/32 || true )\nif [[ ! \"$LINE\" == *169.254.169.254/32*  ]]; then\n    /sbin/ip addr add 169.254.169.254/32 dev lo scope host\nfi\n\nLINE=$( grep iptables-restore /etc/network/interfaces || true )\nif [[ ! \"$LINE\" == *iptables-restore*  ]]; then\n    sed -i '/up ip addr add/a pre-up iptables-restore &lt; /etc/network/iptables.rules' /etc/network/interfaces\nfi\n\nLINE=$( iptables -t nat -L |grep 8000 || true )\nif [[ ! \"$LINE\" == *8000*  ]]; then\n    iptables -t nat -A OUTPUT -p tcp -d 169.254.169.254/32 --dport 80 -j REDIRECT --to-ports 8000\nfi\n\n/usr/sbin/update-rc.d fake-metadata defaults\n</code></pre>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#postuninstall-script","title":"PostUninstall Script","text":"<p>This script is the inverse of the post install script; it returns the system to its previous state.</p> <pre><code>LINE=$( grep 169.254.169.254/32 /etc/network/interfaces || true )\nif [[ \"$LINE\" == *169.254.169.254/32*  ]]; then\n    sed -i '/up ip addr add 169.254.169.254\\/32 dev lo scope host/d' /etc/network/interfaces\nfi\n\nLINE=$( /sbin/ip addr |grep 169.254.169.254/32 || true )\nif [[ \"$LINE\" == *169.254.169.254/32*  ]]; then\n    /sbin/ip addr delete 169.254.169.254/32 dev lo scope host\nfi\n\nLINE=$( grep iptables-restore /etc/network/interfaces || true )\nif [[ \"$LINE\" == *iptables-restore*  ]]; then\n    sed -i '/pre-up iptables-restore &lt; \\/etc\\/network\\/iptables.rules/d' /etc/network/interfaces\nfi\n\nLINE=$( iptables -t nat -L |grep 8000 || true )\nif [[ \"$LINE\" == *8000*  ]]; then\n    iptables -t nat -D OUTPUT -p tcp -d 169.254.169.254/32 --dport 80 -j REDIRECT --to-ports 8000\nfi\n\n/usr/sbin/update-rc.d -f fake-metadata remove\nrm -rf /apps/fake-metadata\npkill -f 'supervise fake-metadata'\n</code></pre>"},{"location":"aws/build-a-fake-instance-metadata-server-for-ubuntu-on-vagrant/#log-rotation","title":"Log Rotation","text":"<p>To keep the local Vagrant instance clean, it is useful to configure log rotation.  Sending a HUP signal to the service allows it to continue writing to the new logfile.</p> <pre><code>/mnt/logs/fake-metadata/server.log {\n    daily\n    rotate 7\n    compress\n    missingok\n    notifempty\n    create 644 root root\n    postrotate\n        svc -h /service/fake-metadata\n    endscript\n}\n</code></pre> <p></p>"},{"location":"aws/checking-and-changing-content-type-of-s3-object-with-awscli/","title":"Checking and Changing Content-Type of S3 Object with AWSCLI","text":"2017-03-04 Discuss <pre><code>aws s3api get-object \\\n    --bucket my.bucket \\\n    --key foo/bar/2017-01-26/usage.json \\\n    usage.json\n\naws s3api copy-object \\\n    --bucket archive \\\n    --content-type \"application/rss+xml\" \\\n    --copy-source archive/test/test.html \\\n    --key test/test.html \\\n    --metadata-directive \"REPLACE\"\n</code></pre>"},{"location":"aws/configuring-multiple-interfaces-on-the-same-network-in-ec2/","title":"Configuring Multiple Interfaces on the Same Network in EC2","text":"2014-07-09 Discuss <p>I've been reading a bit lately on Linux policy and source-based routing for the purpose of configuring multiple NICs on the same network in EC2.  I found the following links helpful:</p> <ul> <li>Multiple IPs and ENIs on EC2 in a VPC</li> <li>Routing for Multiple Uplinks/Providers</li> <li>A Quick Introduction to Linux Policy Routing</li> </ul> <p></p>"},{"location":"github/how-i-built-my-site/","title":"How I Built My Site","text":"2021-12-01 Discuss <p>I recently rebuilt my GitHub Pages site, switching my tech stack to Python, MkDocs and Material for MkDocs. The previous workflow had too many moving parts and the theme was hard to read.  It turns out that MkDocs has some nice automation hooks for GitHub Pages and it looks great on mobile browsers with the Material theme. Since I use MkDocs for site generation at work, I am already pretty familiar with it.</p> <p>Writing documents in Markdown and turning them into web pages with a static site generator is the fastest and easiest way to post articles on your GitHub Pages site.  This format allows you to drop down into HTML, when necessary, to enhance your page formatting, but you shouldn't need to do this for most pages.</p>"},{"location":"github/how-i-built-my-site/#install-python-and-mkdocs-packages","title":"Install Python and MkDocs Packages","text":"<p>If you are relying on a system Python, consider following these instructions: Install the Latest Python Versions on Mac OSX.</p> <p>Install Material for MkDocs. This package will bring along all the necessary dependencies, including MkDocs, for a fully functioning site.</p> <pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"github/how-i-built-my-site/#sign-up-for-google-analytics","title":"Sign Up for Google Analytics","text":"<p>Note</p> <p>You must disable ad-blocking software in order to be able to see the Google Analytics page.</p> <ol> <li>Navigate to Google Analytics &gt; Admin</li> <li>Property &gt; Create New Property<ol> <li>Account Name: $YOUR_ACCOUNT_NAME</li> <li>Website Name: username.github.io</li> <li>Website URL: https://username.github.io</li> <li>Go with the default configuration options - you can change these later.</li> <li>Get Tracking ID</li> </ol> </li> <li>Note the Tracking ID (looks like: <code>UA-00000000-0</code>) assigned to this property.</li> </ol>"},{"location":"github/how-i-built-my-site/#build-the-site","title":"Build the Site","text":"<ol> <li> <p>Create a new repository named <code>$USERNAME.github.io</code>, where <code>$USERNAME</code> is your username (or organization name) on GitHub.  It must match exactly, or it will not work.</p> </li> <li> <p>Clone the repository locally. Requires SSH keys.</p> <pre><code>git clone git@github.com:$USERNAME/$USERNAME.github.io.git\ncd $USERNAME.github.com\n</code></pre> </li> <li> <p>Create a directory structure that looks like the following:</p> <pre><code>$USERNAME.github.io/\n\u251c\u2500\u2500 .github/\n\u2502   \u2514\u2500\u2500 workflows/\n\u2502       \u251c\u2500\u2500 pr.yml\n\u2502       \u2514\u2500\u2500 release.yml\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 css/\n\u2502   \u2502   \u2514\u2500\u2500 custom.css\n\u2502   \u2514\u2500\u2500 index.md\n\u251c\u2500\u2500 mkdocs.yml\n\u2514\u2500\u2500 requirements.txt\n</code></pre> </li> <li> <p>Create a GitHub Actions configuration file for pull requests (<code>./.github/workflows/pr.yml</code>). This will build the site for every PR that is submitted.</p> <pre><code>name: pull-request\n\non: [pull_request]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n        with:\n          python-version: 3.x\n      - run: pip install -r requirements.txt\n      - run: mkdocs build\n</code></pre> </li> <li> <p>Create a GitHub Actions configuration file for deploying the site (<code>./.github/workflows/release.yml</code>), replacing <code>$USERNAME</code> with your username.</p> <pre><code>name: release\n\non:\n  push:\n    branches:\n    - main\njobs:\n  deploy:\n    if: ${{ github.repository == '$USERNAME/$USERNAME.github.io' }}\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - uses: actions/setup-python@v2\n        with:\n          python-version: 3.x\n      - run: pip install -r requirements.txt\n      - run: git fetch origin gh-pages:gh-pages\n      - run: mkdocs gh-deploy\n</code></pre> </li> <li> <p>Create a <code>./mkdocs.yml</code> site configuration file. Choose a Creative Commons license for your site - I chose CC BY-NC-SA 4.0 for my site. Add your Google Analytics property Tracking ID to <code>extra.analytics</code>. If you do not have a Tracking ID, then delete these lines in the configuration file.</p> <pre><code>site_name: My Site\nsite_url: 'http://$USERNAME.github.io/'\nrepo_url: 'https://github.com/username/$USERNAME.github.io'\nedit_uri: ''\nsite_description: My Site\nsite_author: My Name\ncopyright: &lt;a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"&gt;&lt;img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /&gt;&lt;/a&gt;\n\nstrict: True\n\npages:\n- Home: index.md\n\ntheme: material\n\nextra:\n  analytics:\n    provider: google\n    property: UA-00000000-0\n\nextra_css:\n- css/custom.css\n\nmarkdown_extensions:\n- admonition\n- codehilite\n- pymdownx.tilde\n- toc:\n    permalink: True\n</code></pre> </li> <li> <p>Create a <code>./docs/index.md</code> file.</p> <pre><code>Hello World!\n</code></pre> </li> <li> <p>Create a <code>./docs/css/custom.css</code> file.</p> <pre><code>/* Avoid showing first header, i.e., the page title in the sidebar.\n *\n * https://github.com/mkdocs/mkdocs/issues/318#issuecomment-98520139\n */\nli.toctree-l3:first-child {\n  display: none;\n}\n\ncode {\n  font-family: Menlo, monospace;\n}\n</code></pre> </li> <li> <p>Create a <code>./requirements.txt</code> file, so that you can easily reinstall the necessary Python packages with <code>pip install -r requirements.txt</code>.</p> <pre><code>mkdocs-material\n</code></pre> </li> <li> <p>Create a <code>./.gitignore</code> file.</p> <pre><code>site/\nvenv/\n</code></pre> </li> <li> <p>Build and serve the site locally, to verify that your changes look good.  When MkDocs is up and running, you will see <code>Serving on http://127.0.0.1:8000</code>. Leave this process running. When you are done developing and testing your site, you can stop this process with <code>ctrl+c</code>. Open a new Terminal to run the second command.</p> <pre><code>mkdocs serve\nopen http://localhost:8000\n</code></pre> </li> <li> <p>Push the changes to GitHub to save your progress. The first release build will fail due to the absence of the <code>gh-pages</code> branch.</p> <pre><code>git add --all\ngit commit -m \"first commit\"\ngit push origin\n</code></pre> </li> <li> <p>Build and deploy the site for the first time. This step is done to establish the <code>gh-pages</code> branch which is used by GitHub Actions and Pages. The static HTML site generated by this process will be pushed to the <code>gh-pages</code> branch at the origin for this repository. Changes will be live within one minute. Navigate to the site url to see your changes, after you have configured the Pages source branch in Repo Settings.</p> <pre><code>mkdocs gh-deploy\n</code></pre> </li> </ol>"},{"location":"github/how-i-built-my-site/#repo-configuration","title":"Repo Configuration","text":"<p>Navigate to the repository on GitHub and set some useful configuration options.</p> <ol> <li>Code &gt; Description</li> <li>Code &gt; Website</li> <li>Code &gt; Manage topics</li> <li>Settings &gt; Branches &gt; Protected branches: <code>main</code><ol> <li>Check: Protect this branch</li> <li>Check: Include administrators</li> </ol> </li> <li>Settings &gt; Pages &gt; Source<ol> <li>Branch: <code>gh-pages</code></li> <li>Folder: <code>/ (root)</code></li> </ol> </li> </ol>"},{"location":"github/how-i-built-my-site/#new-post-workflow","title":"New Post Workflow","text":"<ol> <li> <p>Create a new branch, so you can check your work in a PR.</p> <pre><code>git checkout -b new-post\n</code></pre> </li> <li> <p>Start serving the site locally, with file change detection.</p> <pre><code>mkdocs serve\nopen http://localhost:8000\n</code></pre> </li> <li> <p>Start a new post by creating a markdown file in the <code>./docs</code> directory hierarchy.</p> </li> <li> <p>Images can be served from a location such as <code>./docs/images</code>, with references as follows:</p> <pre><code>![Link Name](/images/my-file.png \"Alt Text\")\n</code></pre> </li> <li> <p>Add the new markdown file to the <code>./mkdocs.yml</code> site configuration and continue editing. See Writing Your Docs for tips on arranging your Markdown files.</p> </li> <li> <p>When editing is complete, commit changes and push. Open a PR on the GitHub site and check the build output. Merge when you are happy with the changes and the release build will deploy the updated site.</p> <pre><code>git add --all\ngit commit -m \"my new post\"\ngit push origin\n</code></pre> </li> </ol> <p></p>"},{"location":"linux/apache-deflate-and-cors-headers/","title":"Apache Deflate and CORS Headers","text":"<p>For internal static websites, you may want to configure CORS headers with generous permissions, to improve the cross-site experience. A reasonable deflate configuration is provided which will compress the largest elements of a website.</p> <pre><code>LoadModule deflate_module modules/mod_deflate.so\n\n&lt;IfModule mod_deflate.c&gt;\n  # Compress HTML, CSS, JavaScript, Text, XML and fonts\n  AddOutputFilterByType DEFLATE application/javascript\n  AddOutputFilterByType DEFLATE application/rss+xml\n  AddOutputFilterByType DEFLATE application/vnd.ms-fontobject\n  AddOutputFilterByType DEFLATE application/x-font\n  AddOutputFilterByType DEFLATE application/x-font-opentype\n  AddOutputFilterByType DEFLATE application/x-font-otf\n  AddOutputFilterByType DEFLATE application/x-font-truetype\n  AddOutputFilterByType DEFLATE application/x-font-ttf\n  AddOutputFilterByType DEFLATE application/x-javascript\n  AddOutputFilterByType DEFLATE application/xhtml+xml\n  AddOutputFilterByType DEFLATE application/xml\n  AddOutputFilterByType DEFLATE font/opentype\n  AddOutputFilterByType DEFLATE font/otf\n  AddOutputFilterByType DEFLATE font/ttf\n  AddOutputFilterByType DEFLATE image/svg+xml\n  AddOutputFilterByType DEFLATE image/x-icon\n  AddOutputFilterByType DEFLATE text/css\n  AddOutputFilterByType DEFLATE text/html\n  AddOutputFilterByType DEFLATE text/javascript\n  AddOutputFilterByType DEFLATE text/plain\n  AddOutputFilterByType DEFLATE text/xml\n&lt;/IfModule&gt;\n\nLoadModule headers_module modules/mod_headers.so\n\n# Once a resource becomes stale, caches must not use their stale copy without successful\n# validation on the origin server.\nHeader always set Cache-Control \"public, must-revalidate, max-age=0\"\n\n# Expose the response to frontend JavaScript code, when the request's credentials mode is\n# include. Credentials are cookies, authorization headers or TLS client certificates.\nHeader always set Access-Control-Allow-Credentials true\n\n# When the Origin header is set, copy it from the request to the response.\nSetEnvIf Origin \"(.+)\" HAVE_origin=1\nRewriteCond %{HTTP:Origin} (.+)\nRewriteRule .* - [E=ORIGIN:%1]\nHeader always set Access-Control-Allow-Origin \"%{ORIGIN}e\" env=HAVE_origin\n\n# When the Access-Control-Allow-Methods header is set, replace it with GET,PATCH,POST,PUT,DELETE.\nSetEnvIf Access-Control-Request-Method \"(.+)\" HAVE_method=1\nHeader always set Access-Control-Allow-Methods \"GET,PATCH,POST,PUT,DELETE\" env=HAVE_method\n\n# When the Access-Control-Request-Headers header is set, copy it from the request to the response.\nSetEnvIf Access-Control-Request-Headers \"(.+)\" HAVE_headers=1\nRewriteCond %{HTTP:Access-Control-Request-Headers} (.+)\nRewriteRule .* - [E=HEADERS:%1]\nHeader always set Access-Control-Allow-Headers \"%{HEADERS}e\" env=HAVE_headers\n</code></pre>"},{"location":"linux/linux-ftrace-delivers-dtrace-like-functionality/","title":"Linux ftrace Delivers dtrace-like Functionality","text":"2014-07-22 Discuss <p>Brendan Gregg recently released perf-tools, which is a collection of low-level performance observability scripts akin to the DTraceToolkit.</p> <p>He has a new article posted on the tools at LWN.</p> <p></p>"},{"location":"misc/clone-stash-pull-requests/","title":"Clone Stash Pull Requests","text":"2015-11-18 Discuss <p>https://answers.atlassian.com/questions/179848/local-checkout-of-a-pull-request-in-stash</p> <pre><code>[alias]\n  prstash = \"!f() { git fetch $1 refs/pull-requests/$2/from:$3; } ; f\"\n</code></pre> <pre><code># where 3 is the 3rd pull request\ngit prstash origin 3 dest-branch\ngit checkout dest-branch\n\n# to reclone\ngit checkout master\ngit prstash origin 3 dest-branch\ngit checkout dest-branch\n</code></pre> <p></p>"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/","title":"Dual Boot OSX 10.10 and Ubuntu 14.04 on a 2013 Macbook Pro Retina","text":"2015-06-24 Discuss"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#introduction","title":"Introduction","text":"<p>Why?  Some recent benchmarks have shown that Ubuntu can out-perform OSX on Macbook hardware.</p> <ul> <li>http://www.phoronix.com/scan.php?page=article&amp;item=osx10_ubuntu1410&amp;num=1</li> <li>http://www.phoronix.com/scan.php?page=article&amp;item=ubuntu_1404_mba2013gl&amp;num=1</li> </ul> <p>This process was tested on a late 2013 Macbook11,2 A1398 model:</p> <ul> <li>2GHz Intel Core i7</li> <li>16GB 1600 MHz DDR3</li> <li>Intel Iris Pro 1536MB</li> </ul> <p>You can check your Macbook model from Ubuntu like so:</p> <pre><code>sudo dmidecode --type 1\n</code></pre>"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#issues","title":"Issues","text":"<p>Thunderbolt Monitor support is not great.  It will work if the monitor is connected to the system at boot-up, but it does not support hot-plug.</p> <p>Note</p> <p>See https://blog.jessfraz.com/post/linux-on-mac/ for some new information on this.</p> <p>Apparently, kernel 3.17 has support for hotplugging Thunderbolt connections.</p> <p>See http://ubuntuhandbook.org/index.php/2014/11/how-to-upgrade-to-linux-kernel-3-17-4-in-ubuntu-14-10/ for adding 3.17 to Ubuntu 14.</p>"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#installing","title":"Installing","text":"<ol> <li> <p>Prepare Macbook for Ubuntu installation.</p> <ol> <li> <p>Partition hard disk.</p> <ol> <li>Finder &gt; Applications &gt; Utilities &gt; Disk Utility</li> <li>Macintosh HD &gt; Partition</li> <li>Add a \"Free Space\" partition equal to half the drive.</li> <li>Apply and wait for the file system to shrink (30-60 minutes).</li> </ol> </li> <li> <p>Download a binary zip and install rEFInd Boot Manager.  This will allow you to switch between booting OSX 10.10 and Ubuntu 14.04.</p> <pre><code>unzip refind-bin-0.8.4.zip\ncd refind-bin-0.8.4\n./install.sh --alldrivers\n</code></pre> </li> <li> <p>Download Ubuntu and create a bootable USB stick.</p> <pre><code>hdiutil convert -format UDRW -o ~/path/to/target.img ~/path/to/ubuntu.iso\ndiskutil list\n# find usb disk\ndiskutil unmountDisk /dev/diskN\nsudo dd if=/path/to/downloaded.img of=/dev/rdiskN bs=1m\ndiskutil eject /dev/diskN\n</code></pre> </li> </ol> </li> <li> <p>Install Ubuntu.</p> <ol> <li>Reboot, hold down the Option key and choose Ubuntu USB stick.</li> <li>Grub Boot Menu: Install Ubuntu</li> <li>Continue with the installation, without networking.</li> <li>Installation Type &gt; Something Else</li> <li>This will allow you to create a custom partition configuration and preserve your Mac OSX install.</li> <li> <p>You should see a partition list like the following:</p> Partition Type Size /dev/sda1 efi 209 MB /dev/sda2 hfs+ 125441 MB /dev/sda3 hfs+ 650 MB free space 124699 MB </li> <li> <p>Within the free space, create two new partitions:</p> <ol> <li>Swap space, equal to the same size as system RAM.</li> <li>An Ext4 partition for the root mount point, consuming the remaining space.</li> </ol> </li> <li>Select the new root partition and continue the installation.</li> <li>Time Zone: Los Angeles</li> <li>Keyboard Layout: English (US) - English (Macintosh)</li> <li>Who Are You?  Re-use your existing username and computer name.</li> <li>Reboot and choose Ubuntu installation from rEFInd menu.</li> </ol> </li> <li> <p>Configure Ubuntu Environment.</p> <ol> <li> <p>Start a Terminal</p> <ol> <li>Ubuntu Search &gt; Terminal</li> <li>Right-click Launcher Icon &gt; Lock to Launcher</li> </ol> </li> <li> <p>Configure wireless networking.</p> <ol> <li>Insert USB stick with Ubuntu installation media.</li> <li> <p>Install dkms and bcmwl-kernel-source packages.</p> <pre><code>cd /media/`whoami`/Ubuntu\\ 14.04.1\\ LTS\\ amd64/\nsudo dpgk -i pool/main/d/dkms/dkms_2.2.0.3-1.1ubuntu5_all.deb\nsudo dpgk -i pool/restricted/b/bcmwl/bcmwl-kernel-source_6.30.223.141+bdcom-0ubuntu2_amd64.deb\n</code></pre> </li> <li> <p>Create a network manager wakeup script <code>/etc/pm/sleep.d/99_wakeup</code> and make it executable. This will restore wireless networking when resuming from suspend.  Bugs 1299282 and 1289884 are tracking this issue.</p> <pre><code>#!/bin/bash\n\ncase \"$1\" in\n  thaw|resume)\n    nmcli nm sleep false\n    ;;\nesac\n\nexit $?\n</code></pre> </li> </ol> </li> <li> <p>Install binary drivers for Intel Iris Pro Graphics 5200.</p> <ol> <li>Download the 64-bit Ubuntu installer from https://01.org/linuxgraphics/<pre><code>sudo dpkg -i intel-linux-graphics-installer_1.0.7-0intel1_amd64.deb\nsudo apt-get install -f\n</code></pre> </li> </ol> </li> <li> <p>Disable suspend when closing the lid.</p> <pre><code>sudo vi /etc/systemd/logind.conf\n\nHandleLidSwitch=ignore\n\nsudo restart systemd-logind\n</code></pre> </li> <li> <p>Configure mouse behavior.</p> <ol> <li>System Settings &gt; Mouse and Touchpad</li> <li>Check: Natural Scrolling</li> <li>Uncheck: Disable while typing</li> <li>Uncheck: Tap to click</li> </ol> </li> <li> <p>Disable turning the screen off.</p> <ol> <li>System Settings &gt; Brightness &amp; Lock</li> <li>Turn Screen Off When Inactive For: Never</li> <li>Lock Screen After: 10 minutes</li> </ol> </li> <li> <p>Add a screensaver.  The default gnome-screensaver is just a black screen.</p> <pre><code>sudo apt-get remove gnome-screensaver\nsudo apt-get install xscreensaver xscreensaver-data-extra xscreensaver-gl-extra\n</code></pre> </li> </ol> </li> <li> <p>Install Applications</p> <ol> <li> <p>Oracle Java 7 and Java 8</p> <pre><code>sudo add-apt-repository ppa:webupd8team/java\nsudo apt-get update\necho oracle-java7-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections\nsudo apt-get install oracle-java7-installer\n\necho oracle-java8-installer shared/accepted-oracle-license-v1-1 select true | sudo /usr/bin/debconf-set-selections\nsudo apt-get install oracle-java8-installer\n\n# switching versions\nsudo update-java-alternatives -s java-7-oracle\nsudo update-java-alternatives -s java-8-oracle\nsudo apt-get install oracle-java8-set-default\n</code></pre> </li> <li> <p>Google Chrome</p> <ol> <li>As of version 35, Chrome no longer supports the NPAPI plugin, which is required by the Oracle and OpenJDK Java plugins.  If you need to run a Java plugin from a web browser, use Firefox instead.</li> </ol> </li> </ol> </li> <li> <p>Test Java version.</p> <ol> <li>https://www.java.com/en/download/testjava.jsp</li> </ol> </li> <li> <p>Test WiFi speed.</p> <ol> <li>Download large files.<pre><code>curl -O http://releases.ubuntu.com/14.10/ubuntu-14.10-desktop-amd64.iso\ncurl -O http://www.wswd.net/testdownloadfiles/1GB.zip\n</code></pre> </li> </ol> </li> </ol>"},{"location":"misc/dual-boot-osx-10.10-and-ubuntu-14.04-on-a-2013-macbook-pro-retina/#uninstalling","title":"Uninstalling","text":"<ol> <li> <p>Reboot into OSX.</p> </li> <li> <p>Uninstall rEFInd.</p> <pre><code>diskutil list\ndiskutil mount disk0s1\nsudo su -\ncd /Volumes/EFI\nrm -rf BOOT ubuntu\n</code></pre> </li> <li> <p>Download GParted Live and install it on a USB stick.</p> <pre><code>hdiutil convert -format UDRW -o ~/path/to/target.img ~/path/to/gparted.iso\ndiskutil list\n# find usb disk\ndiskutil unmountDisk /dev/diskN\nsudo dd if=/path/to/downloaded.img of=/dev/rdiskN bs=1m\ndiskutil eject /dev/diskN\n</code></pre> </li> <li> <p>Reboot and hold down the option key.</p> </li> <li>Remove the Linux partitions.</li> <li> <p>Reboot into Mac OSX and extend the partition.</p> <ol> <li> <p>Covert the CoreStorage volume back into HFS+.</p> <pre><code>diskutil cs list\ndiskutil coreStorage revert $VOLUME_UUID\n</code></pre> </li> <li> <p>Reboot.</p> </li> <li>Delete the recovery partition.</li> <li>Extend the primary partition.</li> <li>Recreate the recovery partition.</li> <li> <p>Convert the HFS+ volume back to CoreStorage.</p> <pre><code>diskutil cs convert \"Macintosh HD\"\n</code></pre> </li> <li> <p>Reboot.</p> </li> </ol> </li> </ol> <p></p>"},{"location":"misc/make-sublimetext-awesome/","title":"Make SublimeText Awesome","text":"XXXX-XX-XX Discuss"},{"location":"misc/make-sublimetext-awesome/#introduction","title":"Introduction","text":"<p>I was using Atom for awhile, but it's slow (3+ second) startup times started to get to me.  Once I learned the magic of cmd+shift+p from Atom (inspired by the Sublime Text Command Palette), I went back to the original and figured out how to get it configured nicely.  One of the good things about this approach is that you end up with a reasonable Python IDE when you are done.  Also works nicely for Clojure - the REPL has good support for selecting and pasting code snippets from the editing window.  Comparing the two briefly:</p> Application Pros Cons Atom <ul><li>Faster to get started. You end up with a prettier and functional setup with less configuration effort.</li><li>Package manager is batteries included.</li><li>Markdown preview rendering in the application.</li><li>Treeview colors files that have been modified in a Git repo.</li></ul> <ul><li>Super slow (3-10 second) application load times.</li><li>Python code completion still a work in progress.</li><li>No support for treeview directory flattening.</li></ul> SublimeText <ul><li>Super fast (0-1 second) application load times.</li><li>Python Jedi code completion.</li><li>Supports directory flattening into Java package names, although a bit weird.</li><li>Python console for programmatic configuration.</li>li&gt;<li>Multi-language REPL in the application.</li></ul> <ul><li>Package Control is not part of the standard distribution.</li><li>The ST2/ST3 schism, although ST3 seems to be in better shape these days.</li><li>Markdown preview rendering forks a browser process, although it keeps track of the rendered temporary file and can reload it.</li><li>No colorized flagging in treeview for modified Git repo files.</li></ul>"},{"location":"misc/make-sublimetext-awesome/#install-and-configure","title":"Install and Configure","text":""},{"location":"misc/make-sublimetext-awesome/#install-pyenv","title":"Install pyenv","text":"<p>Warning</p> <p>When you use pyenv with Sublime Text and SublimeLinter-flake8, it will attempt to use python3 from the /usr/local/var/pyenv/shims directory, regardless of your global configuration. You must have flake8 installed with the latest version of python3 on your system in order for the plugin to work.</p> <p>Uses Homebrew; this makes it easy to switch between Python2 and Python3.</p> <pre><code>xcode-select --install\n\nbrew update\nbrew install pyenv readline\n\ncat &gt;&gt; $HOME/.bash_profile &lt;&lt;\"EOF\"\nexport PYENV_ROOT=/usr/local/var/pyenv\nif which pyenv &gt; /dev/null; then eval \"$(pyenv init -)\"; fi\nEOF\n\nsource $HOME/.bash_profile\npyenv install --list\npyenv install 2.7.10\npyenv install 3.5.0\n\npyenv versions\npyenv global 2.7.10\npip install pip --upgrade\n</code></pre>"},{"location":"misc/make-sublimetext-awesome/#install-leiningen","title":"Install leiningen","text":"<pre><code>brew install leiningen\n</code></pre>"},{"location":"misc/make-sublimetext-awesome/#install-application","title":"Install Application","text":"<p>Install Sublime Text 3.</p> <p>Create the following symlinks so you can launch SublimeText from the command line:</p> <pre><code>ln -nsf \"/Applications/Sublime Text.app/Contents/SharedSupport/bin/subl\" /usr/local/bin/subl\n\n# open current directory in sublimetext to see a treeview\nsubl .\n</code></pre>"},{"location":"misc/make-sublimetext-awesome/#install-packages","title":"Install Packages","text":"<p>Install Package Control (ctrl+`):</p> <pre><code>import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)\n</code></pre> <p>Install the following packages (cmd+shift+p: Package Control: Install Package).</p> Package Purpose BracketHighlighter Matches a variety of brackets such as: <code>[]</code>, <code>()</code>, <code>{}</code>, <code>\"\"</code>, <code>''</code>, <code>#!xml</code>, <code>&lt;tag&gt;&lt;/tag&gt;</code>, and even custom brackets. EditorConfig Standardize editor configurations for projects. Enhanced Clojure Improved code completion for Clojure. FileDiffs Shows diffs between the current file, or selection(s) in the current file, and clipboard, another file, or unsaved changes. fish-shell Syntax highlighting and whatnot for fish-shell. Git Git integration. GitGutter Show an icon in the gutter area indicating whether a line has been inserted, modified or deleted. Gradle_Language Gradle language support. Jedi - Python autocompletion Plugin to the awesome autocomplete library Jedi. lispindent Properly indents lisp code. Markdown Preview Preview and build your markdown files quickly in your web browser, supports Python Markdown Parser and Github Markdown. Origami You tell SublimeText where you want a new pane, and it makes one for you. It works seamlessly alongside the built-in layout commands. ShellCommand The ShellCommand plugin allows arbitrary shell commands to be run and their output to be sent to buffers or panels. SublimeLinter A framework for interactive code linting in the Sublime Text 3 editor. SublimeLinter-contrib-scalastyle This linter plugin for SublimeLinter provides an interface to scalastyle. It will be used with files that have the \u201cscala\u201d syntax. SublimeLinter-flake8 This linter plugin for SublimeLinter provides an interface to flake8. It will be used with files that have the \u201cPython\u201d syntax. SublimeLinter-jshint This linter plugin for SublimeLinter provides an interface to jshint. It will be used with files that have the \u201cJavaScript\u201d syntax, or within <code>&lt;script&gt;</code> tags in HTML files. SublimeLinter-shellcheck This linter plugin for SublimeLinter provides an interface to shellcheck. It will be used with files that have the \u201cShell-Unix-Generic\u201d syntax (aka Shell Script (Bash)). SublimeREPL Run an interpreter inside ST2 (Clojure, CoffeeScript, F#, Groovy, Haskell, Lua, MozRepl, NodeJS, Python + virtualenv, R, Ruby, Scala...). Theme - Soda Dark and light custom UI themes for Sublime Text. View In Browser Open whatever is in your current view/tab. If the file current open is new and has not been saved a temporary file is created (in your default temp directory for your OS) with the extension of .htm and your browser will open it. Trailing Spaces Highlights trailing spaces (and lets you delete them in a flash!)"},{"location":"misc/make-sublimetext-awesome/#install-package-helpers","title":"Install Package Helpers","text":"<p>In order for some packages to work properly, they need helper binaries to be installed separately, such as:</p> <pre><code>brew install scalastyle shellcheck\npip install flake8 jedi\nnpm install -g jshint\n</code></pre>"},{"location":"misc/make-sublimetext-awesome/#configure-sublimetext","title":"Configure SublimeText","text":"<p>There are a number of configuration tweaks that you will want to make to SublimeText to improve overall behavior.  The easiest way to manage this is to establish a dotfiles repository and track changes in source control.  See the subl directory for a collection of configuration files; the init-dots.sh script describes how these are linked to the main configuration location on your machine.</p> <p>After installing the configuration files, quit and restart SublimeText - this is particularly necessary for SublimeLinter to function correctly.</p>"},{"location":"misc/make-sublimetext-awesome/#automate-package-installation","title":"Automate Package Installation","text":"<p>You can automate the installation of Package Control packages on first startup.</p>"},{"location":"misc/make-sublimetext-awesome/#editing-tips","title":"Editing Tips","text":"<p>SublimeText has some interesting multi-editing features.</p> Shortcut Action ctrl+g Go to line number. cmd+d Select the next match of the current selection. cmd+ctrl+g Select all matches of the current selection. cmd+l Select the next line. cmd+shift+j Select all child elements within a structured (e.g. HTML) document. cmd+r Jump to function. cmd+shift+d Duplicate current line, preserving all indentation. cmd+ctrl+up Move line up. cmd+ctrl+down Move line down. cmd+k,b Hide or show the sidebar. cmd+click Create multiple cursors. cmd+p Search for file within project. cmd+alt+left Go to left tab. cmd+alt+right Go to right tab. ctrl+shift+m    Select text within brackets (expandable). <p></p>"},{"location":"misc/reset-jenkins-build-number/","title":"Reset Jenkins Build Number","text":"2017-03-14 Discuss <p>http://stackoverflow.com/questions/20901791/how-to-reset-build-number-in-jenkins</p> <p>Go to the Jenkins Script Console and enter:</p> <pre><code>item = Jenkins.instance.getItemByFullName(\"your-job-name-here\")\n\n// remove all build history\nitem.builds.each() { build -&gt;\n  build.delete()\n}\n\nitem.updateNextBuildNumber(1)\n</code></pre> <p></p>"},{"location":"misc/switching-jdks-in-osx/","title":"Switching JDKs in OSX","text":"2015-02-03 Discuss <p>Add the following to your <code>$HOME/.bash_profile</code>:</p> <pre><code>export JAVA7_HOME=\"$(/usr/libexec/java_home -v 1.7)\"\nexport JAVA8_HOME=\"$(/usr/libexec/java_home -v 1.8)\"\nexport JAVA_HOME=$JAVA8_HOME\n\nswitch_java() {\n    if echo $JAVA_HOME |grep -q 1.8; then\n        export JAVA_HOME=$JAVA7_HOME\n    else\n        export JAVA_HOME=$JAVA8_HOME\n    fi\n    echo \"JAVA_HOME=$JAVA_HOME\"\n}\n</code></pre> <p></p>"},{"location":"python/accessing-google-apis-with-python/","title":"Accessing Google APIs with Python","text":"2016-06-03 Discuss"},{"location":"python/accessing-google-apis-with-python/#links","title":"Links","text":"<ul> <li>https://developers.google.com/admin-sdk/directory/v1/quickstart/python#prerequisites</li> <li>https://developers.google.com/admin-sdk/directory/v1/reference/users/list#try-it</li> <li>http://oauth2client.readthedocs.io/en/latest/source/oauth2client.service_account.html</li> <li>https://github.com/google/oauth2client/issues/401</li> <li>https://github.com/google/oauth2client/issues/418</li> <li>https://github.com/google/oauth2client/pull/420</li> </ul>"},{"location":"python/accessing-google-apis-with-python/#method","title":"Method","text":"<pre><code>import httplib2\nimport secure_storage\nfrom apiclient.discovery import build\nfrom oauth2client.service_account import ServiceAccountCredentials\n\nclient_email = '...@developer.gserviceaccount.com'\nscopes = [\n    'https://www.googleapis.com/auth/admin.directory.user.readonly',\n    'https://www.googleapis.com/auth/admin.directory.group.readonly',\n    'https://www.googleapis.com/auth/apps.groups.settings'\n]\nuname = '...@example.com'\nfname = '/.../private_key.p12'\n\ncreds = ServiceAccountCredentials.from_p12_keyfile(client_email, fname, scopes=scopes)\ndelegated_creds = creds.create_delegated(uname)\n\nhttp_auth = delegated_creds.authorize(httplib2.Http())\nservice = build('admin', 'directory_v1', http=http_auth)\nservice.users().list(domain='example.com', maxResults=10, orderBy='email').execute()\nservice.groups().list(domain='example.com', maxResults=10).execute()\n</code></pre>"},{"location":"python/building-rest-apis-with-python-flask/","title":"Building REST APIs with Python Flask","text":"2014-07-15 Discuss <p>Miguel Grinberg, author of Flask Web Development put together an excellent blog post on Designing a RESTful API with Python and Flask.</p> <p></p>"},{"location":"python/install-the-latest-python-versions-on-macosx/","title":"Install the Latest Python Versions on Mac OSX","text":"2017-10-12 Discuss <p>Once you have decided that Python is an awesome language to learn and you have heard all about the the cool features awaiting you, you are determined to install the latest versions on your laptop so that you can start developing.</p> <ul> <li>What\u2019s New In Python 3.6 - December 23, 2016</li> <li>What\u2019s New In Python 3.5 - September 13, 2015</li> <li>What\u2019s New In Python 3.4 - March 16, 2014</li> <li>What\u2019s New In Python 3.3 - September 29, 2012</li> <li>What\u2019s New In Python 3.2 - February 20, 2011</li> <li>What\u2019s New In Python 3.1 - June 27, 2009</li> <li>What\u2019s New In Python 3.0 - December 3, 2008</li> <li>What\u2019s New In Python 2.7 - July 3, 2010</li> <li>What\u2019s New In Python 2.6 - October 1, 2008</li> </ul> <p>You may be a little concerned about the differences between Python 2 and Python 3, so you want to make sure that you have the latest versions of each available. This will enable you to switch between them easily and work on porting Python 2 programs to Python 3.</p> <ul> <li>Should I use Python 2 or Python 3 for my development activity?</li> <li>Key Differences between Python 2.7 and Python 3 with Examples</li> <li>Porting Python 2 Code to Python 3</li> <li>Writing code that runs under both Python2 and 3</li> </ul> <p>OS vendor Python versions lag behind the latest available and can only be updated by installing a major patch or bolting-on an additional Python installation:</p> <p> OS Version System Python Version      OSX 10.13 2.7.10      OSX 10.12 2.7.10      OSX 10.11 2.7.10      Ubuntu 16.04 LTS Xenial Xerus 2.7.11 Ubuntu 14.04 LTS Trusty Tahr 2.7.5 Ubuntu 12.04 LTS Precise Pangolin 2.7.3 <p>One way to work around this issue is to use <code>pyenv</code> to install the versions of Python you want to use.  This tool provides a simplified build environment for installing the Pythons you want, while providing a set of shell script shims that makes it easy to switch between them.  This tool was inspired by and forked from <code>rbenv</code> and <code>ruby-build</code>.</p>"},{"location":"python/install-the-latest-python-versions-on-macosx/#install-python-with-pyenv","title":"Install Python with pyenv","text":"<ol> <li> <p>Start a Terminal session.</p> </li> <li> <p>Install Homebrew and Xcode Command Line Tools.</p> <pre><code>xcode-select --install\n\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n</code></pre> </li> <li> <p>Install <code>pyenv</code>.</p> <pre><code>brew install pyenv\n</code></pre> </li> <li> <p>If you already have <code>pyenv</code> installed, you can upgrade it to gain access to the latest Python versions.  The <code>pyenv</code> tool is updated periodically with new formulas for the latest releases.</p> <pre><code>brew upgrade pyenv\n</code></pre> </li> <li> <p>Add configuration to your <code>.bash_profile</code> to initialize <code>pyenv</code> every time you start a new Terminal. The <code>PYENV_ROOT</code>, which is where Python versions and packages will be installed, defaults to <code>$HOME/.pyenv</code>.</p> <pre><code>cat &gt;&gt;$HOME/.bash_profile &lt;&lt;\"EOF\"\nif which pyenv &gt; /dev/null 2&amp;&gt;1; then eval \"$(pyenv init -)\"; fi\nEOF\n</code></pre> </li> <li> <p>Load your <code>.bash_profile</code> configuration changes.</p> <pre><code>source $HOME/.bash_profile\n</code></pre> </li> <li> <p>List the available Python versions.  Notice that you have access to several different distributions: python.org (plain version numbers), anaconda, ironpython, jython, miniconda, pypy and stackless.  We will install the standard Python versions released by python.org, otherwise known as CPython, because the interpreter is written in C.</p> <pre><code>pyenv install --list\n</code></pre> </li> <li> <p>If you are running OSX 10.13, you will need to set the following environment variables, when you install new Python versions. See #988 for more details.</p> <pre><code>export CFLAGS=\"-I$(brew --prefix openssl)/include\"\nexport LDFLAGS=\"-L$(brew --prefix openssl)/lib\"\n</code></pre> </li> <li> <p>Install Python versions.</p> <pre><code>pyenv install 2.7.14\npyenv install 3.6.3\n</code></pre> </li> <li> <p>List the available Python versions.</p> <pre><code>pyenv versions\n</code></pre> </li> <li> <p>Activate a Python version and verify that it is available.</p> <pre><code>pyenv global 2.7.14\npyenv versions\npython -V\n</code></pre> </li> <li> <p>Activate another Python version and verify that it is available.</p> <pre><code>pyenv global 3.6.3\npyenv versions\npython -V\n</code></pre> </li> <li> <p>If desired, activate multiple Python versions and verify that they are available. PEP 394 -- The \"python\" Command on Unix-Like Systems explains conventions for naming <code>python</code> binaries.</p> <pre><code>pyenv global 2.7.14 3.6.3\npyenv versions\npython -V\npython2 -V\npython2.7 -V\npython3 -V\npython3.6 -V\n</code></pre> </li> <li> <p>Create new directories for Python projects, add <code>pyenv local</code> version files and verify the Python versions.  Your <code>python</code> version will automatically switch when you change into these directories.</p> <pre><code>mkdir python2-project\ncd python2-project\npyenv local 2.7.13\ncat .python-version\npyenv local\npython -V\ncd ..\n\nmkdir python3-project\ncd python3-project\npyenv local 3.6.3\ncat .python-version\npyenv local\npython -V\ncd ..\n</code></pre> </li> </ol>"},{"location":"python/install-the-latest-python-versions-on-macosx/#useful-python-packages","title":"Useful Python Packages","text":"<p>This is a small collecton of useful packages that will help get you started doing useful things with Python.</p> <p>Install flake8, which is a static analyzer that enforces good Python coding style and alerts you to coding mistakes.  Now that you have Python installed, you can use the <code>pip</code> command to install any additional modules that you want to use.  You will need to install modules separately for each version of Python you are actively using.</p> <pre><code>pip install flake8\n</code></pre> <p>Install advanced Python Read-Eval-Print-Loop (REPL) packages, which will make it easier to explore Python code, because they grant access to tab completion and fancy editing capabilities.  The <code>ipython</code> tool is a command line REPL, which can be exited with <code>ctrl-d</code>.  Jupyter Notebook is a browser-based REPL that operates on individual cells of code.  The IPython Interactive Computing website has more information on these tools.</p> <pre><code>pip install ipython jupyter\nipython\njupyter notebook\n</code></pre> <p>Install Requests: HTTP for Humans, which makes it easy to consume HTTP services. See GitHub's REST API v3 documentation for more details on endpoints that are avaialble.</p> <pre><code>pip install requests\n</code></pre> <pre><code>python\n&gt;&gt;&gt; import requests\n&gt;&gt;&gt; r = requests.get('https://api.github.com/users/copperlight')\n&gt;&gt;&gt; r.ok\nTrue\n&gt;&gt;&gt; r.status_code\n200\n&gt;&gt;&gt; r.headers['content-type']\n'application/json; charset=utf8'\n&gt;&gt;&gt; r.encoding\n'utf-8'\n&gt;&gt;&gt; r.text\nu'{\"login\":\"copperlight\", ...}'\n&gt;&gt;&gt; r.json()\n{u'public_repos': 27, ...}\n&gt;&gt;&gt; r.json()['login']\nu'copperlight'\n</code></pre> <p>Install Flask: A Python Microframework, which can be used to quickly build small websites that automate everyday tasks.</p> <pre><code>pip install Flask\n</code></pre> <pre><code>from flask import Flask\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"Hello World!\"\n</code></pre> <pre><code>FLASK_APP=hello.py flask run\ncurl http://127.0.0.1:5000\n</code></pre> <p>If you need Python package isolation on a per-project basis, because you have conflicting sets of packages specified for different projects, you can use virtualenv to set up separate environments.  The <code>virtualenv</code> tool establishes a clean room Python installation, either based upon the current version of Python in use, or a version specified on the command line. If you have multiple Python versions activated in <code>pyenv</code>, you can use <code>virtualenv</code> to switch between them easily.</p> <pre><code>pip install virtualenv\npip list\n\nmkdir python2-project\ncd python2-project\nvirtualenv -p python2.7 venv\nsource venv/bin/activate\npython -V\nwhich python\npip list\nwhich pip\ndeactivate\ncd ..\n\nmkdir python3-project\ncd python3-project\nvirtualenv -p python3.6 venv\nsource venv/bin/activate\npython -V\nwhich python\npip list\nwhich pip\ndeactivate\ncd ..\n</code></pre>"},{"location":"python/install-the-latest-python-versions-on-macosx/#automated-testing-with-python","title":"Automated Testing with Python","text":"<p>While working on Python programs, you may be interested in automating testing.  The most common choice for implementing this is tox, which will allow you to run tests with multiple different versions of Python, if needed.</p>"},{"location":"python/install-the-latest-python-versions-on-macosx/#python-packaging","title":"Python Packaging","text":"<p>After completing a Python program or two, you will be interested in learning how to distribute them effectively.  The Python Packaging User Guide has some good advice on this topic.</p>"},{"location":"python/install-the-latest-python-versions-on-macosx/#quickstart-commands","title":"Quickstart Commands","text":"<pre><code># if you need homebrew\nxcode-select --install\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n\n# install pyenv and latest major python versions\nbrew install pyenv\n\ncat &gt;&gt;$HOME/.bash_profile &lt;&lt;\"EOF\"\nif which pyenv &gt; /dev/null; then eval \"$(pyenv init -)\"; fi\nEOF\n\nsource $HOME/.bash_profile\n\npyenv install 2.7.14\npyenv install 3.6.3\npyenv versions\n</code></pre>"},{"location":"python/memory-profiling-with-pyrasite-and-heapy/","title":"Memory Profiling with Pyrasite and Heapy","text":"2017-09-17 Discuss <ol> <li> <p>Build or install Pyrasite from the develop branch in the GitHub repo.  The PyPi package does not have the latest code, which allows you to control the IPC timeout.</p> <ol> <li>https://github.com/lmacken/pyrasite</li> </ol> </li> <li> <p>Install profiling tools.</p> <pre><code>sudo apt-get update\nsudo apt-get install pyrasite guppy\n</code></pre> </li> <li> <p>Enable ptrace, so that you can inject into the process.</p> <pre><code>echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n</code></pre> </li> <li> <p>Connect to the running process.</p> <pre><code>export PYRASITE_IPC_TIMEOUT=10  # default is 5 seconds\n\n/apps/python/bin/pyrasite-shell 3640\nPyrasite Shell 2.0\nConnected to '...'\nPython 2.7.12 (default, Nov 19 2016, 06:48:10)\n[GCC 5.4.0 20160609] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(DistantInteractiveConsole)\n\n&gt;&gt;&gt;\n</code></pre> </li> <li> <p>Enumerate the threads in the current process.</p> <pre><code>import sys, traceback\n\nfor thread, frame in sys._current_frames().items():\n    print('Thread 0x%x' % thread)\n    traceback.print_stack(frame)\n    print()\n</code></pre> </li> <li> <p>Profile process memory with heapy.</p> <pre><code>from guppy import hpy\nhp = hpy()\nhp.heap()\n</code></pre> </li> <li> <p>Debug the pyrasite process, if needed.</p> <pre><code>/apps/python/bin/pyrasite --verbose &lt;pid&gt; helloworld.py\n</code></pre> </li> </ol> <p></p>"},{"location":"python/stop-using-print-for-debugging/","title":"Stop Using \"print\" for Debugging","text":"2017-10-12 Discuss <p>My favorite quickstart guide to the Python logging module, by Al Sweigart:</p> <p>Stop Using \"print\" for Debugging: A 5 Minute Quickstart Guide to Python\u2019s logging Module</p> <p>This is a slight tweak to that pattern, which adds the name of the logging source and an example of disabling selected log sources.</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogging.getLogger('urllib3').setLevel(logging.CRITICAL)\nlogging.debug('This is a log message.')\n</code></pre> <p></p>"},{"location":"recipes/galaxys-best-margarita/","title":"Galaxy's Best Margarita","text":"<p>I don't recall where this came from, but the recipe is amazing and the original site is no longer on the Internet.</p> <p>Not a difficult one, but formulated during the great Margarita consumption in Phoenix AZ during the late 80's.</p>"},{"location":"recipes/galaxys-best-margarita/#important-stuff","title":"Important Stuff","text":"<p>Put these ingredients in the blender. Mix the ingredients for a short period, but do not \"blend\". A pulsating action with four or five jolts of the blender works the best. This recipe makes 3 and a half 12 ounce margaritas.</p> <ul> <li>Tequila - one cup (8 ounces)</li> <li>Triple Sec - a quarter cup</li> <li>Limeade Frozen Concentrate - half can (6 ounces)</li> <li>Lemon Juice - one fresh lemon</li> <li>Ice - two cups large cubes</li> <li>Water - a half cup</li> </ul> <p>Toss 4 ice cubes into a 12 ounce, salt rimmed glass, and then pour the margarita!</p>"},{"location":"recipes/galaxys-best-margarita/#details","title":"Details","text":""},{"location":"recipes/galaxys-best-margarita/#water","title":"Water","text":"<p>Adjust the amount of water to alter the strength of the margarita mix. Do not adjust the alcohol measurements. If they are too strong, use one cup of water in the next batch instead of a half cup.</p>"},{"location":"recipes/galaxys-best-margarita/#ice","title":"Ice","text":"<p>The most important thing is - DO NOT CRUSH the ice. Use large cubes in the blender. They will lose some size in the mixing process. Do not allow the ice to become a puree. Many people do not like their margaritas with crushed ice because it will freeze the roof of your mouth and can cause a headache.</p>"},{"location":"recipes/galaxys-best-margarita/#lemon-juice","title":"Lemon Juice","text":"<p>Fresh lemon juice is a must. The bottled crap should be used only as a last resort, and only if the local grocery store is closed or more than 5 miles away. Make sure when you are juicing the lemon that no seeds get into the mix.</p>"},{"location":"recipes/galaxys-best-margarita/#lime-juice-concentrate","title":"Lime Juice Concentrate","text":"<p>Minute Maid is a common brand but the only problem with Minute Maid is that there is too much pulp. I used to not recommend this brand but I am changing my tune. I have had quality problems with the lower-priced brands. Go with the Minute Maid.</p>"},{"location":"recipes/galaxys-best-margarita/#tequila","title":"Tequila","text":"<p>Any tequila will work in this recipe. Most people think they need the most expensive tequila. Not so with this mix. Cheap white tequila works great. My favorite is Sauza Giro Gold tequila because it is smooth and the price is low (about $9 US for 750 ml). I do not recommend Cuervo Gold because it is overpriced (about $14) and has a rough flavor. It still turns out OK in the margarita mix, but the Sauza is cheaper and better.</p>"},{"location":"recipes/galaxys-best-margarita/#triple-sec","title":"Triple Sec","text":"<p>Many triple sec brands have an orange flavor. I believe the best margarita is made with \"neutral\" triple sec. Try to use triple sec like Chateaux which is plain, and about $7 for 750 ml. It's a little more potent than the orange stuff. The problem with the orange triple sec is that it makes a margarita that is a little on the sweet side, and mixing orange flavor with lemon and lime just doesn't sound right to me.</p>"},{"location":"recipes/galaxys-best-margarita/#salt-rim","title":"Salt Rim","text":"<p>To create a salt rimmed glass, take one of the lemon rinds from above and rub the drinking surface of the glass so it is barely moist. Dip the edge of the glass into salt. Table salt will work, but larger size margarita (or kosher) salt is better. Garnish with a lime wedge, which is merely a decoration, although some folks like to squeeze the lime juice into their drink as it goes down the hatch.</p> <p>Some people do not like salt on the glass. Although 8 out of 10 like the salt, I have found that women are generally the ones who don't. You will make points with these folks if you can remember their preference in advance.</p> <p>This recipe is featured on the Bacardi web site and averages 4000 quality hits per month! I am a party-er and enjoy drinking, and sharing that experience with fellow party-ers like you. Let's Party!</p>"},{"location":"recipes/galaxys-best-margarita/#a-few-editorial-comments","title":"A Few Editorial Comments","text":"<p>Some people think that you should use the best (and most expensive) liquor. This recipe is very inexpensive to make, but if you want to splurge for the better tequila, go for it! Expensive liquor makes the margarita only marginally better because of the neutralizing effect of the other ingredients. Since a margarita is a blend of several flavors, changing the tequila has limited affect.</p> <p>I realize it is presumptuous to call this recipe \"The Galaxy's Best.\" But I registered the name in Yahoo without much thought two years ago, and there it stays. It really should be named \"A Good Marg!\" - I'm sure everyone could agree to that.</p>"},{"location":"recipes/galaxys-best-margarita/#about-dreaded-hangovers","title":"About Dreaded Hangovers","text":"<p>There are two types: first there is the \"in-a-daze mild headache\" kind. This hangover is solved by drinking orange juice on an empty stomach in the morning. I am serious dammit - make sure you use real orange juice. Sixteen ounces will do. The bad kind of hangover cannot be cured - you know - barfing etc.</p>"},{"location":"running/garmin-935-navigation/","title":"Garmin 935 Navigation","text":""},{"location":"running/garmin-935-navigation/#introduction","title":"Introduction","text":"<p>Out in the SF Bay Area, there are tons of trails which are fantastic for running. The Trailstompers site is an excellent guide, covering six distinct regions split into four difficulty levels. Descriptions of the trails, with some turn navigation advice and GPX tracks are provided for each of the trails. The tracks can be imported into Garmin watches and you can use the Navigation feature to help stay on course. There are two issues with using these tracks directly on the watch:</p> <ul> <li>When loading a GPX track for navigation, it takes awhile to process the course on the watch and it ends up having far too many points, throwing out many thousands of them.</li> <li>Garmin will make some guesses about what turn-by-turn navigation hints to display, but the ones that you get end up being either obvious or they do not occur when you need them.</li> </ul> <p>This page is a guide to a track processing process which allows you to add custom turn-by-turn waypoints to tracks and post-processes them so that they appear correctly on your Garmin watch.</p> <p>The following sites and tools are used:</p> <ul> <li>Trailstompers</li> <li>GPSies</li> <li>CourseTCX2Fit conversion tool from @CTCX2Fit</li> </ul>"},{"location":"running/garmin-935-navigation/#process","title":"Process","text":"<ul> <li>Find a trail you like on Trailstompers and download the GPX track.</li> <li>Upload the GPX track to GPSies.</li> <li>Edit the track and then modify the track. The course title will display as-is on the Garmin.</li> <li>You can add a cue sheet, which will populate turn-by-turn hints on the track.<ul> <li>The cue sheet feature is capable of adding waypoints to the course points.</li> </ul> </li> <li>Add or delete waypoints, as desired. It is best to keep waypoints for forks in the trail.<ul> <li>Waypoints cannot be manually added to course points - they must be placed alongside course points.</li> </ul> </li> <li>Download the track and waypoints as a Garmin Course TCX file.</li> <li>Run CourseTCX2FIT and convert the TCX file to a FIT file.</li> <li>Connect to your Garmin via USB.</li> <li>Access the Garmin storage device and copy the FIT file to the GARMIN/COURSES directory. Eject the Garmin device.</li> <li>On your Garmin, load the course.<ul> <li>Start &gt; Select: Trail Run &gt; Start</li> <li>Hold Up &gt; Select: Navigation</li> <li>Select: Courses &gt; Select: Named Course &gt; Start</li> <li>Do Course &gt; Start</li> </ul> </li> <li>The Garmin has 15 characters to display course titles, so choose wisely.</li> </ul>"},{"location":"scala/intellij-configuration-tips/","title":"IntelliJ Configuration Tips","text":"2017-10-05 Discuss <p>This page provides a few tips on configuring IntelliJ for working with Scala code.</p>"},{"location":"scala/intellij-configuration-tips/#sbt-projects","title":"sbt Projects","text":"<p>Sometimes, IntelliJ struggles to import sbt projects - the build phase will last forever (20 min is not uncommon), when it should complete in 3-4 minutes. The solution is to switch the sbt build process to use sbt shell.</p> <ul> <li>IntelliJ IDEA &gt; Preferences</li> <li>Build, Execution, Deployment &gt; Build Tools &gt; sbt</li> <li>sbt projects &gt; sbt shell &gt; use for: project reload, builds</li> </ul>"},{"location":"scala/intellij-configuration-tips/#optimize-imports","title":"Optimize Imports","text":"<p>The preference for organizing imports is to keep them on a single line per import, rather than bunching them together with curly braces.  You can enforce this behavior with IntelliJ by modifying the following configuration:</p> <ul> <li>IntelliJ IDEA &gt; Preferences</li> <li>Code Style &gt; Scala &gt; Imports</li> <li>Uncheck: Collect imports with the same prefix into one import</li> </ul> <p>When you run Code &gt; Optimize Imports on a source file, it will unbundle imports and leave them with a single one per line.</p>"},{"location":"scala/intellij-configuration-tips/#code-style","title":"Code Style","text":"<p>Navigate to these configuration options as follows:</p> <ul> <li>IntelliJ IDEA &gt; Preferences</li> <li>Editor &gt; Code Style &gt; Scala</li> </ul>"},{"location":"scala/intellij-configuration-tips/#method-declaration-parameters","title":"Method declaration parameters","text":"<ul> <li>Uncheck: Align when multiline</li> </ul> <p>This prevents Intellij from indenting parameter lists when cutting and pasting code within a file.</p>"},{"location":"scala/intellij-configuration-tips/#method-signature-warnings","title":"Method Signature Warnings","text":"<p>Navigate to these configuration options as follows:</p> <ul> <li>IntelliJ IDEA &gt; Preferences</li> <li>Editor &gt; Inspections &gt; Scala &gt; Method Signature</li> </ul>"},{"location":"scala/intellij-configuration-tips/#method-with-accessor-like-name-has-unit-result-type","title":"Method with accessor-like name has Unit result type","text":"<p>The intent of this warning is to cover the following case:</p> <p>Methods that follow JavaBean naming contract for accessors are expected to have no side effects. However, methods with a result type of Unit are only executed for their side effects.</p> <p>Refer to Programming in Scala, 2.3 Define some functions</p> <p>This indicates poor naming of the method involved, but depending on the code you are working with, there may be multiple examples of this present.  You may want to set this to be a weak warning until you can update method names to be more descriptive.</p>"},{"location":"scala/intellij-configuration-tips/#method-with-unit-result-type-is-defined-like-procedure","title":"Method with Unit result type is defined like procedure","text":"<p>This inspection is disabled by default.  It should be enabled.</p> <p>It is not recommended to use procedure-like syntax for methods with Unit return type. It is inconsistent, may lead to errors and will be deprecated in future versions of Scala.</p> <p>Reference: The talk \"Scala with Style\" by Martin Odersky at ScalaDays 2013</p> <p>Hint: You can use Analyze / Run Inspection by Name (Ctrl+Alt+Shift+I) to apply this inspection to the whole project</p>"},{"location":"scala/intellij-configuration-tips/#scala-worksheet","title":"Scala Worksheet","text":"<p>This is a good way to gain access to an interactive repl-style experience for evaluating Scala code.  As long as the worksheet file is established within your <code>main</code> directory, you will have access to import all the dependencies of your project.</p> <p>However, the default configuration of the Scala Worksheet will not be able to find Akka actor resources (SCL-9229). You will receive the following error:</p> <pre><code>com.typesafe.config.ConfigException$Missing:\n  No configuration setting found for key 'akka'\n</code></pre> <p>The work-around for this issue is as follows:</p> <ul> <li>IntelliJ IDEA &gt; Preferences</li> <li>Languages &amp; Frameworks &gt; Scala &gt; Worksheet</li> <li>Uncheck: Run worksheet in the compiler process</li> </ul>"},{"location":"scala/intellij-configuration-tips/#zero-latency-typing","title":"Zero-Latency Typing","text":"<p>See Experimental Zero-latency Typing in IntelliJ IDEA 15 EAP for an explanation of this feature. To implement it, follow these steps:</p> <ul> <li>Help &gt; Edit Custom Properties...</li> <li>Add <code>editor.zero.latency.typing=true</code></li> <li>Restart IntelliJ</li> </ul> <p>At some point, this will become the default.</p> <p></p>"},{"location":"scala/jmh-testing-with-scala/","title":"JMH Testing with Scala","text":"2018-02-22 Discuss <p>JMH is the Java Microbenchmark Harness provided by the OpenJDK project.  It can be a useful tool for validating assumptions about the performance of small code sections subject to many iterations.</p> <p>See the following links for more details:</p> <ul> <li>OpenJDK Code Tools: jmh</li> <li>JMH Tutorial</li> </ul>"},{"location":"scala/jmh-testing-with-scala/#gradle","title":"Gradle","text":"<p>A Gradle plugin is available for JMH.</p> <p>Project layout:</p> <pre><code>.\n\u251c\u2500\u2500 build.gradle\n\u251c\u2500\u2500 gradle\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 wrapper\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 gradle-wrapper.jar\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 gradle-wrapper.properties\n\u251c\u2500\u2500 gradlew\n\u251c\u2500\u2500 gradlew.bat\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 jmh\n        \u2514\u2500\u2500 scala\n            \u2514\u2500\u2500 io\n                \u2514\u2500\u2500 github\n                    \u2514\u2500\u2500 brharrington\n                        \u251c\u2500\u2500 AtomicLongUpdate.scala\n                        \u251c\u2500\u2500 CurrentTimeMillis.scala\n                        \u251c\u2500\u2500 NanoTime.scala\n                        \u2514\u2500\u2500 TimeLimiting.scala\n</code></pre> <p>The <code>build.gradle</code> configuration:</p> <pre><code>plugins {\n    id 'scala'\n    id 'idea'\n    id 'me.champeau.gradle.jmh' version '0.4.5'\n}\n\nrepositories {\n    jcenter()\n}\n\ndependencies {\n    compile 'com.netflix.servo:servo-core:0.12.17'\n    compile 'org.scala-lang:scala-library:2.12.4'\n}\n\nidea {\n    module {\n        testSourceDirs += sourceSets.jmh.scala.srcDirs\n    }\n}\n</code></pre> <p>Run the tests:</p> <pre><code>./gradlew jmh\n</code></pre>"},{"location":"scala/jmh-testing-with-scala/#sbt","title":"sbt","text":"<p>An sbt plugin is available for JMH.</p> <p>Example project layout:</p> <pre><code>.\n\u251c\u2500\u2500 build.sbt\n\u251c\u2500\u2500 project\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 build.properties\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 plugins.sbt\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 sbt\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sbt-launch-1.0.0.jar\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 main\n        \u2514\u2500\u2500 scala\n            \u2514\u2500\u2500 io\n                \u2514\u2500\u2500 github\n                    \u2514\u2500\u2500 brharrington\n                        \u251c\u2500\u2500 AtomicLongUpdate.scala\n                        \u251c\u2500\u2500 CurrentTimeMillis.scala\n                        \u251c\u2500\u2500 NanoTime.scala\n                        \u2514\u2500\u2500 TimeLimiting.scala\n</code></pre> <p>The <code>build.sbt</code> configuration:</p> <pre><code>name := \"misc-jmh\"\n\norganization := \"io.github.brharrington\"\n\nscalaVersion := \"2.12.4\"\n\nenablePlugins(JmhPlugin)\n\nlibraryDependencies ++= Seq(\n    \"com.netflix.servo\" % \"servo-core\" % \"0.12.17\"\n)\n</code></pre> <p>The <code>plugins.sbt</code> configuration:</p> <pre><code>addSbtPlugin(\"pl.project13.scala\" % \"sbt-jmh\" % \"0.2.27\")\n</code></pre> <p>Run the tests:</p> <pre><code>./project/sbt jmh:run\n</code></pre>"},{"location":"scala/jmh-testing-with-scala/#example-test-class","title":"Example Test Class","text":"<pre><code>package io.github.brharrington\n\nimport java.util.concurrent.atomic.AtomicLong\n\nimport org.openjdk.jmh.annotations.Benchmark\nimport org.openjdk.jmh.annotations.Scope\nimport org.openjdk.jmh.annotations.State\nimport org.openjdk.jmh.infra.Blackhole\n\n@State(Scope.Thread)\nclass AtomicLongUpdate {\n\n  private val value = new AtomicLong()\n\n  @Benchmark\n  def incrementAndGet(bh: Blackhole): Unit = {\n    bh.consume(value.incrementAndGet())\n  }\n\n  @Benchmark\n  def addAndGet(bh: Blackhole): Unit = {\n    bh.consume(value.addAndGet(42))\n  }\n\n  @Benchmark\n  def get(bh: Blackhole): Unit = {\n    bh.consume(value.get())\n  }\n\n}\n</code></pre>"},{"location":"scala/overview-of-debugging-tools/","title":"Overview of Debugging Tools","text":"2016-10-13 Discuss <p> <p> Tool Use Cases      Java Flight Recorder <ul> <li>first choice for GC allocation pauses and heap pressure             <li>first choice for thread contention      Java Flame Graphs <ul> <li>first choice for CPU contention      VisualVM <ul> <li>second choice for CPU contention      YourKit Profiler <ul> <li>third choice for CPU contention, other options are more accessible             <li>not as good as flight recorder for heap pressure      Java Micro Harness (JMH) <ul> <li>first choice for writing tiny benchmarks to evaluate code samples  <p></p>"},{"location":"scala/scalafmt-configuration-tips/","title":"Scalafmt Configuration Tips","text":"2018-02-22 Discuss <p>Scalafmt is a code formatter for Scala.  It is available as an IntelliJ plugin and a CLI version can be installed using Homebrew.</p> <pre><code>brew install --HEAD olafurpg/scalafmt/scalafmt\n</code></pre>"},{"location":"scala/scalafmt-configuration-tips/#configuration","title":"Configuration","text":"<p>The following <code>.scalafmt.conf</code> configuration is recommended for use with Scala projects. It should be created in the project's root directory.</p> <pre><code>style = defaultWithAlign\n\nalign.openParenCallSite = false\nalign.openParenDefnSite = false\nalign.tokens = [{code = \"-&gt;\"}, {code = \"&lt;-\"}, {code = \"=&gt;\", owner = \"Case\"}]\ncontinuationIndent.callSite = 2\ncontinuationIndent.defnSite = 2\ndanglingParentheses = true\nindentOperator = spray\nmaxColumn = 100\nnewlines.alwaysBeforeTopLevelStatements = true\nproject.excludeFilters = [\".*\\\\.sbt\"]\nrewrite.rules = [RedundantParens, SortImports]\nspaces.inImportCurlyBraces = false\nunindentTopLevelOperators = true\n</code></pre> <p>The <code>maxColumn</code> value is chosen so that it is easy to do side-by-side views of code and tests, while also avoiding wrapping in diffs on GitHub and Stash.</p> <p>Alignment is allowed for case statements <code>=&gt;</code> and map assignments <code>-&gt;</code> and it is disallowed for assignment <code>=</code>.  This option taken together with the <code>openParen.*Site</code> configuration minimizes the amount of whitespace fiddling that will occur in code changes, which should lead to more readable diffs.</p>"},{"location":"scala/scalafmt-configuration-tips/#usage-tips","title":"Usage Tips","text":"<ul> <li>Blank lines separate alignment blocks.</li> <li>For sections of code that require formatting which does not follow the scalafmt conventions, use format:off blocks to disable the formatter.</li> <li>When formatter changes include altering whitespace to align operators, it can make diffs more difficult to read.  You can use <code>git diff -w</code> to ignore whitespace changes.</li> <li>To enable formatting on file save in IntelliJ: IntelliJ IDEA &gt; Preferences &gt; Tools &gt; Scalafmt &gt; check: Format on file save.</li> </ul>"},{"location":"scala/scalafmt-configuration-tips/#enforce-formatting","title":"Enforce Formatting","text":""},{"location":"scala/scalafmt-configuration-tips/#gradle","title":"Gradle","text":"<p>A Scalafmt Gradle plugin is available, which can be used to enforce scalafmt conventions for your project.  It expects the configuration file to be located in the root of your project.</p> <p>Add the following to your <code>build.gradle</code> file:</p> <pre><code>buildscript {\n    dependencies {\n        classpath 'cz.alenkacz:gradle-scalafmt:1.6.0'\n    }\n}\n\napply plugin: 'scalafmt'\n</code></pre> <p>This adds a task <code>checkScalafmtAll</code> to your project, which can be used to verify that your project files adhere to the established configuration.</p>"},{"location":"scala/scalafmt-configuration-tips/#sbt","title":"Sbt","text":"<p>A Scalafmt sbt plugin is available, which can be used to enforce scalafmt conventions for your project.  It expects the configuration file to be located in the root of your project.</p> <pre><code>scalafmt::test test:scalafmt::test\n</code></pre> <p></p>"},{"location":"scala/starting-scala-repls-with-gradle-and-sbt/","title":"Starting Scala REPLs with Gradle and SBT","text":"2015-07-04 Discuss <p>These examples are tailored for usage with the Atomic Scala exercises, but they demonstrate the principles.</p> <p>build.gradle <pre><code>apply plugin: 'scala'\n\nrepositories {\n    mavenCentral()\n}\n\next {\n    versions = [\n        commons_math3: '3.5',\n        jline: '2.12.1',\n        scala: '2.11.6'\n    ]\n}\n\ndependencies {\n    compile \"org.apache.commons:commons-math3:${versions.commons_math3}\"\n    compile \"org.scala-lang:scala-library:${versions.scala}\"\n\n    runtime \"jline:jline:${versions.jline}\"\n    runtime \"org.scala-lang:scala-compiler:${versions.scala}\"\n}\n\nsourceSets {\n    main {\n        scala {\n            srcDirs = ['.']\n            include 'AtomicTest.scala'\n        }\n    }\n}\n\nscalaConsole.dependsOn(build)\nscalaConsole.classpath += sourceSets.main.runtimeClasspath\n\n// usage: gradlew scalaConsole -q\n</code></pre></p> <p>build.sbt <pre><code>scalaVersion := \"2.11.6\"\n\nsources in Compile &lt;&lt;= (sources in Compile).map(_ filter(_.name == \"AtomicTest.scala\"))\n\nlibraryDependencies += \"org.apache.commons\" % \"commons-math3\" % \"3.5\"\n\ninitialCommands in console := \"\"\"\n    |import org.apache.commons.math3._\n    |import com.atomicscala.AtomicTest._\n    |\"\"\".stripMargin\n\n// usage: sbt console\n</code></pre></p> <p></p>"},{"location":"scala/testing-aws-clients-with-iam-assumerole-credentials-in-scala/","title":"Testing AWS Clients with IAM AssumeRole Credentials in Scala","text":"2020-01-06 Discuss <p>This technique is meant to be used with IntelliJ Scala worksheets or similar scratch code, so you can test clients and validate their behavior. Don't check-in secrets \u2013 use on-instance credentials with the <code>DefaultAWSCredentialsProviderChain</code> in production code.</p> <pre><code>import com.amazonaws.auth.AWSStaticCredentialsProvider\nimport com.amazonaws.auth.BasicSessionCredentials\nimport com.amazonaws.regions.Regions\nimport com.amazonaws.services.autoscaling.AmazonAutoScalingClientBuilder\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder\nimport com.amazonaws.services.securitytoken.model.AssumeRoleRequest\n\n// config values\n\nval accessKeyId = \"\"\nval secretAccessKey = \"\"\nval token = \"\"\n\nval role: Option[String] = None\nval accountId = \"\"\nval region = Regions.US_WEST_1\n\n// client configuration\n\nval staticProvider = {\n  role.fold {\n    val basic = new BasicSessionCredentials(accessKeyId, secretAccessKey, token)\n    new AWSStaticCredentialsProvider(basic)\n  } { role =&gt;\n    val instanceProvider = {\n      val basic = new BasicSessionCredentials(accessKeyId, secretAccessKey, token)\n      new AWSStaticCredentialsProvider(basic)\n    }\n\n    val stsClient = AWSSecurityTokenServiceClientBuilder\n      .standard()\n      .withCredentials(instanceProvider)\n      .withRegion(region)\n      .build()\n\n    val req = new AssumeRoleRequest()\n      .withRoleSessionName(s\"$role-testing\")\n      .withRoleArn(s\"arn:aws:iam::$accountId:role/$role\")\n\n    val assumedCreds = stsClient.assumeRole(req).getCredentials\n\n    val basic = new BasicSessionCredentials(\n      assumedCreds.getAccessKeyId,\n      assumedCreds.getSecretAccessKey,\n      assumedCreds.getSessionToken\n    )\n\n    new AWSStaticCredentialsProvider(basic)\n  }\n}\n\nval client = AmazonAutoScalingClientBuilder\n  .standard()\n  .withCredentials(staticProvider)\n  .withRegion(region)\n  .build()\n</code></pre> <p></p>"},{"location":"scala/using-gradle-to-run-scala-projects-locally/","title":"Using Gradle to Run Scala Projects Locally","text":"2017-10-12 Discuss <p>Running projects locally in a minimal fashion is often useful for understanding the code, using a debugger and performing interactive integration testing.</p> <p>You can run REPLs with <code>./gradlew</code>, but due to the input fiddling that is going on, it's very distracting and not effective, especially if you need to build up anything more than simple state. You will be better served by using the IntelliJ Scala Worksheet, which makes it easy to gain access to the libraries you have included in your build.</p> <pre><code>buildscript {\n    dependencies {\n        classpath \"gradle.plugin.com.github.maiflai:gradle-scalatest:0.14\"\n        classpath \"org.akhikhl.gretty:gretty:2.0.0\"\n    }\n}\n\napply plugin: 'com.github.maiflai.scalatest'\napply plugin: 'org.akhikhl.gretty'\n\ntasks.withType(ScalaCompile) {\n    scalaCompileOptions.setAdditionalParameters([\n        '-deprecation',\n        '-unchecked',\n        '-Xexperimental',\n        '-Xlint:_,-infer-any',\n        '-feature',\n        '-Ydelambdafy:method'\n    ])\n}\n\n// for servlet applications\ngretty {\n    httpPort = 7101\n    contextPath = '/'\n    servletContainer = 'jetty7'\n    jvmArgs = [\n        \"-Dlog4j.configurationFile=\" + System.getProperty(\"user.dir\") + \"/src/main/resources/log4j_dev.xml\"\n    ]\n}\n\n// for main class applications\ntask runService(dependsOn: 'classes', type: JavaExec) {\n    main = 'com.example.app.Main'\n    classpath sourceSets.main.runtimeClasspath\n}\n</code></pre> <pre><code>./gradlew appRun\n./gradlew runService\n</code></pre> <p></p>"},{"location":"scala/using-the-scala-repl-to-configure-amazon-ses-notifications/","title":"Using the Scala REPL to Configure Amazon SES Notifications","text":"2014-06-30 Discuss <p>Let's say you want to configure bounce notifications for SES emails using the Scala REPL.  How do you go about accessing the AWS API to make it happen?  Make sure to have the AWS SDK for Java API Reference available for additional details.</p> <p>Install Scala and SBT:</p> <pre><code>brew install scala sbt\n</code></pre> <p>Create an AWS credentials file at <code>~/.aws/credentials</code> that is formatted like so:</p> <pre><code>[default]\naws_access_key_id=...\naws_secret_access_key=...\naws_session_token=...\n</code></pre> <p>Create a <code>build.sbt</code> file that looks like:</p> <pre><code>name := \"aws-sdk-client\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.11.0\"\n\nlibraryDependencies ++= Seq(\n    \"commons-logging\" % \"commons-logging\" % \"1.1.3\",\n    \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\"\n)\n</code></pre> <p>You can then engage the Scala REPL to talk to the AWS API like so (Scala will download the AWS SDK from Maven Central automatically):</p> <pre><code>sbt\nconsole\n</code></pre> <pre><code>import com.amazonaws.auth._\nimport com.amazonaws.auth.profile._\nimport com.amazonaws.services.simpleemail._\nimport com.amazonaws.services.simpleemail.model._\nimport com.amazonaws.services.sns._\nimport com.amazonaws.services.sns.model._\n\nval c = new AmazonSNSClient(\n    new AWSCredentialsProviderChain(\n        // attempt on-instance credentials first\n        new InstanceProfileCredentialsProvider(),\n        // fallback to aws credentials file\n        new ProfileCredentialsProvider()\n    )\n)\n\nc.setEndpoint(\"sns.us-east-1.amazonaws.com\")\n\nc.createTopic(\"ses-email-bounce\")\n\nres0: com.amazonaws.services.sns.model.CreateTopicResult =\n{TopicArn: arn:aws:sns:us-east-1:000000000000:ses-email-bounce}\n\nval c = new AmazonSimpleEmailServiceClient(\n    new AWSCredentialsProviderChain(\n        // attempt on-instance credentials first\n        new InstanceProfileCredentialsProvider(),\n        // fallback to aws credentials file\n        new ProfileCredentialsProvider()\n    )\n)\n\nc.setEndpoint(\"email.us-east-1.amazonaws.com\")\n\nc.getSendQuota()\n\nres1: com.amazonaws.services.simpleemail.model.GetSendQuotaResult =\n{Max24HourSend: 100.0,MaxSendRate: 10.0,SentLast24Hours: 1.0}\n\nc.getSendStatistics()\n\nres2: com.amazonaws.services.simpleemail.model.GetSendStatisticsResult =\n{SendDataPoints: [{Timestamp: ...,DeliveryAttempts: 0,Bounces: 0,Complaints: 0,Rejects: 0}, ...\n\nc.getIdentityVerificationAttributes(\n    new GetIdentityVerificationAttributesRequest()\n        .withIdentities(\"some.email@example.com\")\n)\n\nres3: com.amazonaws.services.simpleemail.model.GetIdentityVerificationAttributesResult =\n{VerificationAttributes: {some.email@mydomain.com={VerificationStatus: Success,}}}\n\nc.setIdentityNotificationTopic(\n    new SetIdentityNotificationTopicRequest()\n        .withIdentity(\"some.email@example.com\")\n        .withNotificationType(\"Bounce\")\n        .withSnsTopic(s\"arn:aws:sns:us-east-1:$accountId:ses-email-bounce\")\n)\n\nc.getIdentityNotificationAttributes(\n    new GetIdentityNotificationAttributesRequest()\n        .withIdentities(\"some.email@example.com\")\n)\n</code></pre> <p>One of the nice things about using the Scala REPL is that you get some useful tab completions (SES object here):</p> <pre><code>scala&gt; c.\naddRequestHandler                   sendEmail\nasInstanceOf                        sendRawEmail\ndeleteIdentity                      setConfiguration\ndeleteVerifiedEmailAddress          setEndpoint\ngetCachedResponseMetadata           setIdentityDkimEnabled\ngetIdentityDkimAttributes           setIdentityFeedbackForwardingEnabled\ngetIdentityNotificationAttributes   setIdentityNotificationTopic\ngetIdentityVerificationAttributes   setRegion\ngetRequestMetricsCollector          setServiceNameIntern\ngetSendQuota                        setSignerRegionOverride\ngetSendStatistics                   setTimeOffset\ngetServiceName                      shutdown\ngetSignerByURI                      toString\ngetSignerRegionOverride             verifyDomainDkim\ngetTimeOffset                       verifyDomainIdentity\nisInstanceOf                        verifyEmailAddress\nlistIdentities                      verifyEmailIdentity\nlistVerifiedEmailAddresses          withTimeOffset\nremoveRequestHandler\n</code></pre> <p>Once you have a feel for how the Scala REPL works with a library, you can create alternate entry point for sbt called <code>scalas</code> which allows you to write full Scala scripts with library dependencies.  See Scripts, REPL, and Dependencies for additional details.  In brief, create a <code>scalas</code> script and make it available on your <code>PATH</code>:</p> <pre><code>#!/bin/sh\ntest -f ~/.sbtconfig &amp;&amp; . ~/.sbtconfig\nexec java \\\n    -Xms512M \\\n    -Xmx1536M \\\n    -Xss1M \\\n    -XX:+CMSClassUnloadingEnabled \\\n    -XX:MaxPermSize=256M \\\n    ${SBT_OPTS} \\\n    -jar /usr/local/Cellar/sbt/0.13.2/libexec/sbt-launch.jar \\\n    -Dsbt.main.class=sbt.ScriptMain \\\n    \"$@\"\n</code></pre> <p>You can then write Scala scripts like so:</p> <pre><code>#!/usr/bin/env scalas\n\n/***\n    scalaVersion := \"2.11.0\"\n\n    libraryDependencies ++= Seq(\n        \"commons-logging\" % \"commons-logging\" % \"1.1.3\",\n        \"com.amazonaws\" % \"aws-java-sdk\" % \"1.8.2\"\n    )\n*/\n\nimport com.amazonaws.auth._\nimport com.amazonaws.auth.profile._\nimport com.amazonaws.services.simpleemail._\nimport com.amazonaws.services.simpleemail.model._\n\nval c = new AmazonSimpleEmailServiceClient(\n    new AWSCredentialsProviderChain(\n        // attempt on-instance credentials first\n        new InstanceProfileCredentialsProvider(),\n        // fallback to aws credentials file\n        new ProfileCredentialsProvider()\n    )\n)\n\nc.setEndpoint(\"email.us-east-1.amazonaws.com\")\n\nprintln(c.getSendQuota())\n\nprintln(c.getSendStatistics().getSendDataPoints.get(0))\n\nprintln(c.getSendStatistics().getSendDataPoints.size)\n</code></pre> <p>Execute the script as follows:</p> <pre><code>$ ./demo.scala\n.\n.\n.\n{Timestamp: Sat Jul 05 04:01:00 PDT 2014,DeliveryAttempts: 3,Bounces: 0,Complaints: 0,Rejects: 0}\n1301\n</code></pre> <p></p>"},{"location":"shell/measuring-transfer-speed-over-time-with-curl/","title":"Measuring Transfer Speed Over Time with cURL","text":"2021-12-02 Discuss <p>Ordinarily when you run the cURL command to download a file, you see a progress meter that updates every second.</p> <pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   346  100   346    0     0    422      0 --:--:-- --:--:-- --:--:--   422\n  4  635M    4 29.8M    0     0  1793k      0  0:06:02  0:00:17  0:05:45 2394k\n</code></pre> <p>This progress meter is written to stderr and if you were to redirect both stderr and stdout to a file and then run <code>tail -f</code> on that file, you would see the exact same progress meter being updated once per second, with no running log of download speed.  The reason that this output updates in place is because the program is writing a carriage return <code>\\r</code> at the end of the progress line instead of a newline <code>\\n</code>.  This causes the cursor to return to the beginning of the line without advancing.</p> <p>With the knowledge of how this operates, it is possible to alter the output of the cURL command to save the per-second speed of a download.  If you further send the results of a large file download to <code>/dev/null</code>, then you have a reasonable approximation of of a speedtest tool and you can graph the download speed over time.  The command below uses <code>tr</code> to rewrite carriage returns as newlines in an unbuffered manner, so that data is instantly available in the output file.</p> <p>As an aside on the power of the tr command, the More Shell, Less Egg blog post by Dr. Drang discusses a programming challenge proposed to Donald Knuth, who solved it with ~10 pages of literate Pascal, and Doug McIlroy who critiqued the solution and provided an alternative solution in six shell commands.</p> <pre><code>URL=\"https://releases.ubuntu.com/20.04/ubuntu-20.04.3-live-server-amd64.iso\"\n\ncurl -L -o /dev/null \"$URL\" 2&gt;&amp;1 \\\n  |tr -u '\\r' '\\n' &gt; curl.out\n</code></pre> <p>This results in an output file that looks like this:</p> <pre><code>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100   346  100   346    0     0    295      0  0:00:01  0:00:01 --:--:--   295\n\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n  0  635M    0 70871    0     0  12988      0 14:14:26  0:00:05 14:14:21 17260\n  0  635M    0  608k    0     0  97534      0  1:53:46  0:00:06  1:53:40  120k\n  0  635M    0 1489k    0     0   201k      0  0:53:41  0:00:07  0:53:34  296k\n  0  635M    0 2742k    0     0   328k      0  0:33:00  0:00:08  0:32:52  548k\n  0  635M    0 4297k    0     0   456k      0  0:23:43  0:00:09  0:23:34  849k\n  0  635M    0 6015k    0     0   580k      0  0:18:40  0:00:10  0:18:30 1210k\n  1  635M    1 8014k    0     0   701k      0  0:15:27  0:00:11  0:15:16 1471k\n  1  635M    1 10.0M    0     0   827k      0  0:13:05  0:00:12  0:12:53 1749k\n  1  635M    1 11.0M    0     0   841k      0  0:12:52  0:00:13  0:12:39 1682k\n  .\n  .\n  .\n</code></pre> <p>Write a Python script <code>plot-curl-data.py</code> to process the data to convert it into a format useful for gnuplot and render a plot:</p> <pre><code>#!/usr/bin/env python3\n\nimport subprocess\nimport sys\n\n\ndef transform_curl_data(curl_data_filename: str) -&gt; None:\n    raw_lines = []\n    with open(curl_data_filename) as f:\n        for line in f.readlines():\n            raw_lines.append(line.split())\n\n    converted_lines = []\n    for line in raw_lines[3:]:\n        if len(line) == 12 and '--' not in line[9]:\n            # curl reports speed in bytes per second\n            if 'k' in line[11]:\n                line[11] = str(float(line[11].replace('k', '')) * 8 * 1024)\n            elif 'M' in line[11]:\n                line[11] = str(float(line[11].replace('M', '')) * 8 * 1048576)\n            elif 'G' in line[11]:\n                line[11] = str(float(line[11].replace('G', '')) * 8 * 1073741824)\n            converted_lines.append([line[9], line[11]])\n\n    with open(f'{curl_data_filename}.gnuplot.data', 'w') as f:\n        for line in converted_lines:\n            f.write(','.join(line) + '\\n')\n\n\ndef plot(curl_data_filename: str) -&gt; None:\n    payload = ''.join([\n        f'set output \"{curl_data_filename}.png\"\\n',\n        'set datafile separator \",\"\\n',\n        'set terminal png size 1400,800\\n',\n        'set title \"Download Speed\"\\n',\n        'set ylabel \"Speed (Mbits/s)\"\\n',\n        'set xlabel \"Time (seconds)\"\\n',\n        'set xdata time\\n',\n        'set timefmt \"%H:%M:%S\"\\n',\n        'set key outside\\n',\n        'set grid\\n',\n        'plot \\\\\\n',\n        f'\"{curl_data_filename}.gnuplot.data\" using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \"speed\"\\n'\n    ])\n\n    gnuplot_command_file = f'{curl_data_filename}.gp'\n    with open(gnuplot_command_file, 'w') as f:\n        f.write(payload)\n\n    try:\n        subprocess.run(\n            ['which', 'gnuplot'],\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.STDOUT\n        )\n    except subprocess.CalledProcessError:\n        print(f'gnuplot is not available on the PATH')\n        exit(1)\n\n    subprocess.run(['gnuplot', gnuplot_command_file])\n\n\nif __name__ == '__main__':\n    if len(sys.argv) &lt; 2:\n        print(f'Usage: {sys.argv[0]} [curl_data_filename]')\n        exit(1)\n    else:\n        transform_curl_data(sys.argv[1])\n        plot(sys.argv[1])\n</code></pre> <p>Run this script like so:</p> <pre><code>./plot-curl-data.py curl.out\n</code></pre> <p>You will end up with data (<code>curl.out.gp.data</code>) and configuration files (<code>curl.out.gp</code>) like so:</p> <pre><code>0:00:01,295\n0:00:01,0\n0:00:02,0\n0:00:03,0\n0:00:04,0\n0:00:05,17260\n0:00:06,983040\n0:00:07,2424832\n0:00:08,4489216\n0:00:09,6955008\n0:00:10,9912320\n0:00:11,12050432\n0:00:12,14327808\n0:00:13,13778944\n0:00:14,12173312\n.\n.\n.\n</code></pre> <pre><code>set output \"curl.out.png\"\nset datafile separator \",\"\nset terminal png size 1400,800\nset title \"Download Speed\"\nset ylabel \"Speed (Mbits/s)\"\nset xlabel \"Time (seconds)\"\nset xdata time\nset timefmt \"%H:%M:%S\"\nset key outside\nset grid\nplot \\\n\"curl.out.gp.data\" using 1:($2/1e6) with lines lw 1 lt 1 lc 1 title \"speed\"\n</code></pre> <p>The graph will be rendered in PNG format:</p> <p></p> <p></p>"},{"location":"shell/parsing-json-on-the-command-line/","title":"Parsing JSON on the Command Line","text":"2014-07-16 Discuss <p>With more APIs moving to JSON, being able to parse it at the command line allows you to write more sophisticated shell scripts that can interact with your favorite services.  Most first attempts at JSON parsing using some variation of the following to get the job done, which initially seems reasonable:</p> <pre><code>curl -s http://endpoint.info \\\n    |python -mjson.tool \\\n    |grep foo \\\n    |cut -d: -f2 \\\n    |sed -e 's/\"//g'\n</code></pre> <p>However, this can rapidly get out of hand, if you have key duplication, complex nested structures or you need to pull in all of the elements of a list.  For awhile, underscore-cli was my favorite fully-featured JSON parser, but I found its documentation somewhat lacking and it hasn't seen serious development since November 2012.  Since then, I found jq which has a beautiful, well-written manual with many usage examples and it is under active development.  It also has the benefit of being written in C, which helps speed and it has a fairly concise descriptor language.  To install:</p> <pre><code>brew install jq\n</code></pre> <p>Then you can do things like flattening a complex JSON structure into a simple CSV:</p> <pre><code>PAYLOADS=$( curl -s $URL |jq '.payloads' )\nif [ \"$PAYLOADS\" != \"[]\" ]; then\n    echo $PAYLOADS \\\n        | jq -r '.[] | .minutes[].payload.items[] | [.actions.actionTime, (.actions | {actions} | .actions.email.state), .sourceInstance, (.actions | {actions} | .actions.email.info )] | @csv'\nfi\n</code></pre> <p>As a bonus feature, if you have to deal in XML rather than JSON, then xmlstarlet is a good choice for handling it.  Naturally, installation:</p> <pre><code>brew install xmlstarlet\n</code></pre> <p>Once you have xmlstarlet, you can do things like pull the build number out of an Atlassian Bamboo HTML page, which was necessary when they did not have an API call available to report the version number of the last known good build for a project:</p> <pre><code>curl -s --insecure https://bamboo.local/browse/${BUILDKEY}${BUILD} \\\n    |tidy -asxhtml -numeric --force-output true 2&gt;/dev/null \\\n    |xmlstarlet sel -N x=\"http://www.w3.org/1999/xhtml\" -t -m \"//x:div[@id='sr-build']/x:h2/x:a\" -v \".\" \\\n    |sed -e \"s/#//g\"\n</code></pre> <p></p>"},{"location":"shell/pet-cli-snippet-manager/","title":"pet - CLI Snippet Manager","text":"2017-10-13 Discuss <p>https://github.com/knqyf263/pet</p> <p>This shell utility that makes it easy to save, search and recall shell commands.  It uses GitHub Gists as the backend storage, so your work is backed up and accessible from multiple locations.  Of course, since you are storing data publicly, make sure that you are sanitizing the data, so no secrets are shared.</p>"},{"location":"shell/pet-cli-snippet-manager/#install-and-configure","title":"Install and Configure","text":"<p>Install with Homebrew:</p> <pre><code>brew install pet\n</code></pre> <p>Create a new access token on GitHub that allows only the <code>gist</code> scope.  Create a pet configuration file and set the <code>access_token</code>.</p> <pre><code>pet configure\n</code></pre>"},{"location":"shell/pet-cli-snippet-manager/#usage","title":"Usage","text":"<p>Create new snippets:</p> <pre><code>pet new\n</code></pre> <p>List existing snippets:</p> <pre><code>pet list\n</code></pre> <p>Search for snippets:</p> <pre><code>pet search\n</code></pre> <p>Warning</p> <p>The <code>pet</code> command does not have version validation associated with snippets uploaded to Gist, so it is possible to wipe out your snippets in two separate ways:</p> <ol> <li> <p>Downloading empty snippets from your Gist, which overwrite your local snippets.</p> </li> <li> <p>Uploading empty local snippets to your Gist.  Since GitHub stores revisions, you can recover from this.</p> </li> </ol> <p>Make sure that you have the latest snippets locally before updating your Gist.</p> <p>Upload snippets to Gist:</p> <pre><code>pet sync -u\n</code></pre> <p>Download snippets from Gist:</p> <pre><code>pet sync\n</code></pre> <p></p>"},{"location":"shell/schedule-ad-hoc-jobs-for-later-on-an-instance-with-atd/","title":"Schedule Ad-Hoc Jobs for Later on an Instance with atd","text":"2017-08-15 Discuss <p>https://www.computerhope.com/unix/uat.htm</p> <p>The purpose of this technique is to allow you to disconnect from the instance, while leaving the job running (i.e. not dependent on a shell fork).</p> <pre><code>(root) # at now + 1 minute\nwarning: commands will be executed using /bin/sh\nat&gt; /apps/something/bin/upload_to_s3.sh\nat&gt; ^d\njob 2 at Wed Aug  9 20:37:00 2017\n\n(root) # atq\n2   Wed Aug  9 20:37:00 2017 a root\n</code></pre> <p>If you need to remove and reschedule a job, you can do the following:</p> <pre><code>(root) # atrm 2\n</code></pre> <p>The following are examples of casual times that can be used with at:</p> expression time noon 12:00 PM October 18 2014 midnight 12:00 AM October 19 2014 teatime 4:00 PM October 18 2014 tomorrow 10:00 AM October 19 2014 noon tomorrow 12:00 PM October 19 2014 next week 10:00 AM October 25 2014 next monday 10:00 AM October 24 2014 fri 10:00 AM October 21 2014 NOV 10:00 AM November 18 2014 9:00 AM 9:00 AM October 19 2014 2:30 PM 2:30 PM October 18 2014 1430 2:30 PM October 18 2014 2:30 PM tomorrow 2:30 PM October 19 2014 2:30 PM next month 2:30 PM November 18 2014 2:30 PM Fri 2:30 PM October 21 2014 2:30 PM 10/21 2:30 PM October 21 2014 2:30 PM Oct 21 2:30 PM October 21 2014 2:30 PM 10/21/2014 2:30 PM October 21 2014 2:30 PM 21.10.14 2:30 PM October 21 2014 now + 30 minutes 10:30 AM October 18 2014 now + 1 hour 11:00 AM October 18 2014 now + 2 days 10:00 AM October 20 2014 4 PM + 2 days 4:00 PM October 20 2014 now + 3 weeks 10:00 AM November 8 2014 now + 4 months 10:00 AM February 18 2015 now + 5 years 10:00 AM October 18 2019 <p></p>"},{"location":"shell/zsh-keyboard-shortcuts/","title":"Z Shell Keyboard Shortcuts","text":""},{"location":"shell/zsh-keyboard-shortcuts/#introduction","title":"Introduction","text":"<p>Default Z shell keyboard shortcuts, tailored for MacOS and Kitty.</p> <p>I have been on a minimal configuration kick lately and don't want to customize things heavily.</p> <p>Use <code>bindkey</code> to see current configuration settings. More details available in the <code>zshzle</code> man page.</p>"},{"location":"shell/zsh-keyboard-shortcuts/#kitty-windows-tabs","title":"Kitty Windows &amp; Tabs","text":"Description Shortcut Switch between enabled window layouts <code>ctrl-shift-l</code> New window <code>cmd-enter</code> Close window <code>cmd-shift-d</code> Next window <code>ctrl-shift-]</code> Previous window <code>ctrl-shift-[</code> Move window forward <code>ctrl-shift-f</code> Move window backward <code>ctrl-shift-b</code> New tab <code>cmd-t</code> Close tab <code>cmd-w</code> Next tab <code>cmd-shift-]</code> Previous tab <code>cmd-shift-[</code> Move tab forward <code>ctrl-shift-.</code> Move tab backward <code>ctrl-shift-,</code>"},{"location":"shell/zsh-keyboard-shortcuts/#moving-within-a-line","title":"Moving within a Line","text":"Description Mapping Shortcut Move one character backwards <code>backward-char</code> <code>leftArrow</code> <code>ctrl-b</code> Move one character forwards <code>forward-char</code> <code>rightArrow</code> <code>ctrl-f</code> Move one word backwards <code>backward-word</code> <code>ctrl-[b</code> Move one word forwards <code>forward-word</code> <code>ctrl-[f</code> Move to the beginning of the line <code>beginning-of-line</code> <code>ctrl-a</code> Move to the end of the line <code>end-of-line</code> <code>ctrl-e</code>"},{"location":"shell/zsh-keyboard-shortcuts/#editing-a-line","title":"Editing a Line","text":"Description Mapping Shortcut Delete the character before the cursor <code>backward-delete-char</code> <code>delete</code> <code>ctrl-h</code> Delete the character under the cursor <code>delete-char-or-list</code> <code>ctrl-d</code> Delete the word before the cursor <code>backward-kill-word</code> <code>ctrl-w</code> <code>ctrl-[,ctrl-h</code> Delete the word after the cursor <code>kill-word</code> <code>ctrl-[d</code> Delete the line after the cursor <code>kill-line</code> <code>ctrl-k</code> Delete the whole line <code>kill-whole-line</code> <code>ctrl-u</code> Transpose the two characters before the cursor <code>transpose-chars</code> <code>ctrl-t</code> Transpose the two words before the cursor <code>transpose-words</code> <code>ctrl-[t</code> Make a word lowercase <code>down-case-word</code> <code>ctrl-[l</code> Make a word uppercase <code>up-case-word</code> <code>ctrl-[u</code> Quote line <code>quote-line</code> <code>ctrl-['</code> Push line onto buffer <code>push-line</code> <code>ctrl-[q</code> Get line from buffer <code>get-line</code> <code>ctrl-[g</code> Delete buffer <code>kill-buffer</code> <code>ctrl-x,ctrl-k</code> Undo the last change <code>undo</code> <code>ctrl-xu</code> <code>ctrl-x,ctrl-u</code> Execute line <code>accept-line</code> <code>enter</code> <code>ctrl-j</code> <code>ctrl-m</code>"},{"location":"shell/zsh-keyboard-shortcuts/#screen-management","title":"Screen Management","text":"Description Mapping Shortcut Clear screen, leaving current line intact <code>clear-screen</code> <code>ctrl-l</code> Halt output to screen ? <code>ctrl-s</code> Resume output to screen ? <code>ctrl-q</code>"},{"location":"shell/zsh-keyboard-shortcuts/#process-management","title":"Process Management","text":"Description Shortcut Terminate/kill current foreground process <code>ctrl-c</code> Suspend/stop current foreground process <code>ctrl-z</code> Execute last command in history <code>!!</code> Execute last command in history that starts with abc <code>!abc</code> Print last command in history beginning with abc <code>!abc:p</code>"},{"location":"shell/zsh-keyboard-shortcuts/#history","title":"History","text":"Description Mapping Shortcut Previous history line <code>up-line-or-history</code> <code>upArrow</code> <code>ctrl-p</code> Next history line <code>down-line-or-history</code> <code>downArrow</code> <code>ctrl-n</code> Search history backwards <code>history-incremental-search-backward</code> <code>ctrl-r</code> Search history forwards <code>history-incremental-search-forward</code> <code>ctrl-s</code> Exit history search <code>send-break</code> <code>ctrl-g</code>"},{"location":"stocks/nflx-quarterly-subscriber-growth/","title":"Netflix Quarterly Subscriber Growth","text":"<p>All of the information presented here is taken from quarterly earnings reports publicly posted on the Netflix Investors website. The subscriber numbers can be found on the Segment Information tab of the Quarterly Earnings Financial Statements spreadsheets. The Domestic and International streaming paid memberships were first broken out separately in 2011-Q3.</p> <p>The spreadsheet with the source data that generated these charts is available for review, so you can see that it is a straight copy from the source.</p> <p></p> <p></p>"},{"location":"talks/favorite/","title":"Favorite Talks","text":""},{"location":"talks/favorite/#c","title":"C","text":"<ul> <li>Handmade Hero Day 001: Setting Up the Windows Build - Casey Muratori | 2014</li> </ul>"},{"location":"talks/favorite/#career","title":"Career","text":"<ul> <li>Rethinking the Developer Career Path \u2013 Randall Koutnik | The Lead Developer UK 2017</li> </ul>"},{"location":"talks/favorite/#clojure","title":"Clojure","text":"<ul> <li>Are We There Yet? - Rich Hickey | JVM Language Summit 2009</li> <li>Boot Can Built It - Alan Dipert, Micha Niskin | Clojure/west 2015</li> <li>Bottom Up vs Top Down Design in Clojure - Mark Bastian | Clojure/conj 2015</li> <li>Clojure Concurrency - Rich Hickey | Western Mass. Developers Group</li> <li>Clojure core.async - Rich Hickey | Strange Loop 2013</li> <li>Clojure Made Simple - Rich Hickey | Oracle Developers 2015</li> <li>Clojure Parallelism Beyond Futures - Leon Barrett | Clojure/west 2015</li> <li>clojure.spec - Rich Hickey | LispNYC 2017</li> <li>Composing Interactive Apps With Zelkova - James MacAulay | Clojure/west 2015</li> <li>Dagobah, a Data centric Meta scheduler - Matt Bossenbroek | Clojure/conj 2015</li> <li>Debugging Clojure Code with Cursive - Colin Fleming | Clojure/west 2015</li> <li>Design and Prototype a Language In Clojure - Jeanine Adkisson | Clojure/west 2015</li> <li>Hammock Driven Development - Rich Hickey | Clojure/conj 2010</li> <li>Parallel Programming, Fork Join, and Reducers - Daniel Higginbotham | Clojure/west 2016</li> <li>Parsing Text with a Virtual Machine - Ghadi Shayban | Clojure/west 2016</li> <li>REPL-Based Development Demo - Valentin Waeselynck | 2017 (from What makes a good REPL?)</li> <li>Simple Made Easy - Rich Hickey | Strange Loop 2011</li> <li>Spec-ulation - Rich Hickey | Clojure/conj 2016</li> <li>The Joys and Perils of Interactive Development - Stuart Sierra | Clojure/west 2016</li> <li>The ReactJS Landscape - Luke VanderHart | Clojure/west 2015</li> <li>Types are Like the Weather, Type Systems are Like Weathermen - Matthias Felleisen | Clojure/west 2016</li> </ul>"},{"location":"talks/favorite/#clojurescript","title":"ClojureScript","text":"<ul> <li>ClojureScript for Skeptics - Derek Slager | Clojure/conj 2015</li> <li>Developing ClojureScript With Figwheel - Bruce Hauman | Clojure/west 2015</li> <li>From 0 to Prototype Using ClojureScript, Re-Frame and Friends - Martin Clausen | Dutch Clojure Days 2017</li> <li>Interactive Programming Flappy Bird in ClojureScript - Bruce Hauman | 2014</li> </ul>"},{"location":"talks/favorite/#netflix-atlas","title":"Netflix Atlas","text":"<ul> <li>Netflix: Amazon S3 &amp; Amazon Elastic MapReduce to Monitor at Gigascale - Roy Rapoport | AWS re:Invent 2013</li> <li>Monitoring Monitoring Systems at Netflix - Roy Rapoport | Monitorama PDX 2017</li> <li>Netflix Atlas Telemetry - A Platform Begets an Ecosystem</li> </ul>"},{"location":"talks/favorite/#python","title":"Python","text":"<ul> <li>Automating Your Browser and Desktop Apps - Al Sweigart | PyBay2016</li> <li>Beyond PEP 8: Best Practices for Beautiful Intelligible Code - Raymond Hettinger | PyCon 2015</li> <li>Built in Super Heroes - David Beazley | PyData Chicago 2016</li> <li>Fear and Awaiting in Async: A Savage Journey to the Heart of the Coroutine Dream - David Beazley | PyOhio 2016</li> <li>Logging and Testing and Debugging, Oh My! - Albert Sweigart | PyBay2017</li> <li>Machete-Mode Debugging: Hacking Your Way Out of a Tight Spot - Ned Batchelder | PyCon 2016</li> <li>Python as a Configuration Language - Guillermo P\u00e9rez | PyCon 2016</li> <li>Python Concurrency From the Ground Up: LIVE! - David Beazley | PyCon 2015</li> <li>Python Language - Guido van Rossum | PyCon 2016</li> <li>Python vs Ruby: A Battle to The Death - Gary Bernhardt | NWPD 2010</li> <li>Refactoring Python: Why and How to Restructure Your Code - Brett Slatkin | PyCon 2016</li> <li>Removing Python's GIL: The Gilectomy - Larry Hastings | PyCon 2016</li> <li>Stop Writing Classes - Jack Diederich | PyCon 2012</li> <li>The Packaging Gradient - Mahmoud Hashemi | PyBay 2017</li> <li>Think Like a Pythonista - Luciano Ramalho | PyBay 2017</li> <li>Thinking In Coroutines - \u0141ukasz Langa | PyCon 2016</li> <li>To Mock, or Not to Mock, That is the Question - Ana Balica | PyCon 2016</li> <li>Writing Awesome Command-Line Programs in Python - Mark Smith | EuroPython 2014</li> </ul>"},{"location":"talks/favorite/#scala","title":"Scala","text":"<ul> <li>A Better Scala REPL - Li Haoyi | SBTB 2015</li> </ul>"},{"location":"talks/favorite/#unix","title":"Unix","text":"<ul> <li>The Unix Chainsaw - Gary Bernhardt | Cascadia Ruby Conf 2011</li> </ul>"},{"location":"talks/netflix-atlas-telemetry-a-platform-begets-an-ecosystem/","title":"Netflix Atlas Telemetry - A Platform Begets an Ecosystem","text":"<p> <p></p>"}]}